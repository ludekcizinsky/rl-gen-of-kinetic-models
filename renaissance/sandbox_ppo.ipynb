{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aae562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import renaissance.helper as hp\n",
    "from configparser import ConfigParser\n",
    "\n",
    "from renaissance.ppo_refinement import PPORefinement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51519fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_flag = 0\n",
    "lambda_partition = -2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d113bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lambda_max(p_tensor_single):\n",
    "    # Convert tensor to NumPy\n",
    "    p_numpy = p_tensor_single.detach().cpu().numpy()\n",
    "\n",
    "    # Determine size n from length = n(n+1)/2\n",
    "    L = p_numpy.shape[0]\n",
    "    # Solve n(n+1)/2 = L\n",
    "    n = int((np.sqrt(8 * L + 1) - 1) / 2)\n",
    "    assert n * (n + 1) // 2 == L, f\"Input length {L} is not valid for any symmetric n x n matrix.\"\n",
    "\n",
    "    # Fill upper triangular matrix\n",
    "    A = np.zeros((n, n))\n",
    "    idx = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):  # Only upper triangle including diagonal\n",
    "            A[i, j] = p_numpy[idx]\n",
    "            A[j, i] = p_numpy[idx]  # Mirror to lower triangle\n",
    "            idx += 1\n",
    "\n",
    "    # Compute eigenvalues\n",
    "    eigvals = np.linalg.eigvals(A)\n",
    "\n",
    "    # Sort by real part descending\n",
    "    eigvals_sorted = sorted(eigvals, key=lambda x: x.real, reverse=True)\n",
    "\n",
    "    return eigvals_sorted\n",
    "\n",
    "def compute_reward(p_tensor_single, n_consider=10):\n",
    "    lambdas_val = _get_lambda_max(p_tensor_single)\n",
    "\n",
    "    if reward_flag == 0:\n",
    "        lambda_max_val = lambdas_val[0]\n",
    "        penalty = np.maximum(0, lambda_max_val)\n",
    "        if lambda_max_val > 100:\n",
    "            lambda_max_val = 100\n",
    "        r = 1.0 / (1.0 + np.exp(lambda_max_val - lambda_partition))\n",
    "        r -= penalty\n",
    "    else:\n",
    "        considered_avg = sum(lambdas_val[:n_consider]) / n_consider\n",
    "        r = np.exp(-0.1 * considered_avg) / 2\n",
    "    # TODO: Right now, we are not using the Incidence part of the reward.\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741db84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Begin PPO refinement strategy\n",
      "Training on cpu. 1 trajectories per update.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 10)) of distribution Normal(loc: torch.Size([1, 10]), scale: torch.Size([1, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n       grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-856c4767e817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrained_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_training_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthis_savepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#print(f\"PPO training finished. Rewards log saved to {this_savepath}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/renaissance/ppo_refinement.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_training_iterations, output_path)\u001b[0m\n\u001b[1;32m    227\u001b[0m             )\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_rollout_data_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_batch_final_rewards\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/renaissance/ppo_refinement.py\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m(self, rollout_data)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mmu_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mstd_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_std_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mdist_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mnew_log_probs_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 56\u001b[0;31m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;34mf\"of distribution {repr(self)} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 10)) of distribution Normal(loc: torch.Size([1, 10]), scale: torch.Size([1, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n       grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "print('--- Begin PPO refinement strategy')\n",
    "\n",
    "configs = ConfigParser()\n",
    "configs.read('renaissance/configfile.ini')\n",
    "output_path = configs['PATHS']['output_path']\n",
    "this_savepath = f'output/ppo-refinement/ppo_sandbox/' \n",
    "os.makedirs(this_savepath, exist_ok=True)\n",
    "\n",
    "ppo_agent = PPORefinement(\n",
    "    param_dim=10,\n",
    "    noise_dim=10,\n",
    "    reward_function=compute_reward,\n",
    "    min_x_bounds=-10,\n",
    "    max_x_bounds=10,\n",
    "    ppo_epochs=10,\n",
    "    T_horizon=1,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-4,\n",
    "    n_trajectories=1,\n",
    ")\n",
    "\n",
    "trained_actor, rewards = ppo_agent.train(num_training_iterations=100, output_path=this_savepath)\n",
    "\n",
    "#print(f\"PPO training finished. Rewards log saved to {this_savepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a94691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'PPO Training Rewards')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Final Reward')\n",
    "plt.title('PPO Training Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74902174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "def evaluate_policy_incidence(trained_actor, ppo_agent, num_trials=50):\n",
    "    T_HORIZON = ppo_agent.T_horizon\n",
    "    NOISE_DIM = ppo_agent.noise_dim\n",
    "    N_TRIALS = num_trials\n",
    "    incidence = 0\n",
    "    for _ in range(N_TRIALS):\n",
    "        current_params_in_state = ppo_agent._initialize_current_params_for_state().clone()\n",
    "        generated_sequence = []\n",
    "        for _ in range(T_HORIZON):\n",
    "            noise = torch.randn(NOISE_DIM, device=ppo_agent.device)\n",
    "            state_1d = torch.cat((noise, current_params_in_state.detach()), dim=0)\n",
    "            state_batch = state_1d.unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                mu_raw, log_std_raw = trained_actor(state_batch)\n",
    "                # For generation, you might want to take the mean (mu_raw) or sample\n",
    "                # action_raw = mu_raw # Deterministic generation\n",
    "                action_raw = Normal(mu_raw, torch.exp(log_std_raw)).sample() # Stochastic generation\n",
    "            \n",
    "            ode_params = ppo_agent._transform_to_bounded(action_raw)\n",
    "            current_params_in_state = ode_params.squeeze(0)\n",
    "            generated_sequence.append(current_params_in_state.cpu().numpy())\n",
    "\n",
    "        final_generated_params = generated_sequence[-1]\n",
    "        final_reward_eval = compute_reward(torch.tensor(final_generated_params, device=ppo_agent.device))\n",
    "        lambda_max = _get_lambda_max(torch.tensor(final_generated_params, device=ppo_agent.device))[0]\n",
    "        if lambda_max < -2.5:\n",
    "            incidence += 1\n",
    "        print(f\"Final lambda_max: {lambda_max:.4f}\")\n",
    "        print(f\"Final reward: {final_reward_eval:.4f}\")\n",
    "    print(f\"Incidence over {N_TRIALS} trials: {incidence}/{N_TRIALS} = {incidence/N_TRIALS:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202681fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final lambda_max: -7.1210\n",
      "Final reward: 0.9903\n",
      "Final lambda_max: -8.2873\n",
      "Final reward: 0.9969\n",
      "Final lambda_max: -8.1637\n",
      "Final reward: 0.9965\n",
      "Final lambda_max: -7.4484\n",
      "Final reward: 0.9930\n",
      "Final lambda_max: -8.2206\n",
      "Final reward: 0.9967\n",
      "Final lambda_max: -7.8938\n",
      "Final reward: 0.9955\n",
      "Final lambda_max: -8.0707\n",
      "Final reward: 0.9962\n",
      "Final lambda_max: -7.3388\n",
      "Final reward: 0.9921\n",
      "Final lambda_max: -8.5687\n",
      "Final reward: 0.9977\n",
      "Final lambda_max: -7.9198\n",
      "Final reward: 0.9956\n",
      "Final lambda_max: -8.0095\n",
      "Final reward: 0.9960\n",
      "Final lambda_max: -8.1965\n",
      "Final reward: 0.9967\n",
      "Final lambda_max: -7.9705\n",
      "Final reward: 0.9958\n",
      "Final lambda_max: -8.0716\n",
      "Final reward: 0.9962\n",
      "Final lambda_max: -7.9326\n",
      "Final reward: 0.9956\n",
      "Final lambda_max: -8.0483\n",
      "Final reward: 0.9961\n",
      "Final lambda_max: -7.8731\n",
      "Final reward: 0.9954\n",
      "Final lambda_max: -8.1165\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -8.2331\n",
      "Final reward: 0.9968\n",
      "Final lambda_max: -8.6573\n",
      "Final reward: 0.9979\n",
      "Final lambda_max: -7.8948\n",
      "Final reward: 0.9955\n",
      "Final lambda_max: -8.3819\n",
      "Final reward: 0.9972\n",
      "Final lambda_max: -8.1332\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -8.1291\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -8.1537\n",
      "Final reward: 0.9965\n",
      "Final lambda_max: -8.2251\n",
      "Final reward: 0.9967\n",
      "Final lambda_max: -7.9329\n",
      "Final reward: 0.9956\n",
      "Final lambda_max: -8.0274\n",
      "Final reward: 0.9960\n",
      "Final lambda_max: -8.1259\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -7.9443\n",
      "Final reward: 0.9957\n",
      "Final lambda_max: -7.7594\n",
      "Final reward: 0.9948\n",
      "Final lambda_max: -8.7048\n",
      "Final reward: 0.9980\n",
      "Final lambda_max: -7.7205\n",
      "Final reward: 0.9946\n",
      "Final lambda_max: -8.0560\n",
      "Final reward: 0.9962\n",
      "Final lambda_max: -8.1431\n",
      "Final reward: 0.9965\n",
      "Final lambda_max: -8.2288\n",
      "Final reward: 0.9968\n",
      "Final lambda_max: -7.7969\n",
      "Final reward: 0.9950\n",
      "Final lambda_max: -7.6468\n",
      "Final reward: 0.9942\n",
      "Final lambda_max: -7.8991\n",
      "Final reward: 0.9955\n",
      "Final lambda_max: -8.0385\n",
      "Final reward: 0.9961\n",
      "Final lambda_max: -8.6161\n",
      "Final reward: 0.9978\n",
      "Final lambda_max: -7.9944\n",
      "Final reward: 0.9959\n",
      "Final lambda_max: -8.2521\n",
      "Final reward: 0.9968\n",
      "Final lambda_max: -7.3098\n",
      "Final reward: 0.9919\n",
      "Final lambda_max: -8.3445\n",
      "Final reward: 0.9971\n",
      "Final lambda_max: -7.6144\n",
      "Final reward: 0.9940\n",
      "Final lambda_max: -7.6060\n",
      "Final reward: 0.9940\n",
      "Final lambda_max: -8.1172\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -7.6183\n",
      "Final reward: 0.9940\n",
      "Final lambda_max: -7.2254\n",
      "Final reward: 0.9912\n",
      "Incidence over 50 trials: 50/50 = 1.00\n"
     ]
    }
   ],
   "source": [
    "evaluate_policy_incidence(trained_actor, ppo_agent, num_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cbd827",
   "metadata": {},
   "source": [
    "## Nonlinear env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b2a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_nonlinear_dynamics(x, theta):\n",
    "    n = x.shape[0]\n",
    "    assert theta.shape[0] >= n * n, f\"Need at least {n * n} parameters, got {theta.shape[0]}\"\n",
    "    \n",
    "    # Reshape first n^2 parameters into n x n matrix\n",
    "    T = theta[:n * n].reshape(n, n)\n",
    "\n",
    "    # Define nonlinear function: f_i(x) = sum_j T_ij * tanh(x_j) + sin(x_i) * x_i\n",
    "    x_tanh = torch.tanh(x)\n",
    "    linear_part = T @ x_tanh\n",
    "    nonlinear_part = torch.sin(x) * x\n",
    "\n",
    "    return linear_part + nonlinear_part\n",
    "\n",
    "def _get_lambda_max(p_tensor_single, x_eval=None):\n",
    "    # Infer n from length\n",
    "    L = p_tensor_single.shape[0]\n",
    "    n = int(np.sqrt(L))\n",
    "    if x_eval is None:\n",
    "        x_eval = torch.zeros(n, dtype=torch.float32, requires_grad=True)\n",
    "    else:\n",
    "        x_eval = x_eval.detach().clone().requires_grad_(True)\n",
    "\n",
    "    theta = p_tensor_single.detach().clone().requires_grad_(False)\n",
    "\n",
    "    # Compute f(x; theta)\n",
    "    f = generalized_nonlinear_dynamics(x_eval, theta)\n",
    "\n",
    "    # Compute Jacobian via autograd\n",
    "    J_rows = []\n",
    "    for f_i in f:\n",
    "        grad_f_i = torch.autograd.grad(f_i, x_eval, retain_graph=True)[0]\n",
    "        J_rows.append(grad_f_i)\n",
    "\n",
    "    J_matrix = torch.stack(J_rows)\n",
    "    J_np = J_matrix.detach().numpy()\n",
    "\n",
    "    # Compute eigenvalues\n",
    "    eigvals = np.linalg.eigvals(J_np)\n",
    "    eigvals_sorted = sorted(eigvals, key=lambda x: x.real, reverse=True)\n",
    "    return eigvals_sorted\n",
    "\n",
    "def compute_reward(p_tensor_single, n_consider=10):\n",
    "    lambdas_val = _get_lambda_max(p_tensor_single)\n",
    "\n",
    "    if reward_flag == 0:\n",
    "        lambda_max_val = np.real(lambdas_val[0])\n",
    "        penalty = np.maximum(0, lambda_max_val)\n",
    "        if lambda_max_val > 100:\n",
    "            lambda_max_val = 100\n",
    "        r = 1.0 / (1.0 + np.exp(lambda_max_val - lambda_partition))\n",
    "        r -= penalty\n",
    "    else:\n",
    "        considered_avg = sum(lambdas_val[:n_consider]) / n_consider\n",
    "        r = np.exp(-0.1 * considered_avg) / 2\n",
    "    # TODO: Right now, we are not using the Incidence part of the reward.\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66713fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Begin PPO refinement strategy\n",
      "Training on cpu. 64 trajectories per update.\n",
      "Iteration 1/100, Avg Batch Final Reward: -8.1502\n",
      "Avg actor loss -0.0546 Avg critic loss 90.9855\n",
      "Iteration 2/100, Avg Batch Final Reward: -7.1286\n",
      "Avg actor loss -0.0354 Avg critic loss 60.2790\n",
      "Iteration 3/100, Avg Batch Final Reward: -7.1762\n",
      "Avg actor loss -0.0347 Avg critic loss 56.6773\n",
      "Iteration 4/100, Avg Batch Final Reward: -8.4056\n",
      "Avg actor loss -0.0356 Avg critic loss 75.5095\n",
      "Iteration 5/100, Avg Batch Final Reward: -6.0248\n",
      "Avg actor loss -0.0318 Avg critic loss 37.7575\n",
      "Iteration 6/100, Avg Batch Final Reward: -7.9332\n",
      "Avg actor loss -0.0326 Avg critic loss 52.5337\n",
      "Iteration 7/100, Avg Batch Final Reward: -6.9918\n",
      "Avg actor loss -0.0432 Avg critic loss 39.7058\n",
      "Iteration 8/100, Avg Batch Final Reward: -5.9068\n",
      "Avg actor loss -0.0379 Avg critic loss 28.8744\n",
      "Iteration 9/100, Avg Batch Final Reward: -5.9719\n",
      "Avg actor loss -0.0392 Avg critic loss 27.2944\n",
      "Iteration 10/100, Avg Batch Final Reward: -5.5444\n",
      "Avg actor loss -0.0309 Avg critic loss 22.3010\n",
      "Iteration 11/100, Avg Batch Final Reward: -5.4704\n",
      "Avg actor loss -0.0310 Avg critic loss 22.7050\n",
      "Iteration 12/100, Avg Batch Final Reward: -5.8596\n",
      "Avg actor loss -0.0350 Avg critic loss 19.8186\n",
      "Iteration 13/100, Avg Batch Final Reward: -5.4764\n",
      "Avg actor loss -0.0410 Avg critic loss 22.8163\n",
      "Iteration 14/100, Avg Batch Final Reward: -3.7110\n",
      "Avg actor loss -0.0344 Avg critic loss 17.7593\n",
      "Iteration 15/100, Avg Batch Final Reward: -4.6262\n",
      "Avg actor loss -0.0308 Avg critic loss 18.7076\n",
      "Iteration 16/100, Avg Batch Final Reward: -4.4638\n",
      "Avg actor loss -0.0390 Avg critic loss 22.4018\n",
      "Iteration 17/100, Avg Batch Final Reward: -3.3952\n",
      "Avg actor loss -0.0431 Avg critic loss 15.7459\n",
      "Iteration 18/100, Avg Batch Final Reward: -3.2943\n",
      "Avg actor loss -0.0335 Avg critic loss 15.2044\n",
      "Iteration 19/100, Avg Batch Final Reward: -2.7225\n",
      "Avg actor loss -0.0436 Avg critic loss 14.1671\n",
      "Iteration 20/100, Avg Batch Final Reward: -2.9008\n",
      "Avg actor loss -0.0417 Avg critic loss 16.7047\n",
      "Iteration 21/100, Avg Batch Final Reward: -2.2672\n",
      "Avg actor loss -0.0358 Avg critic loss 10.9164\n",
      "Iteration 22/100, Avg Batch Final Reward: -1.9594\n",
      "Avg actor loss -0.0353 Avg critic loss 12.8179\n",
      "Iteration 23/100, Avg Batch Final Reward: -1.9363\n",
      "Avg actor loss -0.0323 Avg critic loss 9.0875\n",
      "Iteration 24/100, Avg Batch Final Reward: -2.4852\n",
      "Avg actor loss -0.0400 Avg critic loss 12.2847\n",
      "Iteration 25/100, Avg Batch Final Reward: -1.7237\n",
      "Avg actor loss -0.0400 Avg critic loss 10.3976\n",
      "Iteration 26/100, Avg Batch Final Reward: -1.6535\n",
      "Avg actor loss -0.0425 Avg critic loss 9.2996\n",
      "Iteration 27/100, Avg Batch Final Reward: -1.1057\n",
      "Avg actor loss -0.0390 Avg critic loss 6.0601\n",
      "Iteration 28/100, Avg Batch Final Reward: -0.3804\n",
      "Avg actor loss -0.0386 Avg critic loss 4.4721\n",
      "Iteration 29/100, Avg Batch Final Reward: -0.8164\n",
      "Avg actor loss -0.0521 Avg critic loss 5.5794\n",
      "Iteration 30/100, Avg Batch Final Reward: -0.6254\n",
      "Avg actor loss -0.0314 Avg critic loss 4.1274\n",
      "Iteration 31/100, Avg Batch Final Reward: -0.3215\n",
      "Avg actor loss -0.0293 Avg critic loss 3.9908\n",
      "Iteration 32/100, Avg Batch Final Reward: -0.4707\n",
      "Avg actor loss -0.0489 Avg critic loss 5.0051\n",
      "Iteration 33/100, Avg Batch Final Reward: -0.7278\n",
      "Avg actor loss -0.0353 Avg critic loss 5.1236\n",
      "Iteration 34/100, Avg Batch Final Reward: -0.4856\n",
      "Avg actor loss -0.0450 Avg critic loss 4.3288\n",
      "Iteration 35/100, Avg Batch Final Reward: -0.0329\n",
      "Avg actor loss -0.0385 Avg critic loss 3.2349\n",
      "Iteration 36/100, Avg Batch Final Reward: 0.2655\n",
      "Avg actor loss -0.0376 Avg critic loss 1.3125\n",
      "Iteration 37/100, Avg Batch Final Reward: 0.1565\n",
      "Avg actor loss -0.0356 Avg critic loss 1.8636\n",
      "Iteration 38/100, Avg Batch Final Reward: 0.0409\n",
      "Avg actor loss -0.0435 Avg critic loss 2.7948\n",
      "Iteration 39/100, Avg Batch Final Reward: 0.3793\n",
      "Avg actor loss -0.0327 Avg critic loss 1.3220\n",
      "Iteration 40/100, Avg Batch Final Reward: 0.3017\n",
      "Avg actor loss -0.0286 Avg critic loss 1.8296\n",
      "Iteration 41/100, Avg Batch Final Reward: 0.4070\n",
      "Avg actor loss -0.0242 Avg critic loss 0.7431\n",
      "Iteration 42/100, Avg Batch Final Reward: 0.4172\n",
      "Avg actor loss -0.0391 Avg critic loss 1.0508\n",
      "Iteration 43/100, Avg Batch Final Reward: 0.4573\n",
      "Avg actor loss -0.0284 Avg critic loss 0.9077\n",
      "Iteration 44/100, Avg Batch Final Reward: 0.6418\n",
      "Avg actor loss -0.0304 Avg critic loss 0.4505\n",
      "Iteration 45/100, Avg Batch Final Reward: 0.6175\n",
      "Avg actor loss -0.0341 Avg critic loss 0.5078\n",
      "Iteration 46/100, Avg Batch Final Reward: 0.6733\n",
      "Avg actor loss -0.0280 Avg critic loss 0.5422\n",
      "Iteration 47/100, Avg Batch Final Reward: 0.6879\n",
      "Avg actor loss -0.0427 Avg critic loss 0.2420\n",
      "Iteration 48/100, Avg Batch Final Reward: 0.7962\n",
      "Avg actor loss -0.0277 Avg critic loss 0.0968\n",
      "Iteration 49/100, Avg Batch Final Reward: 0.7017\n",
      "Avg actor loss -0.0292 Avg critic loss 0.7042\n",
      "Iteration 50/100, Avg Batch Final Reward: 0.8421\n",
      "Avg actor loss -0.0380 Avg critic loss 0.1528\n",
      "Iteration 51/100, Avg Batch Final Reward: 0.8423\n",
      "Avg actor loss -0.0302 Avg critic loss 0.0929\n",
      "Iteration 52/100, Avg Batch Final Reward: 0.8419\n",
      "Avg actor loss -0.0259 Avg critic loss 0.3267\n",
      "Iteration 53/100, Avg Batch Final Reward: 0.9113\n",
      "Avg actor loss -0.0296 Avg critic loss 0.0490\n",
      "Iteration 54/100, Avg Batch Final Reward: 0.8756\n",
      "Avg actor loss -0.0325 Avg critic loss 0.1015\n",
      "Iteration 55/100, Avg Batch Final Reward: 0.8922\n",
      "Avg actor loss -0.0336 Avg critic loss 0.0538\n",
      "Iteration 56/100, Avg Batch Final Reward: 0.9111\n",
      "Avg actor loss -0.0369 Avg critic loss 0.0262\n",
      "Iteration 57/100, Avg Batch Final Reward: 0.9238\n",
      "Avg actor loss -0.0290 Avg critic loss 0.0304\n",
      "Iteration 58/100, Avg Batch Final Reward: 0.9327\n",
      "Avg actor loss -0.0282 Avg critic loss 0.0196\n",
      "Iteration 59/100, Avg Batch Final Reward: 0.9084\n",
      "Avg actor loss -0.0429 Avg critic loss 0.0397\n",
      "Iteration 60/100, Avg Batch Final Reward: 0.9570\n",
      "Avg actor loss -0.0302 Avg critic loss 0.0120\n",
      "Iteration 61/100, Avg Batch Final Reward: 0.9483\n",
      "Avg actor loss -0.0296 Avg critic loss 0.0169\n",
      "Iteration 62/100, Avg Batch Final Reward: 0.9655\n",
      "Avg actor loss -0.0308 Avg critic loss 0.0041\n",
      "Iteration 63/100, Avg Batch Final Reward: 0.9571\n",
      "Avg actor loss -0.0373 Avg critic loss 0.0132\n",
      "Iteration 64/100, Avg Batch Final Reward: 0.9643\n",
      "Avg actor loss -0.0294 Avg critic loss 0.0106\n",
      "Iteration 65/100, Avg Batch Final Reward: 0.9396\n",
      "Avg actor loss -0.0301 Avg critic loss 0.0682\n",
      "Iteration 66/100, Avg Batch Final Reward: 0.9798\n",
      "Avg actor loss -0.0229 Avg critic loss 0.0044\n",
      "Iteration 67/100, Avg Batch Final Reward: 0.9844\n",
      "Avg actor loss -0.0235 Avg critic loss 0.0013\n",
      "Iteration 68/100, Avg Batch Final Reward: 0.9765\n",
      "Avg actor loss -0.0289 Avg critic loss 0.0024\n",
      "Iteration 69/100, Avg Batch Final Reward: 0.9852\n",
      "Avg actor loss -0.0322 Avg critic loss 0.0015\n",
      "Iteration 70/100, Avg Batch Final Reward: 0.9895\n",
      "Avg actor loss -0.0298 Avg critic loss 0.0006\n",
      "Iteration 71/100, Avg Batch Final Reward: 0.9706\n",
      "Avg actor loss -0.0254 Avg critic loss 0.0096\n",
      "Iteration 72/100, Avg Batch Final Reward: 0.9704\n",
      "Avg actor loss -0.0200 Avg critic loss 0.0128\n",
      "Iteration 73/100, Avg Batch Final Reward: 0.9870\n",
      "Avg actor loss -0.0444 Avg critic loss 0.0009\n",
      "Iteration 74/100, Avg Batch Final Reward: 0.9880\n",
      "Avg actor loss -0.0344 Avg critic loss 0.0009\n",
      "Iteration 75/100, Avg Batch Final Reward: 0.9900\n",
      "Avg actor loss -0.0459 Avg critic loss 0.0005\n",
      "Iteration 76/100, Avg Batch Final Reward: 0.9888\n",
      "Avg actor loss -0.0269 Avg critic loss 0.0004\n",
      "Iteration 77/100, Avg Batch Final Reward: 0.9869\n",
      "Avg actor loss -0.0282 Avg critic loss 0.0016\n",
      "Iteration 78/100, Avg Batch Final Reward: 0.9927\n",
      "Avg actor loss -0.0238 Avg critic loss 0.0004\n",
      "Iteration 79/100, Avg Batch Final Reward: 0.9901\n",
      "Avg actor loss -0.0350 Avg critic loss 0.0010\n",
      "Iteration 80/100, Avg Batch Final Reward: 0.9902\n",
      "Avg actor loss -0.0301 Avg critic loss 0.0011\n",
      "Iteration 81/100, Avg Batch Final Reward: 0.9908\n",
      "Avg actor loss -0.0287 Avg critic loss 0.0011\n",
      "Iteration 82/100, Avg Batch Final Reward: 0.9927\n",
      "Avg actor loss -0.0328 Avg critic loss 0.0003\n",
      "Iteration 83/100, Avg Batch Final Reward: 0.9941\n",
      "Avg actor loss -0.0196 Avg critic loss 0.0002\n",
      "Iteration 84/100, Avg Batch Final Reward: 0.9963\n",
      "Avg actor loss -0.0279 Avg critic loss 0.0001\n",
      "Iteration 85/100, Avg Batch Final Reward: 0.9957\n",
      "Avg actor loss -0.0233 Avg critic loss 0.0002\n",
      "Iteration 86/100, Avg Batch Final Reward: 0.9928\n",
      "Avg actor loss -0.0246 Avg critic loss 0.0004\n",
      "Iteration 87/100, Avg Batch Final Reward: 0.9914\n",
      "Avg actor loss -0.0312 Avg critic loss 0.0009\n",
      "Iteration 88/100, Avg Batch Final Reward: 0.9957\n",
      "Avg actor loss -0.0245 Avg critic loss 0.0001\n",
      "Iteration 89/100, Avg Batch Final Reward: 0.9953\n",
      "Avg actor loss -0.0170 Avg critic loss 0.0002\n",
      "Iteration 90/100, Avg Batch Final Reward: 0.9952\n",
      "Avg actor loss -0.0216 Avg critic loss 0.0002\n",
      "Iteration 91/100, Avg Batch Final Reward: 0.9944\n",
      "Avg actor loss -0.0337 Avg critic loss 0.0003\n",
      "Iteration 92/100, Avg Batch Final Reward: 0.9965\n",
      "Avg actor loss -0.0341 Avg critic loss 0.0001\n",
      "Iteration 93/100, Avg Batch Final Reward: 0.9942\n",
      "Avg actor loss -0.0311 Avg critic loss 0.0005\n",
      "Iteration 94/100, Avg Batch Final Reward: 0.9963\n",
      "Avg actor loss -0.0263 Avg critic loss 0.0001\n",
      "Iteration 95/100, Avg Batch Final Reward: 0.9967\n",
      "Avg actor loss -0.0173 Avg critic loss 0.0001\n",
      "Iteration 96/100, Avg Batch Final Reward: 0.9969\n",
      "Avg actor loss -0.0189 Avg critic loss 0.0001\n",
      "Iteration 97/100, Avg Batch Final Reward: 0.9969\n",
      "Avg actor loss -0.0228 Avg critic loss 0.0001\n",
      "Iteration 98/100, Avg Batch Final Reward: 0.9962\n",
      "Avg actor loss -0.0240 Avg critic loss 0.0001\n",
      "Iteration 99/100, Avg Batch Final Reward: 0.9959\n",
      "Avg actor loss -0.0271 Avg critic loss 0.0002\n",
      "Iteration 100/100, Avg Batch Final Reward: 0.9972\n",
      "Avg actor loss -0.0343 Avg critic loss 0.0001\n",
      "Model saved at iteration 100\n",
      "Final models saved to output/ppo-refinement/ppo_sandbox/\n",
      "Average final rewards per iteration saved at output/ppo-refinement/ppo_sandbox/avg_final_rewards_per_iteration.npy\n",
      "Training finished.\n",
      "PPO training finished. Rewards log saved to output/ppo-refinement/ppo_sandbox/\n"
     ]
    }
   ],
   "source": [
    "print('--- Begin PPO refinement strategy')\n",
    "\n",
    "configs = ConfigParser()\n",
    "configs.read('configfile.ini')\n",
    "output_path = configs['PATHS']['output_path']\n",
    "this_savepath = f'output/ppo-refinement/ppo_sandbox/' \n",
    "os.makedirs(this_savepath, exist_ok=True)\n",
    "\n",
    "ppo_agent = PPORefinement(\n",
    "    param_dim=10,\n",
    "    noise_dim=10,\n",
    "    reward_function=compute_reward,\n",
    "    min_x_bounds=-10,\n",
    "    max_x_bounds=10,\n",
    "    ppo_epochs=10,\n",
    "    T_horizon=1,\n",
    "    actor_lr=1e-5,\n",
    "    critic_lr=5e-5,\n",
    "    n_trajectories=64,\n",
    ")\n",
    "\n",
    "trained_actor, rewards = ppo_agent.train(num_training_iterations=100, output_path=this_savepath)\n",
    "\n",
    "print(f\"PPO training finished. Rewards log saved to {this_savepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb5ef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'PPO Training Rewards')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Final Reward')\n",
    "plt.title('PPO Training Rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bd02e",
   "metadata": {},
   "source": [
    "# New implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "253bc15f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "GlobalHydra is already initialized, call GlobalHydra.instance().clear() if you want to re-initialize",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3e12d5b88844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Hydra-style manual config loading in a notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"configs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/hydra/initialize.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config_path, job_name, caller_stack_depth, version_base)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mcalling_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalling_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/hydra/_internal/hydra.py\u001b[0m in \u001b[0;36mcreate_main_hydra_file_or_module\u001b[0;34m(cls, calling_file, calling_module, config_path, job_name)\u001b[0m\n\u001b[1;32m     51\u001b[0m         )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mHydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_main_hydra2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_search_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/hydra/_internal/hydra.py\u001b[0m in \u001b[0;36mcreate_main_hydra2\u001b[0;34m(cls, task_name, config_search_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_hydra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalHydra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mGlobalHydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhydra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/hydra/core/global_hydra.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, hydra)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             raise ValueError(\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0;34m\"GlobalHydra is already initialized, call GlobalHydra.instance().clear() if you want to re-initialize\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             )\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: GlobalHydra is already initialized, call GlobalHydra.instance().clear() if you want to re-initialize"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from renaissance.kinetics.jacobian_solver import check_jacobian\n",
    "\n",
    "from helpers.ppo_agent import PPOAgent\n",
    "from helpers.env import KineticEnv\n",
    "from helpers.utils import reward_func, load_pkl\n",
    "import torch\n",
    "\n",
    "# Hydra-style manual config loading in a notebook\n",
    "initialize(config_path=\"configs\", version_base=\"1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea57798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"train.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b868e67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "method:\n",
      "  parameter_dim: 384\n",
      "  latent_dim: 99\n",
      "  name: ppo_refinement\n",
      "  actor_lr: 1.0e-06\n",
      "  critic_lr: 1.0e-05\n",
      "  discount_factor: 0.99\n",
      "  gae_lambda: 0.98\n",
      "  clip_eps: 0.2\n",
      "  value_loss_weight: 0.5\n",
      "  entropy_loss_weight: 0.0\n",
      "  scale_action: 0.1\n",
      "seed: 42\n",
      "device: cpu\n",
      "logger:\n",
      "  project: rl-renaissance\n",
      "  entity: ludekcizinsky\n",
      "  tags:\n",
      "  - dev\n",
      "paths:\n",
      "  names_km: data/varma_ecoli_shikki/parameter_names_km_fdp1.pkl\n",
      "  output_dir: /home/renaissance/work/output\n",
      "  met_model_name: varma_ecoli_shikki\n",
      "constraints:\n",
      "  min_km: -25\n",
      "  max_km: 3\n",
      "  ss_idx: 1712\n",
      "reward:\n",
      "  eig_partition: -2.5\n",
      "env:\n",
      "  p0_init_mean: 0\n",
      "  p0_init_std: 0.1\n",
      "  p_size: 384\n",
      "training:\n",
      "  num_episodes: 100\n",
      "  max_steps_per_episode: 10\n",
      "  num_trajectories_per_episode: 1\n",
      "  n_dist_samples: 20\n",
      "  batch_size: 25\n",
      "  num_epochs: 10\n",
      "  max_grad_norm: 0.5\n",
      "\n",
      "--------------------------------------------------\n",
      "FYI: Loading kinetic and thermodynamic data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 19:32:09,329 - thermomodel_new - INFO - # Model initialized with units kcal/mol and temperature 298.15 K\n",
      "2025-05-18 19:32:10,713 - Unnamed - WARNING - Non integer stoichiometries found ['CYTBO3_4pp', 'LMPD_biomass_c_1_420', 'CYTBDpp'] change to integer for linear dependencies\n",
      "2025-05-18 19:32:11,135 - Unnamed - WARNING - Non integer stoichiometries found ['CYTBO3_4pp', 'LMPD_biomass_c_1_420', 'CYTBDpp'] change to integer for linear dependencies\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 50)\n",
    "print(OmegaConf.to_yaml(cfg))  # print config to verify\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Call solvers from SKimPy\n",
    "chk_jcbn = check_jacobian()\n",
    "\n",
    "# Integrate data\n",
    "print(\"FYI: Loading kinetic and thermodynamic data.\")\n",
    "chk_jcbn._load_ktmodels(cfg.paths.met_model_name, 'fdp1') # Load kinetic and thermodynamic data\n",
    "chk_jcbn._load_ssprofile(cfg.paths.met_model_name, 'fdp1', cfg.constraints.ss_idx) # Integrate steady state information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2249cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger setup, todo: for now disabled, else we would get w&b run object\n",
    "from functools import partial\n",
    "\n",
    "logger = None # get_logger(cfg)\n",
    "\n",
    "# Initialize environment\n",
    "names_km = load_pkl(cfg.paths.names_km)\n",
    "reward_fn = partial(reward_func, chk_jcbn, names_km, cfg.reward.eig_partition)\n",
    "# reward_fn = compute_reward\n",
    "env = KineticEnv(cfg, reward_fn)\n",
    "env.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4939e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100 - Min reward: 0.0000, Max reward: 0.0732, Mean reward: 0.0073, Last reward: 0.0732\n",
      "Episode 1/100 - Policy loss: -0.1746, Value loss: 0.7142, Entropy: 352.8746\n",
      "Best model saved at episode 1 with mean reward 0.0732\n",
      "Episode 2/100 - Min reward: 0.0000, Max reward: 0.0202, Mean reward: 0.0021, Last reward: 0.0202\n",
      "Episode 2/100 - Policy loss: -0.1745, Value loss: 0.2476, Entropy: 352.8780\n",
      "Episode 3/100 - Min reward: 0.0001, Max reward: 0.1374, Mean reward: 0.0138, Last reward: 0.1374\n",
      "Episode 3/100 - Policy loss: -0.1744, Value loss: 0.2777, Entropy: 352.8813\n",
      "Best model saved at episode 3 with mean reward 0.1374\n",
      "Episode 4/100 - Min reward: 0.0000, Max reward: 0.0648, Mean reward: 0.0065, Last reward: 0.0648\n",
      "Episode 4/100 - Policy loss: -0.1733, Value loss: 0.2109, Entropy: 352.8845\n",
      "Episode 5/100 - Min reward: 0.0000, Max reward: 0.1828, Mean reward: 0.0183, Last reward: 0.1828\n",
      "Episode 5/100 - Policy loss: -0.1703, Value loss: 0.0388, Entropy: 352.8879\n",
      "Best model saved at episode 5 with mean reward 0.1828\n",
      "Episode 6/100 - Min reward: 0.0000, Max reward: 0.1603, Mean reward: 0.0161, Last reward: 0.1603\n",
      "Episode 6/100 - Policy loss: -0.1706, Value loss: 0.0411, Entropy: 352.8911\n",
      "Episode 7/100 - Min reward: 0.0000, Max reward: 0.2286, Mean reward: 0.0230, Last reward: 0.2286\n",
      "Episode 7/100 - Policy loss: -0.1694, Value loss: 0.0116, Entropy: 352.8943\n",
      "Best model saved at episode 7 with mean reward 0.2286\n",
      "Episode 8/100 - Min reward: 0.0000, Max reward: 0.0124, Mean reward: 0.0014, Last reward: 0.0124\n",
      "Episode 8/100 - Policy loss: -0.1689, Value loss: 0.0119, Entropy: 352.8975\n",
      "Episode 9/100 - Min reward: 0.0000, Max reward: 0.2006, Mean reward: 0.0201, Last reward: 0.2006\n",
      "Episode 9/100 - Policy loss: -0.1592, Value loss: 0.0104, Entropy: 352.9006\n",
      "Episode 10/100 - Min reward: 0.0000, Max reward: 0.0913, Mean reward: 0.0092, Last reward: 0.0913\n",
      "Episode 10/100 - Policy loss: -0.1658, Value loss: 0.0364, Entropy: 352.9038\n",
      "Episode 11/100 - Min reward: 0.0000, Max reward: 0.0600, Mean reward: 0.0061, Last reward: 0.0600\n",
      "Episode 11/100 - Policy loss: -0.1710, Value loss: 0.0041, Entropy: 352.9070\n",
      "Episode 12/100 - Min reward: 0.0000, Max reward: 0.2063, Mean reward: 0.0207, Last reward: 0.2063\n",
      "Episode 12/100 - Policy loss: -0.1633, Value loss: 0.0027, Entropy: 352.9101\n",
      "Episode 13/100 - Min reward: 0.0000, Max reward: 0.0581, Mean reward: 0.0059, Last reward: 0.0581\n",
      "Episode 13/100 - Policy loss: -0.1643, Value loss: 0.0093, Entropy: 352.9132\n",
      "Episode 14/100 - Min reward: 0.0000, Max reward: 0.1319, Mean reward: 0.0132, Last reward: 0.1319\n",
      "Episode 14/100 - Policy loss: -0.1722, Value loss: 0.0520, Entropy: 352.9164\n",
      "Episode 15/100 - Min reward: 0.0000, Max reward: 0.3470, Mean reward: 0.0347, Last reward: 0.3470\n",
      "Episode 15/100 - Policy loss: -0.1478, Value loss: 0.0017, Entropy: 352.9195\n",
      "Best model saved at episode 15 with mean reward 0.3470\n",
      "Episode 16/100 - Min reward: 0.0000, Max reward: 0.0983, Mean reward: 0.0099, Last reward: 0.0983\n",
      "Episode 16/100 - Policy loss: -0.1364, Value loss: 0.0084, Entropy: 352.9226\n",
      "Episode 17/100 - Min reward: 0.0000, Max reward: 0.0841, Mean reward: 0.0085, Last reward: 0.0841\n",
      "Episode 17/100 - Policy loss: -0.1794, Value loss: 0.0031, Entropy: 352.9256\n",
      "Episode 18/100 - Min reward: 0.0000, Max reward: 0.2775, Mean reward: 0.0278, Last reward: 0.2775\n",
      "Episode 18/100 - Policy loss: -0.1719, Value loss: 0.0068, Entropy: 352.9290\n",
      "Episode 19/100 - Min reward: 0.0001, Max reward: 0.1221, Mean reward: 0.0123, Last reward: 0.1221\n",
      "Episode 19/100 - Policy loss: -0.1798, Value loss: 0.0033, Entropy: 352.9322\n",
      "Episode 20/100 - Min reward: 0.0000, Max reward: 0.0463, Mean reward: 0.0046, Last reward: 0.0463\n",
      "Episode 20/100 - Policy loss: -0.1437, Value loss: 0.0024, Entropy: 352.9355\n",
      "Episode 21/100 - Min reward: 0.0000, Max reward: 0.0437, Mean reward: 0.0044, Last reward: 0.0437\n",
      "Episode 21/100 - Policy loss: -0.1652, Value loss: 0.0483, Entropy: 352.9387\n",
      "Episode 22/100 - Min reward: 0.0000, Max reward: 0.0591, Mean reward: 0.0059, Last reward: 0.0591\n",
      "Episode 22/100 - Policy loss: -0.1708, Value loss: 0.0012, Entropy: 352.9419\n",
      "Episode 23/100 - Min reward: 0.0000, Max reward: 0.0648, Mean reward: 0.0065, Last reward: 0.0648\n",
      "Episode 23/100 - Policy loss: -0.1818, Value loss: 0.0003, Entropy: 352.9450\n",
      "Episode 24/100 - Min reward: 0.0000, Max reward: 0.0850, Mean reward: 0.0086, Last reward: 0.0850\n",
      "Episode 24/100 - Policy loss: -0.1419, Value loss: 0.0011, Entropy: 352.9480\n",
      "Episode 25/100 - Min reward: 0.0001, Max reward: 0.0493, Mean reward: 0.0050, Last reward: 0.0493\n",
      "Episode 25/100 - Policy loss: -0.1491, Value loss: 0.0008, Entropy: 352.9510\n",
      "Episode 26/100 - Min reward: 0.0000, Max reward: 0.0838, Mean reward: 0.0084, Last reward: 0.0838\n",
      "Episode 26/100 - Policy loss: -0.1714, Value loss: 0.1110, Entropy: 352.9543\n",
      "Episode 27/100 - Min reward: 0.0000, Max reward: 0.1103, Mean reward: 0.0111, Last reward: 0.1103\n",
      "Episode 27/100 - Policy loss: -0.1681, Value loss: 0.0083, Entropy: 352.9576\n",
      "Episode 28/100 - Min reward: 0.0000, Max reward: 0.2205, Mean reward: 0.0221, Last reward: 0.2205\n",
      "Episode 28/100 - Policy loss: -0.1690, Value loss: 0.0093, Entropy: 352.9609\n",
      "Episode 29/100 - Min reward: 0.0000, Max reward: 0.0432, Mean reward: 0.0043, Last reward: 0.0432\n",
      "Episode 29/100 - Policy loss: -0.1664, Value loss: 0.0918, Entropy: 352.9641\n",
      "Episode 30/100 - Min reward: 0.0000, Max reward: 0.0010, Mean reward: 0.0001, Last reward: 0.0010\n",
      "Episode 30/100 - Policy loss: -0.1775, Value loss: 0.0170, Entropy: 352.9675\n",
      "Episode 31/100 - Min reward: 0.0000, Max reward: 0.0774, Mean reward: 0.0078, Last reward: 0.0774\n",
      "Episode 31/100 - Policy loss: -0.1603, Value loss: 0.0102, Entropy: 352.9707\n",
      "Episode 32/100 - Min reward: 0.0000, Max reward: 0.0748, Mean reward: 0.0075, Last reward: 0.0748\n",
      "Episode 32/100 - Policy loss: -0.1749, Value loss: 0.0521, Entropy: 352.9737\n",
      "Episode 33/100 - Min reward: 0.0000, Max reward: 0.1062, Mean reward: 0.0106, Last reward: 0.1062\n",
      "Episode 33/100 - Policy loss: -0.1726, Value loss: 0.0967, Entropy: 352.9771\n",
      "Episode 34/100 - Min reward: 0.0000, Max reward: 0.0758, Mean reward: 0.0076, Last reward: 0.0758\n",
      "Episode 34/100 - Policy loss: -0.1586, Value loss: 0.0110, Entropy: 352.9804\n",
      "Episode 35/100 - Min reward: 0.0000, Max reward: 0.0221, Mean reward: 0.0022, Last reward: 0.0221\n",
      "Episode 35/100 - Policy loss: -0.1806, Value loss: 0.0010, Entropy: 352.9836\n",
      "Episode 36/100 - Min reward: 0.0000, Max reward: 0.0760, Mean reward: 0.0076, Last reward: 0.0760\n",
      "Episode 36/100 - Policy loss: -0.1428, Value loss: 0.0009, Entropy: 352.9868\n",
      "Episode 37/100 - Min reward: 0.0000, Max reward: 0.0801, Mean reward: 0.0081, Last reward: 0.0801\n",
      "Episode 37/100 - Policy loss: -0.1536, Value loss: 0.0753, Entropy: 352.9902\n",
      "Episode 38/100 - Min reward: 0.0000, Max reward: 0.0483, Mean reward: 0.0049, Last reward: 0.0483\n",
      "Episode 38/100 - Policy loss: -0.1634, Value loss: 0.1167, Entropy: 352.9934\n",
      "Episode 39/100 - Min reward: 0.0001, Max reward: 0.0463, Mean reward: 0.0047, Last reward: 0.0463\n",
      "Episode 39/100 - Policy loss: -0.1652, Value loss: 0.0042, Entropy: 352.9967\n",
      "Episode 40/100 - Min reward: 0.0000, Max reward: 0.0560, Mean reward: 0.0056, Last reward: 0.0560\n",
      "Episode 40/100 - Policy loss: -0.1612, Value loss: 0.0084, Entropy: 352.9999\n",
      "Episode 41/100 - Min reward: 0.0000, Max reward: 0.0370, Mean reward: 0.0037, Last reward: 0.0370\n",
      "Episode 41/100 - Policy loss: -0.1741, Value loss: 0.0060, Entropy: 353.0029\n",
      "Episode 42/100 - Min reward: 0.0000, Max reward: 0.0787, Mean reward: 0.0079, Last reward: 0.0787\n",
      "Episode 42/100 - Policy loss: -0.1759, Value loss: 0.1076, Entropy: 353.0059\n",
      "Episode 43/100 - Min reward: 0.0000, Max reward: 0.0010, Mean reward: 0.0001, Last reward: 0.0010\n",
      "Episode 43/100 - Policy loss: -0.1548, Value loss: 0.0070, Entropy: 353.0092\n",
      "Episode 44/100 - Min reward: 0.0000, Max reward: 0.0295, Mean reward: 0.0029, Last reward: 0.0295\n",
      "Episode 44/100 - Policy loss: -0.1667, Value loss: 0.0669, Entropy: 353.0125\n",
      "Episode 45/100 - Min reward: 0.0000, Max reward: 0.0305, Mean reward: 0.0031, Last reward: 0.0305\n",
      "Episode 45/100 - Policy loss: -0.1731, Value loss: 0.0556, Entropy: 353.0158\n",
      "Episode 46/100 - Min reward: 0.0000, Max reward: 0.0936, Mean reward: 0.0094, Last reward: 0.0936\n",
      "Episode 46/100 - Policy loss: -0.1662, Value loss: 0.0479, Entropy: 353.0190\n",
      "Episode 47/100 - Min reward: 0.0000, Max reward: 0.0816, Mean reward: 0.0083, Last reward: 0.0816\n",
      "Episode 47/100 - Policy loss: -0.1735, Value loss: 0.0127, Entropy: 353.0224\n",
      "Episode 48/100 - Min reward: 0.0000, Max reward: 0.0815, Mean reward: 0.0082, Last reward: 0.0815\n",
      "Episode 48/100 - Policy loss: -0.1742, Value loss: 0.0022, Entropy: 353.0256\n",
      "Episode 49/100 - Min reward: 0.0000, Max reward: 0.0795, Mean reward: 0.0080, Last reward: 0.0795\n",
      "Episode 49/100 - Policy loss: -0.1663, Value loss: 0.0094, Entropy: 353.0288\n",
      "Episode 50/100 - Min reward: 0.0000, Max reward: 0.0880, Mean reward: 0.0088, Last reward: 0.0880\n",
      "Episode 50/100 - Policy loss: -0.1477, Value loss: 0.0020, Entropy: 353.0320\n",
      "Episode 51/100 - Min reward: 0.0000, Max reward: 0.1098, Mean reward: 0.0110, Last reward: 0.1098\n",
      "Episode 51/100 - Policy loss: -0.1800, Value loss: 0.0048, Entropy: 353.0354\n",
      "Episode 52/100 - Min reward: 0.0000, Max reward: 0.0798, Mean reward: 0.0080, Last reward: 0.0798\n",
      "Episode 52/100 - Policy loss: -0.1847, Value loss: 0.0053, Entropy: 353.0387\n",
      "Episode 53/100 - Min reward: 0.0000, Max reward: 0.0401, Mean reward: 0.0040, Last reward: 0.0401\n",
      "Episode 53/100 - Policy loss: -0.1783, Value loss: 0.0081, Entropy: 353.0419\n",
      "Episode 54/100 - Min reward: 0.0000, Max reward: 0.0255, Mean reward: 0.0026, Last reward: 0.0255\n",
      "Episode 54/100 - Policy loss: -0.1848, Value loss: 0.0038, Entropy: 353.0450\n",
      "Episode 55/100 - Min reward: 0.0000, Max reward: 0.0791, Mean reward: 0.0079, Last reward: 0.0791\n",
      "Episode 55/100 - Policy loss: -0.1385, Value loss: 0.0021, Entropy: 353.0482\n",
      "Episode 56/100 - Min reward: 0.0000, Max reward: 0.0301, Mean reward: 0.0030, Last reward: 0.0301\n",
      "Episode 56/100 - Policy loss: -0.1614, Value loss: 0.0439, Entropy: 353.0515\n",
      "Episode 57/100 - Min reward: 0.0000, Max reward: 0.0337, Mean reward: 0.0034, Last reward: 0.0337\n",
      "Episode 57/100 - Policy loss: -0.1768, Value loss: 0.0683, Entropy: 353.0547\n",
      "Episode 58/100 - Min reward: 0.0000, Max reward: 0.0163, Mean reward: 0.0016, Last reward: 0.0163\n",
      "Episode 58/100 - Policy loss: -0.1643, Value loss: 0.0189, Entropy: 353.0578\n",
      "Episode 59/100 - Min reward: 0.0000, Max reward: 0.0735, Mean reward: 0.0074, Last reward: 0.0735\n",
      "Episode 59/100 - Policy loss: -0.1579, Value loss: 0.0080, Entropy: 353.0610\n",
      "Episode 60/100 - Min reward: 0.0000, Max reward: 0.0800, Mean reward: 0.0080, Last reward: 0.0800\n",
      "Episode 60/100 - Policy loss: -0.1704, Value loss: 0.0112, Entropy: 353.0641\n",
      "Episode 61/100 - Min reward: 0.0000, Max reward: 0.0678, Mean reward: 0.0068, Last reward: 0.0678\n",
      "Episode 61/100 - Policy loss: -0.1780, Value loss: 0.0246, Entropy: 353.0676\n",
      "Episode 62/100 - Min reward: 0.0000, Max reward: 0.0989, Mean reward: 0.0099, Last reward: 0.0989\n",
      "Episode 62/100 - Policy loss: -0.1736, Value loss: 0.0050, Entropy: 353.0708\n",
      "Episode 63/100 - Min reward: 0.0000, Max reward: 0.0871, Mean reward: 0.0088, Last reward: 0.0871\n",
      "Episode 63/100 - Policy loss: -0.1653, Value loss: 0.0064, Entropy: 353.0741\n",
      "Episode 64/100 - Min reward: 0.0000, Max reward: 0.0096, Mean reward: 0.0010, Last reward: 0.0096\n",
      "Episode 64/100 - Policy loss: -0.1720, Value loss: 0.0246, Entropy: 353.0773\n",
      "Episode 65/100 - Min reward: 0.0000, Max reward: 0.0546, Mean reward: 0.0055, Last reward: 0.0546\n",
      "Episode 65/100 - Policy loss: -0.1509, Value loss: 0.0011, Entropy: 353.0803\n",
      "Episode 66/100 - Min reward: 0.0000, Max reward: 0.0561, Mean reward: 0.0056, Last reward: 0.0561\n",
      "Episode 66/100 - Policy loss: -0.1758, Value loss: 0.0527, Entropy: 353.0833\n",
      "Episode 67/100 - Min reward: 0.0000, Max reward: 0.0802, Mean reward: 0.0081, Last reward: 0.0802\n",
      "Episode 67/100 - Policy loss: -0.1476, Value loss: 0.0659, Entropy: 353.0864\n",
      "Episode 68/100 - Min reward: 0.0000, Max reward: 0.0717, Mean reward: 0.0072, Last reward: 0.0717\n",
      "Episode 68/100 - Policy loss: -0.1670, Value loss: 0.0184, Entropy: 353.0896\n",
      "Episode 69/100 - Min reward: 0.0000, Max reward: 0.0877, Mean reward: 0.0088, Last reward: 0.0877\n",
      "Episode 69/100 - Policy loss: -0.1657, Value loss: 0.0173, Entropy: 353.0927\n",
      "Episode 70/100 - Min reward: 0.0000, Max reward: 0.0830, Mean reward: 0.0083, Last reward: 0.0830\n",
      "Episode 70/100 - Policy loss: -0.1771, Value loss: 0.0021, Entropy: 353.0959\n",
      "Episode 71/100 - Min reward: 0.0000, Max reward: 0.0796, Mean reward: 0.0080, Last reward: 0.0796\n",
      "Episode 71/100 - Policy loss: -0.1714, Value loss: 0.0401, Entropy: 353.0991\n",
      "Episode 72/100 - Min reward: 0.0000, Max reward: 0.0997, Mean reward: 0.0100, Last reward: 0.0997\n",
      "Episode 72/100 - Policy loss: -0.1565, Value loss: 0.0040, Entropy: 353.1023\n",
      "Episode 73/100 - Min reward: 0.0001, Max reward: 0.0899, Mean reward: 0.0091, Last reward: 0.0899\n",
      "Episode 73/100 - Policy loss: -0.1543, Value loss: 0.0044, Entropy: 353.1054\n",
      "Episode 74/100 - Min reward: 0.0000, Max reward: 0.0782, Mean reward: 0.0079, Last reward: 0.0782\n",
      "Episode 74/100 - Policy loss: -0.1544, Value loss: 0.0003, Entropy: 353.1086\n",
      "Episode 75/100 - Min reward: 0.0000, Max reward: 0.0814, Mean reward: 0.0082, Last reward: 0.0814\n",
      "Episode 75/100 - Policy loss: -0.1630, Value loss: 0.0055, Entropy: 353.1118\n",
      "Episode 76/100 - Min reward: 0.0000, Max reward: 0.0409, Mean reward: 0.0041, Last reward: 0.0409\n",
      "Episode 76/100 - Policy loss: -0.1725, Value loss: 0.0062, Entropy: 353.1150\n",
      "Episode 77/100 - Min reward: 0.0000, Max reward: 0.0505, Mean reward: 0.0051, Last reward: 0.0505\n",
      "Episode 77/100 - Policy loss: -0.1848, Value loss: 0.0080, Entropy: 353.1183\n",
      "Episode 78/100 - Min reward: 0.0000, Max reward: 0.0781, Mean reward: 0.0079, Last reward: 0.0781\n",
      "Episode 78/100 - Policy loss: -0.1623, Value loss: 0.0096, Entropy: 353.1216\n",
      "Episode 79/100 - Min reward: 0.0000, Max reward: 0.1263, Mean reward: 0.0127, Last reward: 0.1263\n",
      "Episode 79/100 - Policy loss: -0.1615, Value loss: 0.0120, Entropy: 353.1251\n",
      "Episode 80/100 - Min reward: 0.0000, Max reward: 0.0815, Mean reward: 0.0082, Last reward: 0.0815\n",
      "Episode 80/100 - Policy loss: -0.1836, Value loss: 0.0007, Entropy: 353.1284\n",
      "Episode 81/100 - Min reward: 0.0000, Max reward: 0.0291, Mean reward: 0.0029, Last reward: 0.0291\n",
      "Episode 81/100 - Policy loss: -0.1593, Value loss: 0.0030, Entropy: 353.1316\n",
      "Episode 82/100 - Min reward: 0.0000, Max reward: 0.0803, Mean reward: 0.0081, Last reward: 0.0803\n",
      "Episode 82/100 - Policy loss: -0.1783, Value loss: 0.0457, Entropy: 353.1349\n",
      "Episode 83/100 - Min reward: 0.0000, Max reward: 0.0010, Mean reward: 0.0001, Last reward: 0.0010\n",
      "Episode 83/100 - Policy loss: -0.1628, Value loss: 0.0475, Entropy: 353.1382\n",
      "Episode 84/100 - Min reward: 0.0000, Max reward: 0.0452, Mean reward: 0.0046, Last reward: 0.0452\n",
      "Episode 84/100 - Policy loss: -0.1455, Value loss: 0.0027, Entropy: 353.1413\n",
      "Episode 85/100 - Min reward: 0.0000, Max reward: 0.0742, Mean reward: 0.0074, Last reward: 0.0742\n",
      "Episode 85/100 - Policy loss: -0.1640, Value loss: 0.0125, Entropy: 353.1444\n",
      "Episode 86/100 - Min reward: 0.0000, Max reward: 0.0894, Mean reward: 0.0090, Last reward: 0.0894\n",
      "Episode 86/100 - Policy loss: -0.1651, Value loss: 0.0013, Entropy: 353.1475\n",
      "Episode 87/100 - Min reward: 0.0000, Max reward: 0.0965, Mean reward: 0.0097, Last reward: 0.0965\n",
      "Episode 87/100 - Policy loss: -0.1447, Value loss: 0.0021, Entropy: 353.1507\n",
      "Episode 88/100 - Min reward: 0.0000, Max reward: 0.0611, Mean reward: 0.0061, Last reward: 0.0611\n",
      "Episode 88/100 - Policy loss: -0.1585, Value loss: 0.0040, Entropy: 353.1538\n",
      "Episode 89/100 - Min reward: 0.0000, Max reward: 0.1138, Mean reward: 0.0114, Last reward: 0.1138\n",
      "Episode 89/100 - Policy loss: -0.1910, Value loss: 0.0220, Entropy: 353.1572\n",
      "Episode 90/100 - Min reward: 0.0000, Max reward: 0.0777, Mean reward: 0.0078, Last reward: 0.0777\n",
      "Episode 90/100 - Policy loss: -0.1617, Value loss: 0.0561, Entropy: 353.1606\n",
      "Episode 91/100 - Min reward: 0.0000, Max reward: 0.1017, Mean reward: 0.0102, Last reward: 0.1017\n",
      "Episode 91/100 - Policy loss: -0.1491, Value loss: 0.0019, Entropy: 353.1638\n",
      "Episode 92/100 - Min reward: 0.0000, Max reward: 0.0977, Mean reward: 0.0098, Last reward: 0.0977\n",
      "Episode 92/100 - Policy loss: -0.1797, Value loss: 0.0476, Entropy: 353.1672\n",
      "Episode 93/100 - Min reward: 0.0000, Max reward: 0.0011, Mean reward: 0.0002, Last reward: 0.0011\n",
      "Episode 93/100 - Policy loss: -0.1773, Value loss: 0.0355, Entropy: 353.1704\n",
      "Episode 94/100 - Min reward: 0.0000, Max reward: 0.0834, Mean reward: 0.0083, Last reward: 0.0834\n",
      "Episode 94/100 - Policy loss: -0.1425, Value loss: 0.0018, Entropy: 353.1737\n",
      "Episode 95/100 - Min reward: 0.0000, Max reward: 0.0020, Mean reward: 0.0002, Last reward: 0.0020\n",
      "Episode 95/100 - Policy loss: -0.1711, Value loss: 0.0576, Entropy: 353.1768\n",
      "Episode 96/100 - Min reward: 0.0000, Max reward: 0.0305, Mean reward: 0.0031, Last reward: 0.0305\n",
      "Episode 96/100 - Policy loss: -0.1705, Value loss: 0.0016, Entropy: 353.1800\n",
      "Episode 97/100 - Min reward: 0.0002, Max reward: 0.3987, Mean reward: 0.0401, Last reward: 0.3987\n",
      "Episode 97/100 - Policy loss: -0.1633, Value loss: 0.0157, Entropy: 353.1831\n",
      "Best model saved at episode 97 with mean reward 0.3987\n",
      "Episode 98/100 - Min reward: 0.0000, Max reward: 0.0881, Mean reward: 0.0089, Last reward: 0.0881\n",
      "Episode 98/100 - Policy loss: -0.1693, Value loss: 0.0005, Entropy: 353.1866\n",
      "Episode 99/100 - Min reward: 0.0000, Max reward: 0.0809, Mean reward: 0.0081, Last reward: 0.0809\n",
      "Episode 99/100 - Policy loss: -0.1523, Value loss: 0.0022, Entropy: 353.1899\n",
      "Episode 100/100 - Min reward: 0.0000, Max reward: 0.0632, Mean reward: 0.0063, Last reward: 0.0632\n",
      "Episode 100/100 - Policy loss: -0.1768, Value loss: 0.0026, Entropy: 353.1929\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize PPO agent (actor and critic)\n",
    "ppo_agent = PPOAgent(cfg, logger)\n",
    "\n",
    "log_rewards = []\n",
    "best_last_reward = 0\n",
    "best_actor_path = f\"{cfg.paths.output_dir}/best_actor.pth\"\n",
    "best_critic_path = f\"{cfg.paths.output_dir}/best_critic.pth\"\n",
    "\n",
    "for episode in range(cfg.training.num_episodes):\n",
    "    # Collect multiple trajectories per episode\n",
    "    trajectories = []\n",
    "    for _ in range(cfg.training.num_trajectories_per_episode):\n",
    "        trajectory = ppo_agent.collect_trajectory(env)\n",
    "        trajectories.append(trajectory)\n",
    "    \n",
    "    # Combine all trajectories\n",
    "    combined_trajectory = {\n",
    "        \"states\": torch.cat([t[\"states\"] for t in trajectories]),\n",
    "        \"actions\": torch.cat([t[\"actions\"] for t in trajectories]),\n",
    "        \"log_probs\": torch.cat([t[\"log_probs\"] for t in trajectories]),\n",
    "        \"values\": torch.cat([t[\"values\"] for t in trajectories]),\n",
    "        \"rewards\": torch.cat([t[\"rewards\"] for t in trajectories]),\n",
    "        \"dones\": torch.cat([t[\"dones\"] for t in trajectories]),\n",
    "    }\n",
    "    \n",
    "    # Calculate combined rewards statistics\n",
    "    rewards = combined_trajectory[\"rewards\"]\n",
    "    min_rew = rewards.min().item()\n",
    "    max_rew = rewards.max().item()\n",
    "    mean_rew = rewards.mean().item()\n",
    "    print(f\"Episode {episode+1}/{cfg.training.num_episodes} - Min reward: {min_rew:.4f}, Max reward: {max_rew:.4f}, Mean reward: {mean_rew:.4f}, Last reward: {rewards[-1]:.4f}\")\n",
    "    log_rewards.append(mean_rew)\n",
    "\n",
    "    # Update agent with combined trajectory\n",
    "    policy_loss, value_loss, entropy = ppo_agent.update(combined_trajectory)\n",
    "    print(f\"Episode {episode+1}/{cfg.training.num_episodes} - Policy loss: {policy_loss:.4f}, Value loss: {value_loss:.4f}, Entropy: {entropy:.4f}\")\n",
    "\n",
    "    if rewards[-1] > best_last_reward:\n",
    "        best_last_reward = rewards[-1]\n",
    "        import os, torch\n",
    "        os.makedirs(cfg.paths.output_dir, exist_ok=True)\n",
    "        torch.save(ppo_agent.policy_net.state_dict(), best_actor_path)\n",
    "        torch.save(ppo_agent.value_net.state_dict(), best_critic_path)\n",
    "        print(f\"Best model saved at episode {episode+1} with mean reward {best_last_reward:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a64886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e27a648",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(log_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34cbce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
