{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aae562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import helper as hp\n",
    "from configparser import ConfigParser\n",
    "\n",
    "from ppo_refinement import PPORefinement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51519fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_flag = 0\n",
    "lambda_partition = -2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d113bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lambda_max(p_tensor_single):\n",
    "    # Convert tensor to NumPy\n",
    "    p_numpy = p_tensor_single.detach().cpu().numpy()\n",
    "\n",
    "    # Determine size n from length = n(n+1)/2\n",
    "    L = p_numpy.shape[0]\n",
    "    # Solve n(n+1)/2 = L\n",
    "    n = int((np.sqrt(8 * L + 1) - 1) / 2)\n",
    "    assert n * (n + 1) // 2 == L, f\"Input length {L} is not valid for any symmetric n x n matrix.\"\n",
    "\n",
    "    # Fill upper triangular matrix\n",
    "    A = np.zeros((n, n))\n",
    "    idx = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):  # Only upper triangle including diagonal\n",
    "            A[i, j] = p_numpy[idx]\n",
    "            A[j, i] = p_numpy[idx]  # Mirror to lower triangle\n",
    "            idx += 1\n",
    "\n",
    "    # Compute eigenvalues\n",
    "    eigvals = np.linalg.eigvals(A)\n",
    "\n",
    "    # Sort by real part descending\n",
    "    eigvals_sorted = sorted(eigvals, key=lambda x: x.real, reverse=True)\n",
    "\n",
    "    return eigvals_sorted\n",
    "\n",
    "def compute_reward(p_tensor_single, n_consider=10):\n",
    "    lambdas_val = _get_lambda_max(p_tensor_single)\n",
    "\n",
    "    if reward_flag == 0:\n",
    "        lambda_max_val = lambdas_val[0]\n",
    "        penalty = np.maximum(0, lambda_max_val)\n",
    "        if lambda_max_val > 100:\n",
    "            lambda_max_val = 100\n",
    "        r = 1.0 / (1.0 + np.exp(lambda_max_val - lambda_partition))\n",
    "        r -= penalty\n",
    "    else:\n",
    "        considered_avg = sum(lambdas_val[:n_consider]) / n_consider\n",
    "        r = np.exp(-0.1 * considered_avg) / 2\n",
    "    # TODO: Right now, we are not using the Incidence part of the reward.\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741db84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Begin PPO refinement strategy\n",
      "Training on cpu. 1 trajectories per update.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 10)) of distribution Normal(loc: torch.Size([1, 10]), scale: torch.Size([1, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n       grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3c7c44e7d26c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrained_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_training_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthis_savepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#print(f\"PPO training finished. Rewards log saved to {this_savepath}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/ppo_refinement.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_training_iterations, output_path)\u001b[0m\n\u001b[1;32m    227\u001b[0m             )\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_rollout_data_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_batch_final_rewards\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/ppo_refinement.py\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m(self, rollout_data)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mmu_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mstd_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_std_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mdist_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mnew_log_probs_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 56\u001b[0;31m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;34mf\"of distribution {repr(self)} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 10)) of distribution Normal(loc: torch.Size([1, 10]), scale: torch.Size([1, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n       grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "print('--- Begin PPO refinement strategy')\n",
    "\n",
    "configs = ConfigParser()\n",
    "configs.read('configfile.ini')\n",
    "output_path = configs['PATHS']['output_path']\n",
    "this_savepath = f'output/ppo-refinement/ppo_sandbox/' \n",
    "os.makedirs(this_savepath, exist_ok=True)\n",
    "\n",
    "ppo_agent = PPORefinement(\n",
    "    param_dim=10,\n",
    "    noise_dim=10,\n",
    "    reward_function=compute_reward,\n",
    "    min_x_bounds=-10,\n",
    "    max_x_bounds=10,\n",
    "    ppo_epochs=10,\n",
    "    T_horizon=1,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-4,\n",
    "    n_trajectories=1,\n",
    ")\n",
    "\n",
    "trained_actor, rewards = ppo_agent.train(num_training_iterations=100, output_path=this_savepath)\n",
    "\n",
    "#print(f\"PPO training finished. Rewards log saved to {this_savepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd826db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0853,  0.0058,  0.0147, -0.0552, -0.2510,  0.1967,  0.1654, -0.0563,\n",
      "         0.0818, -0.0612]), tensor([ 0.0672,  0.0127, -0.0428, -0.1357, -0.0399,  0.0643,  0.0539,  0.1249,\n",
      "        -0.1591,  0.1332]))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(ppo_agent.actor(torch.randn(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a94691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'PPO Training Rewards')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Final Reward')\n",
    "plt.title('PPO Training Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74902174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "def evaluate_policy_incidence(trained_actor, ppo_agent, num_trials=50):\n",
    "    T_HORIZON = ppo_agent.T_horizon\n",
    "    NOISE_DIM = ppo_agent.noise_dim\n",
    "    N_TRIALS = num_trials\n",
    "    incidence = 0\n",
    "    for _ in range(N_TRIALS):\n",
    "        current_params_in_state = ppo_agent._initialize_current_params_for_state().clone()\n",
    "        generated_sequence = []\n",
    "        for _ in range(T_HORIZON):\n",
    "            noise = torch.randn(NOISE_DIM, device=ppo_agent.device)\n",
    "            state_1d = torch.cat((noise, current_params_in_state.detach()), dim=0)\n",
    "            state_batch = state_1d.unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                mu_raw, log_std_raw = trained_actor(state_batch)\n",
    "                # For generation, you might want to take the mean (mu_raw) or sample\n",
    "                # action_raw = mu_raw # Deterministic generation\n",
    "                action_raw = Normal(mu_raw, torch.exp(log_std_raw)).sample() # Stochastic generation\n",
    "            \n",
    "            ode_params = ppo_agent._transform_to_bounded(action_raw)\n",
    "            current_params_in_state = ode_params.squeeze(0)\n",
    "            generated_sequence.append(current_params_in_state.cpu().numpy())\n",
    "\n",
    "        final_generated_params = generated_sequence[-1]\n",
    "        final_reward_eval = compute_reward(torch.tensor(final_generated_params, device=ppo_agent.device))\n",
    "        lambda_max = _get_lambda_max(torch.tensor(final_generated_params, device=ppo_agent.device))[0]\n",
    "        if lambda_max < -2.5:\n",
    "            incidence += 1\n",
    "        print(f\"Final lambda_max: {lambda_max:.4f}\")\n",
    "        print(f\"Final reward: {final_reward_eval:.4f}\")\n",
    "    print(f\"Incidence over {N_TRIALS} trials: {incidence}/{N_TRIALS} = {incidence/N_TRIALS:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202681fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final lambda_max: -7.1210\n",
      "Final reward: 0.9903\n",
      "Final lambda_max: -8.2873\n",
      "Final reward: 0.9969\n",
      "Final lambda_max: -8.1637\n",
      "Final reward: 0.9965\n",
      "Final lambda_max: -7.4484\n",
      "Final reward: 0.9930\n",
      "Final lambda_max: -8.2206\n",
      "Final reward: 0.9967\n",
      "Final lambda_max: -7.8938\n",
      "Final reward: 0.9955\n",
      "Final lambda_max: -8.0707\n",
      "Final reward: 0.9962\n",
      "Final lambda_max: -7.3388\n",
      "Final reward: 0.9921\n",
      "Final lambda_max: -8.5687\n",
      "Final reward: 0.9977\n",
      "Final lambda_max: -7.9198\n",
      "Final reward: 0.9956\n",
      "Final lambda_max: -8.0095\n",
      "Final reward: 0.9960\n",
      "Final lambda_max: -8.1965\n",
      "Final reward: 0.9967\n",
      "Final lambda_max: -7.9705\n",
      "Final reward: 0.9958\n",
      "Final lambda_max: -8.0716\n",
      "Final reward: 0.9962\n",
      "Final lambda_max: -7.9326\n",
      "Final reward: 0.9956\n",
      "Final lambda_max: -8.0483\n",
      "Final reward: 0.9961\n",
      "Final lambda_max: -7.8731\n",
      "Final reward: 0.9954\n",
      "Final lambda_max: -8.1165\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -8.2331\n",
      "Final reward: 0.9968\n",
      "Final lambda_max: -8.6573\n",
      "Final reward: 0.9979\n",
      "Final lambda_max: -7.8948\n",
      "Final reward: 0.9955\n",
      "Final lambda_max: -8.3819\n",
      "Final reward: 0.9972\n",
      "Final lambda_max: -8.1332\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -8.1291\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -8.1537\n",
      "Final reward: 0.9965\n",
      "Final lambda_max: -8.2251\n",
      "Final reward: 0.9967\n",
      "Final lambda_max: -7.9329\n",
      "Final reward: 0.9956\n",
      "Final lambda_max: -8.0274\n",
      "Final reward: 0.9960\n",
      "Final lambda_max: -8.1259\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -7.9443\n",
      "Final reward: 0.9957\n",
      "Final lambda_max: -7.7594\n",
      "Final reward: 0.9948\n",
      "Final lambda_max: -8.7048\n",
      "Final reward: 0.9980\n",
      "Final lambda_max: -7.7205\n",
      "Final reward: 0.9946\n",
      "Final lambda_max: -8.0560\n",
      "Final reward: 0.9962\n",
      "Final lambda_max: -8.1431\n",
      "Final reward: 0.9965\n",
      "Final lambda_max: -8.2288\n",
      "Final reward: 0.9968\n",
      "Final lambda_max: -7.7969\n",
      "Final reward: 0.9950\n",
      "Final lambda_max: -7.6468\n",
      "Final reward: 0.9942\n",
      "Final lambda_max: -7.8991\n",
      "Final reward: 0.9955\n",
      "Final lambda_max: -8.0385\n",
      "Final reward: 0.9961\n",
      "Final lambda_max: -8.6161\n",
      "Final reward: 0.9978\n",
      "Final lambda_max: -7.9944\n",
      "Final reward: 0.9959\n",
      "Final lambda_max: -8.2521\n",
      "Final reward: 0.9968\n",
      "Final lambda_max: -7.3098\n",
      "Final reward: 0.9919\n",
      "Final lambda_max: -8.3445\n",
      "Final reward: 0.9971\n",
      "Final lambda_max: -7.6144\n",
      "Final reward: 0.9940\n",
      "Final lambda_max: -7.6060\n",
      "Final reward: 0.9940\n",
      "Final lambda_max: -8.1172\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -7.6183\n",
      "Final reward: 0.9940\n",
      "Final lambda_max: -7.2254\n",
      "Final reward: 0.9912\n",
      "Incidence over 50 trials: 50/50 = 1.00\n"
     ]
    }
   ],
   "source": [
    "evaluate_policy_incidence(trained_actor, ppo_agent, num_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cbd827",
   "metadata": {},
   "source": [
    "## Nonlinear env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b2a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_nonlinear_dynamics(x, theta):\n",
    "    n = x.shape[0]\n",
    "    assert theta.shape[0] >= n * n, f\"Need at least {n * n} parameters, got {theta.shape[0]}\"\n",
    "    \n",
    "    # Reshape first n^2 parameters into n x n matrix\n",
    "    T = theta[:n * n].reshape(n, n)\n",
    "\n",
    "    # Define nonlinear function: f_i(x) = sum_j T_ij * tanh(x_j) + sin(x_i) * x_i\n",
    "    x_tanh = torch.tanh(x)\n",
    "    linear_part = T @ x_tanh\n",
    "    nonlinear_part = torch.sin(x) * x\n",
    "\n",
    "    return linear_part + nonlinear_part\n",
    "\n",
    "def _get_lambda_max(p_tensor_single, x_eval=None):\n",
    "    # Infer n from length\n",
    "    L = p_tensor_single.shape[0]\n",
    "    n = int(np.sqrt(L))\n",
    "    if x_eval is None:\n",
    "        x_eval = torch.zeros(n, dtype=torch.float32, requires_grad=True)\n",
    "    else:\n",
    "        x_eval = x_eval.detach().clone().requires_grad_(True)\n",
    "\n",
    "    theta = p_tensor_single.detach().clone().requires_grad_(False)\n",
    "\n",
    "    # Compute f(x; theta)\n",
    "    f = generalized_nonlinear_dynamics(x_eval, theta)\n",
    "\n",
    "    # Compute Jacobian via autograd\n",
    "    J_rows = []\n",
    "    for f_i in f:\n",
    "        grad_f_i = torch.autograd.grad(f_i, x_eval, retain_graph=True)[0]\n",
    "        J_rows.append(grad_f_i)\n",
    "\n",
    "    J_matrix = torch.stack(J_rows)\n",
    "    J_np = J_matrix.detach().numpy()\n",
    "\n",
    "    # Compute eigenvalues\n",
    "    eigvals = np.linalg.eigvals(J_np)\n",
    "    eigvals_sorted = sorted(eigvals, key=lambda x: x.real, reverse=True)\n",
    "    return eigvals_sorted\n",
    "\n",
    "def compute_reward(p_tensor_single, n_consider=10):\n",
    "    lambdas_val = _get_lambda_max(p_tensor_single)\n",
    "\n",
    "    if reward_flag == 0:\n",
    "        lambda_max_val = np.real(lambdas_val[0])\n",
    "        penalty = np.maximum(0, lambda_max_val)\n",
    "        if lambda_max_val > 100:\n",
    "            lambda_max_val = 100\n",
    "        r = 1.0 / (1.0 + np.exp(lambda_max_val - lambda_partition))\n",
    "        r -= penalty\n",
    "    else:\n",
    "        considered_avg = sum(lambdas_val[:n_consider]) / n_consider\n",
    "        r = np.exp(-0.1 * considered_avg) / 2\n",
    "    # TODO: Right now, we are not using the Incidence part of the reward.\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66713fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Begin PPO refinement strategy\n",
      "Training on cpu. 64 trajectories per update.\n",
      "Iteration 1/100, Avg Batch Final Reward: -8.1502\n",
      "Avg actor loss -0.0546 Avg critic loss 90.9855\n",
      "Iteration 2/100, Avg Batch Final Reward: -7.1286\n",
      "Avg actor loss -0.0354 Avg critic loss 60.2790\n",
      "Iteration 3/100, Avg Batch Final Reward: -7.1762\n",
      "Avg actor loss -0.0347 Avg critic loss 56.6773\n",
      "Iteration 4/100, Avg Batch Final Reward: -8.4056\n",
      "Avg actor loss -0.0356 Avg critic loss 75.5095\n",
      "Iteration 5/100, Avg Batch Final Reward: -6.0248\n",
      "Avg actor loss -0.0318 Avg critic loss 37.7575\n",
      "Iteration 6/100, Avg Batch Final Reward: -7.9332\n",
      "Avg actor loss -0.0326 Avg critic loss 52.5337\n",
      "Iteration 7/100, Avg Batch Final Reward: -6.9918\n",
      "Avg actor loss -0.0432 Avg critic loss 39.7058\n",
      "Iteration 8/100, Avg Batch Final Reward: -5.9068\n",
      "Avg actor loss -0.0379 Avg critic loss 28.8744\n",
      "Iteration 9/100, Avg Batch Final Reward: -5.9719\n",
      "Avg actor loss -0.0392 Avg critic loss 27.2944\n",
      "Iteration 10/100, Avg Batch Final Reward: -5.5444\n",
      "Avg actor loss -0.0309 Avg critic loss 22.3010\n",
      "Iteration 11/100, Avg Batch Final Reward: -5.4704\n",
      "Avg actor loss -0.0310 Avg critic loss 22.7050\n",
      "Iteration 12/100, Avg Batch Final Reward: -5.8596\n",
      "Avg actor loss -0.0350 Avg critic loss 19.8186\n",
      "Iteration 13/100, Avg Batch Final Reward: -5.4764\n",
      "Avg actor loss -0.0410 Avg critic loss 22.8163\n",
      "Iteration 14/100, Avg Batch Final Reward: -3.7110\n",
      "Avg actor loss -0.0344 Avg critic loss 17.7593\n",
      "Iteration 15/100, Avg Batch Final Reward: -4.6262\n",
      "Avg actor loss -0.0308 Avg critic loss 18.7076\n",
      "Iteration 16/100, Avg Batch Final Reward: -4.4638\n",
      "Avg actor loss -0.0390 Avg critic loss 22.4018\n",
      "Iteration 17/100, Avg Batch Final Reward: -3.3952\n",
      "Avg actor loss -0.0431 Avg critic loss 15.7459\n",
      "Iteration 18/100, Avg Batch Final Reward: -3.2943\n",
      "Avg actor loss -0.0335 Avg critic loss 15.2044\n",
      "Iteration 19/100, Avg Batch Final Reward: -2.7225\n",
      "Avg actor loss -0.0436 Avg critic loss 14.1671\n",
      "Iteration 20/100, Avg Batch Final Reward: -2.9008\n",
      "Avg actor loss -0.0417 Avg critic loss 16.7047\n",
      "Iteration 21/100, Avg Batch Final Reward: -2.2672\n",
      "Avg actor loss -0.0358 Avg critic loss 10.9164\n",
      "Iteration 22/100, Avg Batch Final Reward: -1.9594\n",
      "Avg actor loss -0.0353 Avg critic loss 12.8179\n",
      "Iteration 23/100, Avg Batch Final Reward: -1.9363\n",
      "Avg actor loss -0.0323 Avg critic loss 9.0875\n",
      "Iteration 24/100, Avg Batch Final Reward: -2.4852\n",
      "Avg actor loss -0.0400 Avg critic loss 12.2847\n",
      "Iteration 25/100, Avg Batch Final Reward: -1.7237\n",
      "Avg actor loss -0.0400 Avg critic loss 10.3976\n",
      "Iteration 26/100, Avg Batch Final Reward: -1.6535\n",
      "Avg actor loss -0.0425 Avg critic loss 9.2996\n",
      "Iteration 27/100, Avg Batch Final Reward: -1.1057\n",
      "Avg actor loss -0.0390 Avg critic loss 6.0601\n",
      "Iteration 28/100, Avg Batch Final Reward: -0.3804\n",
      "Avg actor loss -0.0386 Avg critic loss 4.4721\n",
      "Iteration 29/100, Avg Batch Final Reward: -0.8164\n",
      "Avg actor loss -0.0521 Avg critic loss 5.5794\n",
      "Iteration 30/100, Avg Batch Final Reward: -0.6254\n",
      "Avg actor loss -0.0314 Avg critic loss 4.1274\n",
      "Iteration 31/100, Avg Batch Final Reward: -0.3215\n",
      "Avg actor loss -0.0293 Avg critic loss 3.9908\n",
      "Iteration 32/100, Avg Batch Final Reward: -0.4707\n",
      "Avg actor loss -0.0489 Avg critic loss 5.0051\n",
      "Iteration 33/100, Avg Batch Final Reward: -0.7278\n",
      "Avg actor loss -0.0353 Avg critic loss 5.1236\n",
      "Iteration 34/100, Avg Batch Final Reward: -0.4856\n",
      "Avg actor loss -0.0450 Avg critic loss 4.3288\n",
      "Iteration 35/100, Avg Batch Final Reward: -0.0329\n",
      "Avg actor loss -0.0385 Avg critic loss 3.2349\n",
      "Iteration 36/100, Avg Batch Final Reward: 0.2655\n",
      "Avg actor loss -0.0376 Avg critic loss 1.3125\n",
      "Iteration 37/100, Avg Batch Final Reward: 0.1565\n",
      "Avg actor loss -0.0356 Avg critic loss 1.8636\n",
      "Iteration 38/100, Avg Batch Final Reward: 0.0409\n",
      "Avg actor loss -0.0435 Avg critic loss 2.7948\n",
      "Iteration 39/100, Avg Batch Final Reward: 0.3793\n",
      "Avg actor loss -0.0327 Avg critic loss 1.3220\n",
      "Iteration 40/100, Avg Batch Final Reward: 0.3017\n",
      "Avg actor loss -0.0286 Avg critic loss 1.8296\n",
      "Iteration 41/100, Avg Batch Final Reward: 0.4070\n",
      "Avg actor loss -0.0242 Avg critic loss 0.7431\n",
      "Iteration 42/100, Avg Batch Final Reward: 0.4172\n",
      "Avg actor loss -0.0391 Avg critic loss 1.0508\n",
      "Iteration 43/100, Avg Batch Final Reward: 0.4573\n",
      "Avg actor loss -0.0284 Avg critic loss 0.9077\n",
      "Iteration 44/100, Avg Batch Final Reward: 0.6418\n",
      "Avg actor loss -0.0304 Avg critic loss 0.4505\n",
      "Iteration 45/100, Avg Batch Final Reward: 0.6175\n",
      "Avg actor loss -0.0341 Avg critic loss 0.5078\n",
      "Iteration 46/100, Avg Batch Final Reward: 0.6733\n",
      "Avg actor loss -0.0280 Avg critic loss 0.5422\n",
      "Iteration 47/100, Avg Batch Final Reward: 0.6879\n",
      "Avg actor loss -0.0427 Avg critic loss 0.2420\n",
      "Iteration 48/100, Avg Batch Final Reward: 0.7962\n",
      "Avg actor loss -0.0277 Avg critic loss 0.0968\n",
      "Iteration 49/100, Avg Batch Final Reward: 0.7017\n",
      "Avg actor loss -0.0292 Avg critic loss 0.7042\n",
      "Iteration 50/100, Avg Batch Final Reward: 0.8421\n",
      "Avg actor loss -0.0380 Avg critic loss 0.1528\n",
      "Iteration 51/100, Avg Batch Final Reward: 0.8423\n",
      "Avg actor loss -0.0302 Avg critic loss 0.0929\n",
      "Iteration 52/100, Avg Batch Final Reward: 0.8419\n",
      "Avg actor loss -0.0259 Avg critic loss 0.3267\n",
      "Iteration 53/100, Avg Batch Final Reward: 0.9113\n",
      "Avg actor loss -0.0296 Avg critic loss 0.0490\n",
      "Iteration 54/100, Avg Batch Final Reward: 0.8756\n",
      "Avg actor loss -0.0325 Avg critic loss 0.1015\n",
      "Iteration 55/100, Avg Batch Final Reward: 0.8922\n",
      "Avg actor loss -0.0336 Avg critic loss 0.0538\n",
      "Iteration 56/100, Avg Batch Final Reward: 0.9111\n",
      "Avg actor loss -0.0369 Avg critic loss 0.0262\n",
      "Iteration 57/100, Avg Batch Final Reward: 0.9238\n",
      "Avg actor loss -0.0290 Avg critic loss 0.0304\n",
      "Iteration 58/100, Avg Batch Final Reward: 0.9327\n",
      "Avg actor loss -0.0282 Avg critic loss 0.0196\n",
      "Iteration 59/100, Avg Batch Final Reward: 0.9084\n",
      "Avg actor loss -0.0429 Avg critic loss 0.0397\n",
      "Iteration 60/100, Avg Batch Final Reward: 0.9570\n",
      "Avg actor loss -0.0302 Avg critic loss 0.0120\n",
      "Iteration 61/100, Avg Batch Final Reward: 0.9483\n",
      "Avg actor loss -0.0296 Avg critic loss 0.0169\n",
      "Iteration 62/100, Avg Batch Final Reward: 0.9655\n",
      "Avg actor loss -0.0308 Avg critic loss 0.0041\n",
      "Iteration 63/100, Avg Batch Final Reward: 0.9571\n",
      "Avg actor loss -0.0373 Avg critic loss 0.0132\n",
      "Iteration 64/100, Avg Batch Final Reward: 0.9643\n",
      "Avg actor loss -0.0294 Avg critic loss 0.0106\n",
      "Iteration 65/100, Avg Batch Final Reward: 0.9396\n",
      "Avg actor loss -0.0301 Avg critic loss 0.0682\n",
      "Iteration 66/100, Avg Batch Final Reward: 0.9798\n",
      "Avg actor loss -0.0229 Avg critic loss 0.0044\n",
      "Iteration 67/100, Avg Batch Final Reward: 0.9844\n",
      "Avg actor loss -0.0235 Avg critic loss 0.0013\n",
      "Iteration 68/100, Avg Batch Final Reward: 0.9765\n",
      "Avg actor loss -0.0289 Avg critic loss 0.0024\n",
      "Iteration 69/100, Avg Batch Final Reward: 0.9852\n",
      "Avg actor loss -0.0322 Avg critic loss 0.0015\n",
      "Iteration 70/100, Avg Batch Final Reward: 0.9895\n",
      "Avg actor loss -0.0298 Avg critic loss 0.0006\n",
      "Iteration 71/100, Avg Batch Final Reward: 0.9706\n",
      "Avg actor loss -0.0254 Avg critic loss 0.0096\n",
      "Iteration 72/100, Avg Batch Final Reward: 0.9704\n",
      "Avg actor loss -0.0200 Avg critic loss 0.0128\n",
      "Iteration 73/100, Avg Batch Final Reward: 0.9870\n",
      "Avg actor loss -0.0444 Avg critic loss 0.0009\n",
      "Iteration 74/100, Avg Batch Final Reward: 0.9880\n",
      "Avg actor loss -0.0344 Avg critic loss 0.0009\n",
      "Iteration 75/100, Avg Batch Final Reward: 0.9900\n",
      "Avg actor loss -0.0459 Avg critic loss 0.0005\n",
      "Iteration 76/100, Avg Batch Final Reward: 0.9888\n",
      "Avg actor loss -0.0269 Avg critic loss 0.0004\n",
      "Iteration 77/100, Avg Batch Final Reward: 0.9869\n",
      "Avg actor loss -0.0282 Avg critic loss 0.0016\n",
      "Iteration 78/100, Avg Batch Final Reward: 0.9927\n",
      "Avg actor loss -0.0238 Avg critic loss 0.0004\n",
      "Iteration 79/100, Avg Batch Final Reward: 0.9901\n",
      "Avg actor loss -0.0350 Avg critic loss 0.0010\n",
      "Iteration 80/100, Avg Batch Final Reward: 0.9902\n",
      "Avg actor loss -0.0301 Avg critic loss 0.0011\n",
      "Iteration 81/100, Avg Batch Final Reward: 0.9908\n",
      "Avg actor loss -0.0287 Avg critic loss 0.0011\n",
      "Iteration 82/100, Avg Batch Final Reward: 0.9927\n",
      "Avg actor loss -0.0328 Avg critic loss 0.0003\n",
      "Iteration 83/100, Avg Batch Final Reward: 0.9941\n",
      "Avg actor loss -0.0196 Avg critic loss 0.0002\n",
      "Iteration 84/100, Avg Batch Final Reward: 0.9963\n",
      "Avg actor loss -0.0279 Avg critic loss 0.0001\n",
      "Iteration 85/100, Avg Batch Final Reward: 0.9957\n",
      "Avg actor loss -0.0233 Avg critic loss 0.0002\n",
      "Iteration 86/100, Avg Batch Final Reward: 0.9928\n",
      "Avg actor loss -0.0246 Avg critic loss 0.0004\n",
      "Iteration 87/100, Avg Batch Final Reward: 0.9914\n",
      "Avg actor loss -0.0312 Avg critic loss 0.0009\n",
      "Iteration 88/100, Avg Batch Final Reward: 0.9957\n",
      "Avg actor loss -0.0245 Avg critic loss 0.0001\n",
      "Iteration 89/100, Avg Batch Final Reward: 0.9953\n",
      "Avg actor loss -0.0170 Avg critic loss 0.0002\n",
      "Iteration 90/100, Avg Batch Final Reward: 0.9952\n",
      "Avg actor loss -0.0216 Avg critic loss 0.0002\n",
      "Iteration 91/100, Avg Batch Final Reward: 0.9944\n",
      "Avg actor loss -0.0337 Avg critic loss 0.0003\n",
      "Iteration 92/100, Avg Batch Final Reward: 0.9965\n",
      "Avg actor loss -0.0341 Avg critic loss 0.0001\n",
      "Iteration 93/100, Avg Batch Final Reward: 0.9942\n",
      "Avg actor loss -0.0311 Avg critic loss 0.0005\n",
      "Iteration 94/100, Avg Batch Final Reward: 0.9963\n",
      "Avg actor loss -0.0263 Avg critic loss 0.0001\n",
      "Iteration 95/100, Avg Batch Final Reward: 0.9967\n",
      "Avg actor loss -0.0173 Avg critic loss 0.0001\n",
      "Iteration 96/100, Avg Batch Final Reward: 0.9969\n",
      "Avg actor loss -0.0189 Avg critic loss 0.0001\n",
      "Iteration 97/100, Avg Batch Final Reward: 0.9969\n",
      "Avg actor loss -0.0228 Avg critic loss 0.0001\n",
      "Iteration 98/100, Avg Batch Final Reward: 0.9962\n",
      "Avg actor loss -0.0240 Avg critic loss 0.0001\n",
      "Iteration 99/100, Avg Batch Final Reward: 0.9959\n",
      "Avg actor loss -0.0271 Avg critic loss 0.0002\n",
      "Iteration 100/100, Avg Batch Final Reward: 0.9972\n",
      "Avg actor loss -0.0343 Avg critic loss 0.0001\n",
      "Model saved at iteration 100\n",
      "Final models saved to output/ppo-refinement/ppo_sandbox/\n",
      "Average final rewards per iteration saved at output/ppo-refinement/ppo_sandbox/avg_final_rewards_per_iteration.npy\n",
      "Training finished.\n",
      "PPO training finished. Rewards log saved to output/ppo-refinement/ppo_sandbox/\n"
     ]
    }
   ],
   "source": [
    "print('--- Begin PPO refinement strategy')\n",
    "\n",
    "configs = ConfigParser()\n",
    "configs.read('configfile.ini')\n",
    "output_path = configs['PATHS']['output_path']\n",
    "this_savepath = f'output/ppo-refinement/ppo_sandbox/' \n",
    "os.makedirs(this_savepath, exist_ok=True)\n",
    "\n",
    "ppo_agent = PPORefinement(\n",
    "    param_dim=10,\n",
    "    noise_dim=10,\n",
    "    reward_function=compute_reward,\n",
    "    min_x_bounds=-10,\n",
    "    max_x_bounds=10,\n",
    "    ppo_epochs=10,\n",
    "    T_horizon=1,\n",
    "    actor_lr=1e-5,\n",
    "    critic_lr=5e-5,\n",
    "    n_trajectories=64,\n",
    ")\n",
    "\n",
    "trained_actor, rewards = ppo_agent.train(num_training_iterations=100, output_path=this_savepath)\n",
    "\n",
    "print(f\"PPO training finished. Rewards log saved to {this_savepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb5ef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'PPO Training Rewards')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Final Reward')\n",
    "plt.title('PPO Training Rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bd02e",
   "metadata": {},
   "source": [
    "# New implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba01d776",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers.ppo_agent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a60767421111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrenaissance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkinetics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjacobian_solver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_jacobian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo_agent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPOAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKineticEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreward_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_pkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helpers.ppo_agent'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from renaissance.kinetics.jacobian_solver import check_jacobian\n",
    "\n",
    "from helpers.ppo_agent import PPOAgent\n",
    "from helpers.env import KineticEnv\n",
    "from helpers.utils import reward_func, load_pkl\n",
    "\n",
    "import logging\n",
    "\n",
    "def train(cfg: DictConfig):\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(OmegaConf.to_yaml(cfg))  # print config to verify\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Call solvers from SKimPy\n",
    "    chk_jcbn = check_jacobian()\n",
    "    logging.disable(logging.CRITICAL)\n",
    "\n",
    "    # Integrate data\n",
    "    print(\"FYI: Loading kinetic and thermodynamic data.\")\n",
    "    chk_jcbn._load_ktmodels(cfg.paths.met_model_name, 'fdp1') # Load kinetic and thermodynamic data\n",
    "    chk_jcbn._load_ssprofile(cfg.paths.met_model_name, 'fdp1', cfg.constraints.ss_idx) # Integrate steady state information\n",
    "\n",
    "    # Logger setup, todo: for now disabled, else we would get w&b run object\n",
    "    logger = None # get_logger(cfg)\n",
    "\n",
    "    # Initialize environment\n",
    "    names_km = load_pkl(cfg.paths.names_km)\n",
    "    reward_fn = partial(reward_func, chk_jcbn, names_km, cfg.reward.eig_partition)\n",
    "    env = KineticEnv(cfg, reward_fn)\n",
    "    env.seed(cfg.seed)\n",
    "\n",
    "    # Initialize PPO agent (actor and critic)\n",
    "    ppo_agent = PPOAgent(cfg, logger)\n",
    "   \n",
    "    # Training loop\n",
    "    for episode in range(cfg.training.num_episodes):\n",
    "        trajectory = ppo_agent.collect_trajectory(env)\n",
    "        rewards = trajectory[\"rewards\"]\n",
    "        min_rew, max_rew, mean_rew = rewards.min(), rewards.max(), rewards.mean()\n",
    "        print(f\"Episode {episode+1}/{cfg.training.num_episodes} - Min reward: {min_rew:.4f}, Max reward: {max_rew:.4f}, Mean reward: {mean_rew:.4f}\")\n",
    "\n",
    "        policy_loss, value_loss, entropy = ppo_agent.update(trajectory)\n",
    "        print(f\"Episode {episode+1}/{cfg.training.num_episodes} - Policy loss: {policy_loss:.4f}, Value loss: {value_loss:.4f}, Entropy: {entropy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bc15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from renaissance.kinetics.jacobian_solver import check_jacobian\n",
    "\n",
    "from helpers.ppo_agent import PPOAgent\n",
    "from helpers.env import KineticEnv\n",
    "from helpers.utils import reward_func, load_pkl\n",
    "\n",
    "# Hydra-style manual config loading in a notebook\n",
    "initialize(config_path=\"configs\", version_base=\"1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea57798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"train_sandbox.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b868e67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "method:\n",
      "  name: ppo_refinement\n",
      "  actor_lr: 0.0003\n",
      "  critic_lr: 0.001\n",
      "  discount_factor: 0.99\n",
      "  gae_lambda: 0.98\n",
      "  clip_eps: 0.2\n",
      "  value_loss_weight: 0.5\n",
      "  entropy_loss_weight: 0.0\n",
      "  parameter_dim: 10\n",
      "  latent_dim: 99\n",
      "seed: 42\n",
      "device: cpu\n",
      "logger:\n",
      "  project: rl-renaissance\n",
      "  entity: ludekcizinsky\n",
      "  tags:\n",
      "  - dev\n",
      "paths:\n",
      "  names_km: data/varma_ecoli_shikki/parameter_names_km_fdp1.pkl\n",
      "  output_dir: /home/renaissance/output\n",
      "  met_model_name: varma_ecoli_shikki\n",
      "constraints:\n",
      "  min_km: -25\n",
      "  max_km: 3\n",
      "  ss_idx: 1712\n",
      "reward:\n",
      "  eig_partition: -2.5\n",
      "env:\n",
      "  p0_init_mean: 0\n",
      "  p0_init_std: 0.1\n",
      "  p_size: 10\n",
      "training:\n",
      "  num_episodes: 1000\n",
      "  max_steps_per_episode: 50\n",
      "  batch_size: 25\n",
      "  num_epochs: 10\n",
      "  max_grad_norm: 0.5\n",
      "\n",
      "--------------------------------------------------\n",
      "FYI: Loading kinetic and thermodynamic data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-251:\n",
      "Process ForkPoolWorker-250:\n",
      "Process ForkPoolWorker-226:\n",
      "Process ForkPoolWorker-240:\n",
      "Process ForkPoolWorker-246:\n",
      "Process ForkPoolWorker-242:\n",
      "Process ForkPoolWorker-244:\n",
      "Process ForkPoolWorker-238:\n",
      "Process ForkPoolWorker-256:\n",
      "Process ForkPoolWorker-227:\n",
      "Process ForkPoolWorker-253:\n",
      "Process ForkPoolWorker-239:\n",
      "Process ForkPoolWorker-255:\n",
      "Process ForkPoolWorker-225:\n",
      "Process ForkPoolWorker-254:\n",
      "Process ForkPoolWorker-236:\n",
      "Process ForkPoolWorker-234:\n",
      "Process ForkPoolWorker-235:\n",
      "Process ForkPoolWorker-237:\n",
      "Process ForkPoolWorker-249:\n",
      "Process ForkPoolWorker-245:\n",
      "Process ForkPoolWorker-233:\n",
      "Process ForkPoolWorker-228:\n",
      "Process ForkPoolWorker-243:\n",
      "Process ForkPoolWorker-248:\n",
      "Process ForkPoolWorker-231:\n",
      "Process ForkPoolWorker-241:\n",
      "Process ForkPoolWorker-247:\n",
      "Process ForkPoolWorker-232:\n",
      "Process ForkPoolWorker-230:\n",
      "Process ForkPoolWorker-229:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-252:\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 50)\n",
    "print(OmegaConf.to_yaml(cfg))  # print config to verify\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Call solvers from SKimPy\n",
    "chk_jcbn = check_jacobian()\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "# Integrate data\n",
    "print(\"FYI: Loading kinetic and thermodynamic data.\")\n",
    "chk_jcbn._load_ktmodels(cfg.paths.met_model_name, 'fdp1') # Load kinetic and thermodynamic data\n",
    "chk_jcbn._load_ssprofile(cfg.paths.met_model_name, 'fdp1', cfg.constraints.ss_idx) # Integrate steady state information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2249cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger setup, todo: for now disabled, else we would get w&b run object\n",
    "logger = None # get_logger(cfg)\n",
    "\n",
    "# Initialize environment\n",
    "names_km = load_pkl(cfg.paths.names_km)\n",
    "#reward_fn = partial(reward_func, chk_jcbn, names_km, cfg.reward.eig_partition)\n",
    "reward_fn = compute_reward\n",
    "env = KineticEnv(cfg, reward_fn)\n",
    "env.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4939e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000 - Min reward: -37.7796, Max reward: -17.2200, Mean reward: -33.0110\n",
      "Episode 1/1000 - Policy loss: -0.0047, Value loss: 48.0337, Entropy: 9.1927\n",
      "Episode 2/1000 - Min reward: -25.9326, Max reward: -18.4748, Mean reward: -23.5164\n",
      "Episode 2/1000 - Policy loss: -0.0017, Value loss: 69.6841, Entropy: 9.2009\n",
      "Episode 3/1000 - Min reward: -35.8524, Max reward: -1.6536, Mean reward: -28.6795\n",
      "Episode 3/1000 - Policy loss: 0.1571, Value loss: 60.2559, Entropy: 9.2047\n",
      "Episode 4/1000 - Min reward: -27.9694, Max reward: -17.1126, Mean reward: -23.5138\n",
      "Episode 4/1000 - Policy loss: -0.1401, Value loss: 60.2417, Entropy: 9.2033\n",
      "Episode 5/1000 - Min reward: -35.3870, Max reward: -16.1418, Mean reward: -24.8544\n",
      "Episode 5/1000 - Policy loss: -0.1433, Value loss: 39.8032, Entropy: 9.2089\n",
      "Episode 6/1000 - Min reward: -36.4571, Max reward: -7.5506, Mean reward: -29.7409\n",
      "Episode 6/1000 - Policy loss: -0.3226, Value loss: 25.2852, Entropy: 9.2231\n",
      "Episode 7/1000 - Min reward: -35.9217, Max reward: -14.8579, Mean reward: -26.2990\n",
      "Episode 7/1000 - Policy loss: -0.0499, Value loss: 51.4373, Entropy: 9.2216\n",
      "Episode 8/1000 - Min reward: -42.5624, Max reward: -26.2406, Mean reward: -38.7885\n",
      "Episode 8/1000 - Policy loss: -0.3363, Value loss: 29.2547, Entropy: 9.2150\n",
      "Episode 9/1000 - Min reward: -26.4733, Max reward: -13.3516, Mean reward: -21.1212\n",
      "Episode 9/1000 - Policy loss: -0.2339, Value loss: 28.9533, Entropy: 9.2104\n",
      "Episode 10/1000 - Min reward: -26.8912, Max reward: -7.5200, Mean reward: -17.5342\n",
      "Episode 10/1000 - Policy loss: -0.0900, Value loss: 36.7550, Entropy: 9.2199\n",
      "Episode 11/1000 - Min reward: -31.3500, Max reward: -9.8059, Mean reward: -21.3294\n",
      "Episode 11/1000 - Policy loss: -0.3064, Value loss: 21.9356, Entropy: 9.2236\n",
      "Episode 12/1000 - Min reward: -28.2585, Max reward: -12.5399, Mean reward: -21.8691\n",
      "Episode 12/1000 - Policy loss: 0.0478, Value loss: 26.7957, Entropy: 9.2297\n",
      "Episode 13/1000 - Min reward: -27.9029, Max reward: -14.0235, Mean reward: -24.1519\n",
      "Episode 13/1000 - Policy loss: -0.0388, Value loss: 20.4020, Entropy: 9.2067\n",
      "Episode 14/1000 - Min reward: -28.8696, Max reward: -16.1532, Mean reward: -26.1818\n",
      "Episode 14/1000 - Policy loss: 0.0551, Value loss: 13.8985, Entropy: 9.1814\n",
      "Episode 15/1000 - Min reward: -37.3827, Max reward: -15.3980, Mean reward: -29.1349\n",
      "Episode 15/1000 - Policy loss: -0.0646, Value loss: 16.2277, Entropy: 9.1710\n",
      "Episode 16/1000 - Min reward: -25.6632, Max reward: -20.8406, Mean reward: -22.6665\n",
      "Episode 16/1000 - Policy loss: -0.0052, Value loss: 80.6904, Entropy: 9.1693\n",
      "Episode 17/1000 - Min reward: -32.8031, Max reward: -20.7855, Mean reward: -29.4005\n",
      "Episode 17/1000 - Policy loss: -0.0136, Value loss: 6.1659, Entropy: 9.1789\n",
      "Episode 18/1000 - Min reward: -27.7492, Max reward: -13.3557, Mean reward: -24.6895\n",
      "Episode 18/1000 - Policy loss: -0.0446, Value loss: 12.5777, Entropy: 9.1865\n",
      "Episode 19/1000 - Min reward: -22.5419, Max reward: -5.1478, Mean reward: -17.9323\n",
      "Episode 19/1000 - Policy loss: -0.0327, Value loss: 9.4055, Entropy: 9.1771\n",
      "Episode 20/1000 - Min reward: -33.0049, Max reward: -10.0271, Mean reward: -22.5907\n",
      "Episode 20/1000 - Policy loss: -0.1793, Value loss: 10.1091, Entropy: 9.1691\n",
      "Episode 21/1000 - Min reward: -31.0747, Max reward: -12.9715, Mean reward: -24.4080\n",
      "Episode 21/1000 - Policy loss: -0.2363, Value loss: 22.0108, Entropy: 9.1894\n",
      "Episode 22/1000 - Min reward: -25.7509, Max reward: -15.9271, Mean reward: -22.2378\n",
      "Episode 22/1000 - Policy loss: 0.1102, Value loss: 3.5580, Entropy: 9.1939\n",
      "Episode 23/1000 - Min reward: -18.5670, Max reward: -8.6967, Mean reward: -14.7783\n",
      "Episode 23/1000 - Policy loss: -0.1593, Value loss: 12.4513, Entropy: 9.1677\n",
      "Episode 24/1000 - Min reward: -24.3392, Max reward: -0.4261, Mean reward: -13.2271\n",
      "Episode 24/1000 - Policy loss: -0.0002, Value loss: 33.4215, Entropy: 9.1594\n",
      "Episode 25/1000 - Min reward: -33.6669, Max reward: -15.6921, Mean reward: -25.7141\n",
      "Episode 25/1000 - Policy loss: -0.0422, Value loss: 16.8575, Entropy: 9.1604\n",
      "Episode 26/1000 - Min reward: -23.8898, Max reward: -5.4042, Mean reward: -15.6311\n",
      "Episode 26/1000 - Policy loss: -0.2231, Value loss: 22.6241, Entropy: 9.1607\n",
      "Episode 27/1000 - Min reward: -32.9843, Max reward: -8.1235, Mean reward: -21.8953\n",
      "Episode 27/1000 - Policy loss: -0.1472, Value loss: 15.5102, Entropy: 9.1579\n",
      "Episode 28/1000 - Min reward: -29.3977, Max reward: -20.3225, Mean reward: -23.8996\n",
      "Episode 28/1000 - Policy loss: 0.0092, Value loss: 107.1646, Entropy: 9.1516\n",
      "Episode 29/1000 - Min reward: -30.6738, Max reward: -20.0426, Mean reward: -28.2324\n",
      "Episode 29/1000 - Policy loss: -0.0203, Value loss: 9.0617, Entropy: 9.1451\n",
      "Episode 30/1000 - Min reward: -29.2784, Max reward: -16.2703, Mean reward: -26.0229\n",
      "Episode 30/1000 - Policy loss: -0.1912, Value loss: 47.9052, Entropy: 9.1542\n",
      "Episode 31/1000 - Min reward: -22.4611, Max reward: -8.8478, Mean reward: -18.6771\n",
      "Episode 31/1000 - Policy loss: -0.2659, Value loss: 18.4156, Entropy: 9.1625\n",
      "Episode 32/1000 - Min reward: -33.5164, Max reward: -17.6517, Mean reward: -23.8067\n",
      "Episode 32/1000 - Policy loss: 0.1273, Value loss: 140.1660, Entropy: 9.1596\n",
      "Episode 33/1000 - Min reward: -24.8950, Max reward: -4.9973, Mean reward: -16.3395\n",
      "Episode 33/1000 - Policy loss: -0.2419, Value loss: 18.9441, Entropy: 9.1622\n",
      "Episode 34/1000 - Min reward: -28.2685, Max reward: -23.3381, Mean reward: -25.9406\n",
      "Episode 34/1000 - Policy loss: -0.1960, Value loss: 29.4362, Entropy: 9.1559\n",
      "Episode 35/1000 - Min reward: -23.6552, Max reward: -14.8078, Mean reward: -18.3765\n",
      "Episode 35/1000 - Policy loss: -0.2278, Value loss: 102.9583, Entropy: 9.1541\n",
      "Episode 36/1000 - Min reward: -20.2714, Max reward: -6.9600, Mean reward: -15.5229\n",
      "Episode 36/1000 - Policy loss: -0.0743, Value loss: 51.2812, Entropy: 9.1375\n",
      "Episode 37/1000 - Min reward: -20.2775, Max reward: -11.8554, Mean reward: -15.4022\n",
      "Episode 37/1000 - Policy loss: 0.0397, Value loss: 21.6991, Entropy: 9.1308\n",
      "Episode 38/1000 - Min reward: -28.0246, Max reward: -17.0371, Mean reward: -21.7024\n",
      "Episode 38/1000 - Policy loss: 0.0300, Value loss: 74.9123, Entropy: 9.1278\n",
      "Episode 39/1000 - Min reward: -27.6355, Max reward: -9.2709, Mean reward: -19.1086\n",
      "Episode 39/1000 - Policy loss: -0.1060, Value loss: 28.2469, Entropy: 9.1311\n",
      "Episode 40/1000 - Min reward: -22.6525, Max reward: -18.8489, Mean reward: -20.5738\n",
      "Episode 40/1000 - Policy loss: -0.0194, Value loss: 23.6634, Entropy: 9.1243\n",
      "Episode 41/1000 - Min reward: -23.9165, Max reward: -12.6996, Mean reward: -20.0303\n",
      "Episode 41/1000 - Policy loss: 0.1826, Value loss: 24.8444, Entropy: 9.1364\n",
      "Episode 42/1000 - Min reward: -21.5978, Max reward: -7.3094, Mean reward: -13.6575\n",
      "Episode 42/1000 - Policy loss: 0.0671, Value loss: 27.8441, Entropy: 9.1470\n",
      "Episode 43/1000 - Min reward: -16.6980, Max reward: -11.7810, Mean reward: -14.1251\n",
      "Episode 43/1000 - Policy loss: -0.0483, Value loss: 10.6515, Entropy: 9.1463\n",
      "Episode 44/1000 - Min reward: -13.3256, Max reward: -6.4192, Mean reward: -10.6814\n",
      "Episode 44/1000 - Policy loss: -0.1120, Value loss: 9.6101, Entropy: 9.1428\n",
      "Episode 45/1000 - Min reward: -32.4273, Max reward: -11.4842, Mean reward: -20.8764\n",
      "Episode 45/1000 - Policy loss: 0.0027, Value loss: 52.7247, Entropy: 9.1592\n",
      "Episode 46/1000 - Min reward: -30.0775, Max reward: -9.8737, Mean reward: -21.5570\n",
      "Episode 46/1000 - Policy loss: 0.0239, Value loss: 42.7193, Entropy: 9.1558\n",
      "Episode 47/1000 - Min reward: -28.8680, Max reward: -7.8278, Mean reward: -17.4447\n",
      "Episode 47/1000 - Policy loss: -0.0983, Value loss: 34.0089, Entropy: 9.1386\n",
      "Episode 48/1000 - Min reward: -16.5364, Max reward: -6.1051, Mean reward: -11.4760\n",
      "Episode 48/1000 - Policy loss: -0.0526, Value loss: 26.4690, Entropy: 9.1563\n",
      "Episode 49/1000 - Min reward: -18.9293, Max reward: -10.1747, Mean reward: -13.7196\n",
      "Episode 49/1000 - Policy loss: 0.0946, Value loss: 17.6257, Entropy: 9.1656\n",
      "Episode 50/1000 - Min reward: -15.8474, Max reward: -10.4548, Mean reward: -12.5694\n",
      "Episode 50/1000 - Policy loss: -0.1117, Value loss: 3.3140, Entropy: 9.1632\n",
      "Episode 51/1000 - Min reward: -20.9697, Max reward: -8.9611, Mean reward: -11.6818\n",
      "Episode 51/1000 - Policy loss: -0.3637, Value loss: 25.1763, Entropy: 9.1569\n",
      "Episode 52/1000 - Min reward: -19.9830, Max reward: -0.4065, Mean reward: -7.2574\n",
      "Episode 52/1000 - Policy loss: -0.0972, Value loss: 13.8401, Entropy: 9.1504\n",
      "Episode 53/1000 - Min reward: -22.6263, Max reward: -5.8876, Mean reward: -12.0188\n",
      "Episode 53/1000 - Policy loss: -0.0196, Value loss: 23.7457, Entropy: 9.1338\n",
      "Episode 54/1000 - Min reward: -29.1236, Max reward: -14.1537, Mean reward: -17.1362\n",
      "Episode 54/1000 - Policy loss: -0.1871, Value loss: 34.2787, Entropy: 9.1057\n",
      "Episode 55/1000 - Min reward: -27.4809, Max reward: -12.3835, Mean reward: -19.5334\n",
      "Episode 55/1000 - Policy loss: -0.1299, Value loss: 15.9663, Entropy: 9.0908\n",
      "Episode 56/1000 - Min reward: -14.2891, Max reward: -6.7727, Mean reward: -9.3559\n",
      "Episode 56/1000 - Policy loss: -0.0872, Value loss: 23.9935, Entropy: 9.0734\n",
      "Episode 57/1000 - Min reward: -18.9505, Max reward: -6.4450, Mean reward: -10.2061\n",
      "Episode 57/1000 - Policy loss: -0.0065, Value loss: 12.4375, Entropy: 9.0604\n",
      "Episode 58/1000 - Min reward: -21.5129, Max reward: -10.0805, Mean reward: -14.0429\n",
      "Episode 58/1000 - Policy loss: -0.1190, Value loss: 18.8701, Entropy: 9.0549\n",
      "Episode 59/1000 - Min reward: -20.9682, Max reward: -9.4595, Mean reward: -12.6523\n",
      "Episode 59/1000 - Policy loss: -0.1084, Value loss: 21.2231, Entropy: 9.0437\n",
      "Episode 60/1000 - Min reward: -22.2069, Max reward: -8.6888, Mean reward: -12.2119\n",
      "Episode 60/1000 - Policy loss: -0.1780, Value loss: 20.3284, Entropy: 9.0452\n",
      "Episode 61/1000 - Min reward: -29.7422, Max reward: -11.4493, Mean reward: -18.2317\n",
      "Episode 61/1000 - Policy loss: -0.2402, Value loss: 11.8739, Entropy: 9.0528\n",
      "Episode 62/1000 - Min reward: -21.5188, Max reward: -11.9233, Mean reward: -14.6606\n",
      "Episode 62/1000 - Policy loss: -0.2609, Value loss: 10.1483, Entropy: 9.0558\n",
      "Episode 63/1000 - Min reward: -24.4714, Max reward: -14.6337, Mean reward: -18.8598\n",
      "Episode 63/1000 - Policy loss: -0.1917, Value loss: 29.0047, Entropy: 9.0404\n",
      "Episode 64/1000 - Min reward: -14.5995, Max reward: -1.5526, Mean reward: -5.3886\n",
      "Episode 64/1000 - Policy loss: -0.0906, Value loss: 18.2534, Entropy: 9.0323\n",
      "Episode 65/1000 - Min reward: -14.3507, Max reward: -1.4383, Mean reward: -5.4814\n",
      "Episode 65/1000 - Policy loss: -0.3143, Value loss: 29.4832, Entropy: 9.0426\n",
      "Episode 66/1000 - Min reward: -13.0111, Max reward: -3.5155, Mean reward: -5.5855\n",
      "Episode 66/1000 - Policy loss: -0.1731, Value loss: 5.9855, Entropy: 9.0527\n",
      "Episode 67/1000 - Min reward: -18.8364, Max reward: -10.9958, Mean reward: -13.9846\n",
      "Episode 67/1000 - Policy loss: -0.3167, Value loss: 15.2529, Entropy: 9.0655\n",
      "Episode 68/1000 - Min reward: -11.9190, Max reward: -2.7517, Mean reward: -6.2601\n",
      "Episode 68/1000 - Policy loss: -0.1884, Value loss: 11.2905, Entropy: 9.0636\n",
      "Episode 69/1000 - Min reward: -20.5713, Max reward: -5.0347, Mean reward: -8.1852\n",
      "Episode 69/1000 - Policy loss: -0.0755, Value loss: 2.2642, Entropy: 9.0536\n",
      "Episode 70/1000 - Min reward: -22.6396, Max reward: -15.1606, Mean reward: -17.4650\n",
      "Episode 70/1000 - Policy loss: -0.1110, Value loss: 6.4248, Entropy: 9.0449\n",
      "Episode 71/1000 - Min reward: -25.7514, Max reward: -6.7567, Mean reward: -16.6685\n",
      "Episode 71/1000 - Policy loss: -0.3049, Value loss: 8.0652, Entropy: 9.0441\n",
      "Episode 72/1000 - Min reward: -12.3672, Max reward: 0.3398, Mean reward: -2.6983\n",
      "Episode 72/1000 - Policy loss: -0.1155, Value loss: 56.4398, Entropy: 9.0453\n",
      "Episode 73/1000 - Min reward: -7.4126, Max reward: -1.9999, Mean reward: -4.5018\n",
      "Episode 73/1000 - Policy loss: -0.0757, Value loss: 29.7334, Entropy: 9.0414\n",
      "Episode 74/1000 - Min reward: -30.9301, Max reward: -8.0548, Mean reward: -11.7370\n",
      "Episode 74/1000 - Policy loss: -0.1140, Value loss: 22.9740, Entropy: 9.0430\n",
      "Episode 75/1000 - Min reward: -14.3937, Max reward: -3.5617, Mean reward: -6.2553\n",
      "Episode 75/1000 - Policy loss: -0.1788, Value loss: 21.6174, Entropy: 9.0330\n",
      "Episode 76/1000 - Min reward: -26.6481, Max reward: -7.6531, Mean reward: -14.3199\n",
      "Episode 76/1000 - Policy loss: 0.0187, Value loss: 15.4048, Entropy: 9.0457\n",
      "Episode 77/1000 - Min reward: -15.7790, Max reward: -3.9352, Mean reward: -7.5194\n",
      "Episode 77/1000 - Policy loss: -0.0605, Value loss: 25.4361, Entropy: 9.0614\n",
      "Episode 78/1000 - Min reward: -24.6594, Max reward: -6.0478, Mean reward: -11.9192\n",
      "Episode 78/1000 - Policy loss: -0.2056, Value loss: 9.0827, Entropy: 9.0626\n",
      "Episode 79/1000 - Min reward: -21.5949, Max reward: -4.6049, Mean reward: -10.8209\n",
      "Episode 79/1000 - Policy loss: -0.5127, Value loss: 7.1394, Entropy: 9.0540\n",
      "Episode 80/1000 - Min reward: -23.8620, Max reward: -7.1329, Mean reward: -12.2564\n",
      "Episode 80/1000 - Policy loss: -0.2781, Value loss: 15.2235, Entropy: 9.0404\n",
      "Episode 81/1000 - Min reward: -22.1281, Max reward: -2.1607, Mean reward: -9.0734\n",
      "Episode 81/1000 - Policy loss: -0.0835, Value loss: 13.5962, Entropy: 9.0452\n",
      "Episode 82/1000 - Min reward: -21.4387, Max reward: -4.7245, Mean reward: -10.9054\n",
      "Episode 82/1000 - Policy loss: -0.1405, Value loss: 9.3625, Entropy: 9.0414\n",
      "Episode 83/1000 - Min reward: -15.6112, Max reward: -2.4816, Mean reward: -4.7401\n",
      "Episode 83/1000 - Policy loss: 0.1076, Value loss: 8.2251, Entropy: 9.0266\n",
      "Episode 84/1000 - Min reward: -16.8643, Max reward: -2.6055, Mean reward: -8.0480\n",
      "Episode 84/1000 - Policy loss: -0.2500, Value loss: 8.2666, Entropy: 9.0143\n",
      "Episode 85/1000 - Min reward: -23.9459, Max reward: -3.5284, Mean reward: -6.9392\n",
      "Episode 85/1000 - Policy loss: 0.0483, Value loss: 13.1069, Entropy: 9.0181\n",
      "Episode 86/1000 - Min reward: -18.4411, Max reward: -3.5814, Mean reward: -6.8416\n",
      "Episode 86/1000 - Policy loss: -0.4276, Value loss: 10.2011, Entropy: 9.0220\n",
      "Episode 87/1000 - Min reward: -13.5184, Max reward: -2.1791, Mean reward: -4.4387\n",
      "Episode 87/1000 - Policy loss: -0.0418, Value loss: 7.2319, Entropy: 9.0166\n",
      "Episode 88/1000 - Min reward: -9.4314, Max reward: -0.6274, Mean reward: -3.7369\n",
      "Episode 88/1000 - Policy loss: -0.4720, Value loss: 19.4740, Entropy: 9.0165\n",
      "Episode 89/1000 - Min reward: -15.9392, Max reward: -4.7363, Mean reward: -10.7823\n",
      "Episode 89/1000 - Policy loss: 0.0862, Value loss: 33.0137, Entropy: 9.0180\n",
      "Episode 90/1000 - Min reward: -11.7854, Max reward: -3.8930, Mean reward: -7.3476\n",
      "Episode 90/1000 - Policy loss: 0.1429, Value loss: 114.4804, Entropy: 9.0034\n",
      "Episode 91/1000 - Min reward: -17.4432, Max reward: -9.6050, Mean reward: -14.2872\n",
      "Episode 91/1000 - Policy loss: -0.2052, Value loss: 39.8856, Entropy: 9.0013\n",
      "Episode 92/1000 - Min reward: -19.6472, Max reward: -6.5575, Mean reward: -9.7842\n",
      "Episode 92/1000 - Policy loss: -0.2232, Value loss: 30.0477, Entropy: 9.0025\n",
      "Episode 93/1000 - Min reward: -23.8013, Max reward: -5.9180, Mean reward: -12.8410\n",
      "Episode 93/1000 - Policy loss: -0.1591, Value loss: 12.2169, Entropy: 9.0017\n",
      "Episode 94/1000 - Min reward: -23.9818, Max reward: -3.4706, Mean reward: -11.9556\n",
      "Episode 94/1000 - Policy loss: -0.0823, Value loss: 17.4919, Entropy: 8.9970\n",
      "Episode 95/1000 - Min reward: -15.1954, Max reward: -6.8620, Mean reward: -10.3631\n",
      "Episode 95/1000 - Policy loss: -0.1901, Value loss: 131.2239, Entropy: 8.9972\n",
      "Episode 96/1000 - Min reward: -27.3783, Max reward: -9.9675, Mean reward: -15.0645\n",
      "Episode 96/1000 - Policy loss: 0.0126, Value loss: 10.7328, Entropy: 8.9944\n",
      "Episode 97/1000 - Min reward: -5.0583, Max reward: 0.9490, Mean reward: -2.3766\n",
      "Episode 97/1000 - Policy loss: -0.1195, Value loss: 99.7843, Entropy: 8.9944\n",
      "Episode 98/1000 - Min reward: -27.3981, Max reward: -3.0820, Mean reward: -11.5815\n",
      "Episode 98/1000 - Policy loss: -0.1179, Value loss: 22.9002, Entropy: 8.9853\n",
      "Episode 99/1000 - Min reward: -14.8462, Max reward: -5.9435, Mean reward: -9.5293\n",
      "Episode 99/1000 - Policy loss: -0.1581, Value loss: 34.2660, Entropy: 8.9795\n",
      "Episode 100/1000 - Min reward: -18.6019, Max reward: -0.8839, Mean reward: -6.7523\n",
      "Episode 100/1000 - Policy loss: -0.2981, Value loss: 15.1867, Entropy: 8.9832\n",
      "Episode 101/1000 - Min reward: -9.3311, Max reward: -4.4980, Mean reward: -6.6933\n",
      "Episode 101/1000 - Policy loss: -0.2363, Value loss: 65.9994, Entropy: 8.9822\n",
      "Episode 102/1000 - Min reward: -17.8746, Max reward: -2.8333, Mean reward: -8.5728\n",
      "Episode 102/1000 - Policy loss: -0.1244, Value loss: 22.1348, Entropy: 8.9756\n",
      "Episode 103/1000 - Min reward: -9.4948, Max reward: -2.7128, Mean reward: -6.0721\n",
      "Episode 103/1000 - Policy loss: 0.2606, Value loss: 59.7064, Entropy: 8.9808\n",
      "Episode 104/1000 - Min reward: -17.5599, Max reward: 0.9989, Mean reward: -2.0333\n",
      "Episode 104/1000 - Policy loss: 0.0125, Value loss: 40.1830, Entropy: 8.9902\n",
      "Episode 105/1000 - Min reward: -12.4681, Max reward: -3.4312, Mean reward: -6.6838\n",
      "Episode 105/1000 - Policy loss: -0.1933, Value loss: 34.6883, Entropy: 9.0079\n",
      "Episode 106/1000 - Min reward: -15.7014, Max reward: -3.6017, Mean reward: -7.5880\n",
      "Episode 106/1000 - Policy loss: 0.0945, Value loss: 15.1669, Entropy: 9.0218\n",
      "Episode 107/1000 - Min reward: -18.9555, Max reward: -10.9990, Mean reward: -15.0709\n",
      "Episode 107/1000 - Policy loss: -0.2694, Value loss: 40.3338, Entropy: 9.0357\n",
      "Episode 108/1000 - Min reward: -12.4895, Max reward: -1.3406, Mean reward: -7.3176\n",
      "Episode 108/1000 - Policy loss: -0.3465, Value loss: 62.0881, Entropy: 9.0497\n",
      "Episode 109/1000 - Min reward: -11.6993, Max reward: 0.0971, Mean reward: -1.6482\n",
      "Episode 109/1000 - Policy loss: -0.1011, Value loss: 17.0501, Entropy: 9.0600\n",
      "Episode 110/1000 - Min reward: -28.3037, Max reward: -9.8580, Mean reward: -17.3570\n",
      "Episode 110/1000 - Policy loss: -0.4032, Value loss: 12.6263, Entropy: 9.0528\n",
      "Episode 111/1000 - Min reward: -14.1288, Max reward: -1.2462, Mean reward: -4.4243\n",
      "Episode 111/1000 - Policy loss: -0.3682, Value loss: 9.6444, Entropy: 9.0571\n",
      "Episode 112/1000 - Min reward: -14.7477, Max reward: -7.0919, Mean reward: -10.4647\n",
      "Episode 112/1000 - Policy loss: 0.0406, Value loss: 22.0303, Entropy: 9.0581\n",
      "Episode 113/1000 - Min reward: -27.8521, Max reward: -16.1183, Mean reward: -19.6028\n",
      "Episode 113/1000 - Policy loss: 0.0301, Value loss: 31.3010, Entropy: 9.0568\n",
      "Episode 114/1000 - Min reward: -24.5518, Max reward: -8.9950, Mean reward: -15.3447\n",
      "Episode 114/1000 - Policy loss: -0.2672, Value loss: 22.2168, Entropy: 9.0541\n",
      "Episode 115/1000 - Min reward: -13.1486, Max reward: -3.2559, Mean reward: -5.7947\n",
      "Episode 115/1000 - Policy loss: 0.0846, Value loss: 24.0464, Entropy: 9.0581\n",
      "Episode 116/1000 - Min reward: -22.2781, Max reward: -11.6892, Mean reward: -15.7790\n",
      "Episode 116/1000 - Policy loss: -0.0366, Value loss: 39.6491, Entropy: 9.0694\n",
      "Episode 117/1000 - Min reward: -12.9096, Max reward: 0.6604, Mean reward: -4.9038\n",
      "Episode 117/1000 - Policy loss: -0.1612, Value loss: 89.6829, Entropy: 9.0662\n",
      "Episode 118/1000 - Min reward: -9.5924, Max reward: -3.8470, Mean reward: -6.2466\n",
      "Episode 118/1000 - Policy loss: 0.0278, Value loss: 35.8963, Entropy: 9.0567\n",
      "Episode 119/1000 - Min reward: -18.3761, Max reward: -3.0983, Mean reward: -6.7178\n",
      "Episode 119/1000 - Policy loss: -0.0142, Value loss: 18.2835, Entropy: 9.0478\n",
      "Episode 120/1000 - Min reward: -12.4673, Max reward: -2.0192, Mean reward: -5.4278\n",
      "Episode 120/1000 - Policy loss: -0.2113, Value loss: 18.0814, Entropy: 9.0456\n",
      "Episode 121/1000 - Min reward: -9.4616, Max reward: -3.5556, Mean reward: -5.4854\n",
      "Episode 121/1000 - Policy loss: -0.1054, Value loss: 20.9877, Entropy: 9.0329\n",
      "Episode 122/1000 - Min reward: -12.9744, Max reward: -4.4183, Mean reward: -8.2311\n",
      "Episode 122/1000 - Policy loss: -0.0164, Value loss: 65.0663, Entropy: 9.0173\n",
      "Episode 123/1000 - Min reward: -24.8523, Max reward: -6.2414, Mean reward: -11.4137\n",
      "Episode 123/1000 - Policy loss: -0.1384, Value loss: 15.0121, Entropy: 9.0160\n",
      "Episode 124/1000 - Min reward: -17.8313, Max reward: -5.7700, Mean reward: -10.9895\n",
      "Episode 124/1000 - Policy loss: -0.1349, Value loss: 5.2405, Entropy: 9.0118\n",
      "Episode 125/1000 - Min reward: -18.1228, Max reward: -3.3608, Mean reward: -9.9295\n",
      "Episode 125/1000 - Policy loss: -0.1432, Value loss: 15.4065, Entropy: 9.0131\n",
      "Episode 126/1000 - Min reward: -14.5872, Max reward: -0.9281, Mean reward: -9.1658\n",
      "Episode 126/1000 - Policy loss: 0.1043, Value loss: 216.7482, Entropy: 9.0222\n",
      "Episode 127/1000 - Min reward: -16.1266, Max reward: -0.0367, Mean reward: -5.2938\n",
      "Episode 127/1000 - Policy loss: -0.1683, Value loss: 7.9665, Entropy: 9.0220\n",
      "Episode 128/1000 - Min reward: -8.5598, Max reward: -0.1665, Mean reward: -3.5217\n",
      "Episode 128/1000 - Policy loss: 0.0319, Value loss: 30.0251, Entropy: 9.0107\n",
      "Episode 129/1000 - Min reward: -6.9202, Max reward: 0.1170, Mean reward: -3.2102\n",
      "Episode 129/1000 - Policy loss: -0.3482, Value loss: 28.5004, Entropy: 9.0024\n",
      "Episode 130/1000 - Min reward: -24.6797, Max reward: -4.2452, Mean reward: -8.9268\n",
      "Episode 130/1000 - Policy loss: -0.1626, Value loss: 31.6153, Entropy: 9.0139\n",
      "Episode 131/1000 - Min reward: -8.5303, Max reward: 0.8698, Mean reward: -4.8992\n",
      "Episode 131/1000 - Policy loss: -0.1456, Value loss: 99.8906, Entropy: 8.9957\n",
      "Episode 132/1000 - Min reward: -16.1992, Max reward: -4.0030, Mean reward: -8.2638\n",
      "Episode 132/1000 - Policy loss: -0.0181, Value loss: 43.7233, Entropy: 8.9859\n",
      "Episode 133/1000 - Min reward: -27.0389, Max reward: -9.2137, Mean reward: -18.1905\n",
      "Episode 133/1000 - Policy loss: -0.0362, Value loss: 22.0291, Entropy: 8.9869\n",
      "Episode 134/1000 - Min reward: -6.6612, Max reward: 0.9551, Mean reward: -3.3304\n",
      "Episode 134/1000 - Policy loss: 0.0443, Value loss: 91.9639, Entropy: 8.9856\n",
      "Episode 135/1000 - Min reward: -18.1522, Max reward: -3.4809, Mean reward: -9.6151\n",
      "Episode 135/1000 - Policy loss: 0.0057, Value loss: 26.1773, Entropy: 8.9956\n",
      "Episode 136/1000 - Min reward: -17.0497, Max reward: -4.0948, Mean reward: -10.0586\n",
      "Episode 136/1000 - Policy loss: -0.1003, Value loss: 1.1762, Entropy: 9.0012\n",
      "Episode 137/1000 - Min reward: -26.2136, Max reward: -3.1404, Mean reward: -8.6616\n",
      "Episode 137/1000 - Policy loss: -0.2422, Value loss: 13.4279, Entropy: 9.0026\n",
      "Episode 138/1000 - Min reward: -19.4803, Max reward: -3.2331, Mean reward: -5.2586\n",
      "Episode 138/1000 - Policy loss: -0.1214, Value loss: 18.7278, Entropy: 9.0034\n",
      "Episode 139/1000 - Min reward: -26.1533, Max reward: -4.6277, Mean reward: -9.7732\n",
      "Episode 139/1000 - Policy loss: -0.2473, Value loss: 3.5307, Entropy: 9.0069\n",
      "Episode 140/1000 - Min reward: -9.0484, Max reward: 0.0949, Mean reward: -3.4303\n",
      "Episode 140/1000 - Policy loss: -0.2272, Value loss: 24.1887, Entropy: 9.0192\n",
      "Episode 141/1000 - Min reward: -7.0463, Max reward: -3.1699, Mean reward: -4.2566\n",
      "Episode 141/1000 - Policy loss: -0.1783, Value loss: 4.5491, Entropy: 9.0315\n",
      "Episode 142/1000 - Min reward: -13.0998, Max reward: -3.9010, Mean reward: -7.5111\n",
      "Episode 142/1000 - Policy loss: -0.1540, Value loss: 39.0005, Entropy: 9.0469\n",
      "Episode 143/1000 - Min reward: -15.0412, Max reward: -1.6579, Mean reward: -5.1230\n",
      "Episode 143/1000 - Policy loss: 0.0595, Value loss: 8.6160, Entropy: 9.0537\n",
      "Episode 144/1000 - Min reward: -19.6060, Max reward: -3.2202, Mean reward: -6.6520\n",
      "Episode 144/1000 - Policy loss: -0.2689, Value loss: 22.6731, Entropy: 9.0600\n",
      "Episode 145/1000 - Min reward: -18.1384, Max reward: -3.4645, Mean reward: -9.3812\n",
      "Episode 145/1000 - Policy loss: 0.0945, Value loss: 11.1998, Entropy: 9.0617\n",
      "Episode 146/1000 - Min reward: -22.9011, Max reward: 0.0759, Mean reward: -6.9690\n",
      "Episode 146/1000 - Policy loss: -0.2149, Value loss: 11.3060, Entropy: 9.0660\n",
      "Episode 147/1000 - Min reward: -10.2024, Max reward: -3.3764, Mean reward: -4.2154\n",
      "Episode 147/1000 - Policy loss: -0.4175, Value loss: 9.2593, Entropy: 9.0635\n",
      "Episode 148/1000 - Min reward: -7.3586, Max reward: 0.3256, Mean reward: -2.8962\n",
      "Episode 148/1000 - Policy loss: 0.1655, Value loss: 125.9150, Entropy: 9.0585\n",
      "Episode 149/1000 - Min reward: -16.3777, Max reward: -3.2126, Mean reward: -5.8377\n",
      "Episode 149/1000 - Policy loss: -0.0062, Value loss: 8.6951, Entropy: 9.0686\n",
      "Episode 150/1000 - Min reward: -23.3109, Max reward: -7.1416, Mean reward: -13.3039\n",
      "Episode 150/1000 - Policy loss: -0.3131, Value loss: 9.0076, Entropy: 9.0625\n",
      "Episode 151/1000 - Min reward: -19.7814, Max reward: -3.4680, Mean reward: -6.3233\n",
      "Episode 151/1000 - Policy loss: -0.0059, Value loss: 11.2964, Entropy: 9.0429\n",
      "Episode 152/1000 - Min reward: -4.1988, Max reward: 0.9834, Mean reward: -1.7180\n",
      "Episode 152/1000 - Policy loss: -0.2478, Value loss: 116.8566, Entropy: 9.0407\n",
      "Episode 153/1000 - Min reward: -11.2430, Max reward: -2.8412, Mean reward: -5.5141\n",
      "Episode 153/1000 - Policy loss: -0.2035, Value loss: 24.8035, Entropy: 9.0383\n",
      "Episode 154/1000 - Min reward: -17.0370, Max reward: -3.6459, Mean reward: -7.2777\n",
      "Episode 154/1000 - Policy loss: 0.0564, Value loss: 6.6403, Entropy: 9.0231\n",
      "Episode 155/1000 - Min reward: -9.8145, Max reward: -4.4687, Mean reward: -6.5528\n",
      "Episode 155/1000 - Policy loss: -0.1229, Value loss: 6.2602, Entropy: 9.0037\n",
      "Episode 156/1000 - Min reward: -10.3718, Max reward: 0.1323, Mean reward: -1.6923\n",
      "Episode 156/1000 - Policy loss: -0.2651, Value loss: 8.9548, Entropy: 9.0042\n",
      "Episode 157/1000 - Min reward: -11.3533, Max reward: -3.0201, Mean reward: -5.3460\n",
      "Episode 157/1000 - Policy loss: 0.1587, Value loss: 29.7097, Entropy: 9.0285\n",
      "Episode 158/1000 - Min reward: -16.8342, Max reward: -3.3165, Mean reward: -9.6215\n",
      "Episode 158/1000 - Policy loss: -0.4137, Value loss: 7.9773, Entropy: 9.0390\n",
      "Episode 159/1000 - Min reward: -10.7876, Max reward: 0.4381, Mean reward: -3.0638\n",
      "Episode 159/1000 - Policy loss: 0.1546, Value loss: 61.3923, Entropy: 9.0391\n",
      "Episode 160/1000 - Min reward: -19.0633, Max reward: -3.3828, Mean reward: -7.0806\n",
      "Episode 160/1000 - Policy loss: -0.0046, Value loss: 6.5929, Entropy: 9.0296\n",
      "Episode 161/1000 - Min reward: -10.5902, Max reward: -3.0170, Mean reward: -5.9710\n",
      "Episode 161/1000 - Policy loss: 0.1087, Value loss: 16.2283, Entropy: 9.0134\n",
      "Episode 162/1000 - Min reward: -27.5302, Max reward: 0.1005, Mean reward: -6.4987\n",
      "Episode 162/1000 - Policy loss: 0.0163, Value loss: 17.3774, Entropy: 9.0044\n",
      "Episode 163/1000 - Min reward: -9.4499, Max reward: -1.3840, Mean reward: -3.9948\n",
      "Episode 163/1000 - Policy loss: 0.0870, Value loss: 18.0472, Entropy: 8.9941\n",
      "Episode 164/1000 - Min reward: -16.6199, Max reward: -2.9607, Mean reward: -7.4411\n",
      "Episode 164/1000 - Policy loss: -0.1620, Value loss: 12.6020, Entropy: 8.9811\n",
      "Episode 165/1000 - Min reward: -6.4988, Max reward: 0.0759, Mean reward: -0.8747\n",
      "Episode 165/1000 - Policy loss: 0.0148, Value loss: 15.5896, Entropy: 8.9858\n",
      "Episode 166/1000 - Min reward: -16.3660, Max reward: 0.1343, Mean reward: -2.3119\n",
      "Episode 166/1000 - Policy loss: -0.1561, Value loss: 8.4506, Entropy: 8.9884\n",
      "Episode 167/1000 - Min reward: -17.3820, Max reward: -5.2218, Mean reward: -10.4870\n",
      "Episode 167/1000 - Policy loss: -0.0874, Value loss: 7.8223, Entropy: 8.9918\n",
      "Episode 168/1000 - Min reward: -0.5286, Max reward: 0.9993, Mean reward: 0.3437\n",
      "Episode 168/1000 - Policy loss: -0.3521, Value loss: 79.8028, Entropy: 8.9912\n",
      "Episode 169/1000 - Min reward: -22.4692, Max reward: -3.7321, Mean reward: -8.9287\n",
      "Episode 169/1000 - Policy loss: -0.3264, Value loss: 14.3850, Entropy: 8.9914\n",
      "Episode 170/1000 - Min reward: -12.9916, Max reward: -4.4352, Mean reward: -7.5870\n",
      "Episode 170/1000 - Policy loss: -0.0023, Value loss: 20.1443, Entropy: 8.9956\n",
      "Episode 171/1000 - Min reward: -12.5046, Max reward: -0.2968, Mean reward: -3.7527\n",
      "Episode 171/1000 - Policy loss: -0.2238, Value loss: 8.1815, Entropy: 9.0069\n",
      "Episode 172/1000 - Min reward: -20.7404, Max reward: 0.0759, Mean reward: -5.5174\n",
      "Episode 172/1000 - Policy loss: -0.0054, Value loss: 5.0909, Entropy: 9.0152\n",
      "Episode 173/1000 - Min reward: -12.3676, Max reward: 0.9897, Mean reward: -0.6494\n",
      "Episode 173/1000 - Policy loss: 0.0983, Value loss: 11.4819, Entropy: 9.0138\n",
      "Episode 174/1000 - Min reward: -13.6607, Max reward: -3.0300, Mean reward: -5.1584\n",
      "Episode 174/1000 - Policy loss: -0.0729, Value loss: 9.0147, Entropy: 9.0099\n",
      "Episode 175/1000 - Min reward: -15.1069, Max reward: -2.0941, Mean reward: -10.3941\n",
      "Episode 175/1000 - Policy loss: -0.0279, Value loss: 12.3978, Entropy: 9.0160\n",
      "Episode 176/1000 - Min reward: -14.9932, Max reward: -3.5190, Mean reward: -5.1960\n",
      "Episode 176/1000 - Policy loss: 0.0136, Value loss: 10.6712, Entropy: 9.0301\n",
      "Episode 177/1000 - Min reward: -15.1865, Max reward: -1.0798, Mean reward: -6.5899\n",
      "Episode 177/1000 - Policy loss: 0.0717, Value loss: 23.3055, Entropy: 9.0346\n",
      "Episode 178/1000 - Min reward: -19.0690, Max reward: 0.0759, Mean reward: -2.8069\n",
      "Episode 178/1000 - Policy loss: -0.1655, Value loss: 24.8108, Entropy: 9.0329\n",
      "Episode 179/1000 - Min reward: -23.0648, Max reward: 0.0759, Mean reward: -7.2178\n",
      "Episode 179/1000 - Policy loss: -0.0783, Value loss: 6.5038, Entropy: 9.0322\n",
      "Episode 180/1000 - Min reward: -14.9316, Max reward: 0.0769, Mean reward: -2.5424\n",
      "Episode 180/1000 - Policy loss: -0.3266, Value loss: 6.0730, Entropy: 9.0275\n",
      "Episode 181/1000 - Min reward: -15.6024, Max reward: 0.0759, Mean reward: -5.7418\n",
      "Episode 181/1000 - Policy loss: 0.1466, Value loss: 6.9797, Entropy: 9.0381\n",
      "Episode 182/1000 - Min reward: -26.6535, Max reward: -2.5844, Mean reward: -6.2644\n",
      "Episode 182/1000 - Policy loss: 0.1045, Value loss: 12.5476, Entropy: 9.0408\n",
      "Episode 183/1000 - Min reward: -10.3021, Max reward: -2.3303, Mean reward: -5.5283\n",
      "Episode 183/1000 - Policy loss: -0.2035, Value loss: 23.6440, Entropy: 9.0543\n",
      "Episode 184/1000 - Min reward: -15.6778, Max reward: 0.0759, Mean reward: -6.1272\n",
      "Episode 184/1000 - Policy loss: -0.1000, Value loss: 7.7509, Entropy: 9.0627\n",
      "Episode 185/1000 - Min reward: -3.6411, Max reward: 0.4063, Mean reward: -0.5578\n",
      "Episode 185/1000 - Policy loss: 0.0738, Value loss: 79.7066, Entropy: 9.0734\n",
      "Episode 186/1000 - Min reward: -25.9751, Max reward: 0.0759, Mean reward: -4.2919\n",
      "Episode 186/1000 - Policy loss: -0.0699, Value loss: 7.6636, Entropy: 9.0714\n",
      "Episode 187/1000 - Min reward: -9.8065, Max reward: -0.8079, Mean reward: -3.3655\n",
      "Episode 187/1000 - Policy loss: 0.0634, Value loss: 26.4428, Entropy: 9.0714\n",
      "Episode 188/1000 - Min reward: -11.5679, Max reward: 0.0759, Mean reward: -4.1194\n",
      "Episode 188/1000 - Policy loss: 0.1289, Value loss: 42.8593, Entropy: 9.0788\n",
      "Episode 189/1000 - Min reward: -17.5522, Max reward: 0.0759, Mean reward: -4.6897\n",
      "Episode 189/1000 - Policy loss: 0.0156, Value loss: 13.4136, Entropy: 9.0658\n",
      "Episode 190/1000 - Min reward: -20.7155, Max reward: 0.0834, Mean reward: -4.0474\n",
      "Episode 190/1000 - Policy loss: -0.2456, Value loss: 12.1054, Entropy: 9.0607\n",
      "Episode 191/1000 - Min reward: -27.5018, Max reward: -1.1705, Mean reward: -8.4933\n",
      "Episode 191/1000 - Policy loss: -0.1728, Value loss: 20.6514, Entropy: 9.0552\n",
      "Episode 192/1000 - Min reward: -1.6362, Max reward: 0.9401, Mean reward: 0.0338\n",
      "Episode 192/1000 - Policy loss: -0.3106, Value loss: 11.7879, Entropy: 9.0580\n",
      "Episode 193/1000 - Min reward: -13.8025, Max reward: -1.6261, Mean reward: -4.3143\n",
      "Episode 193/1000 - Policy loss: -0.1811, Value loss: 8.1376, Entropy: 9.0658\n",
      "Episode 194/1000 - Min reward: -3.7257, Max reward: 0.3680, Mean reward: -1.0742\n",
      "Episode 194/1000 - Policy loss: -0.0618, Value loss: 40.7036, Entropy: 9.0702\n",
      "Episode 195/1000 - Min reward: -13.3748, Max reward: -0.3042, Mean reward: -3.2735\n",
      "Episode 195/1000 - Policy loss: -0.3037, Value loss: 12.1834, Entropy: 9.0788\n",
      "Episode 196/1000 - Min reward: -10.7798, Max reward: 0.0759, Mean reward: -4.6659\n",
      "Episode 196/1000 - Policy loss: -0.1451, Value loss: 17.1880, Entropy: 9.0784\n",
      "Episode 197/1000 - Min reward: -22.4787, Max reward: -0.9839, Mean reward: -10.4349\n",
      "Episode 197/1000 - Policy loss: -0.0542, Value loss: 5.3445, Entropy: 9.0760\n",
      "Episode 198/1000 - Min reward: -25.3292, Max reward: 0.0288, Mean reward: -8.5835\n",
      "Episode 198/1000 - Policy loss: -0.3623, Value loss: 6.4261, Entropy: 9.0705\n",
      "Episode 199/1000 - Min reward: -14.6624, Max reward: 0.0759, Mean reward: -4.6083\n",
      "Episode 199/1000 - Policy loss: -0.0602, Value loss: 11.1162, Entropy: 9.0691\n",
      "Episode 200/1000 - Min reward: -13.6413, Max reward: 0.0759, Mean reward: -4.6658\n",
      "Episode 200/1000 - Policy loss: -0.1640, Value loss: 6.9214, Entropy: 9.0614\n",
      "Episode 201/1000 - Min reward: -14.6488, Max reward: 0.0759, Mean reward: -3.8302\n",
      "Episode 201/1000 - Policy loss: 0.1010, Value loss: 15.0713, Entropy: 9.0586\n",
      "Episode 202/1000 - Min reward: -18.2733, Max reward: 0.0759, Mean reward: -3.2565\n",
      "Episode 202/1000 - Policy loss: -0.0474, Value loss: 9.7342, Entropy: 9.0571\n",
      "Episode 203/1000 - Min reward: -15.9820, Max reward: 0.0759, Mean reward: -4.2895\n",
      "Episode 203/1000 - Policy loss: -0.1652, Value loss: 5.5691, Entropy: 9.0507\n",
      "Episode 204/1000 - Min reward: -23.9814, Max reward: 0.0759, Mean reward: -5.4887\n",
      "Episode 204/1000 - Policy loss: -0.0905, Value loss: 12.8884, Entropy: 9.0505\n",
      "Episode 205/1000 - Min reward: -20.7888, Max reward: 0.0759, Mean reward: -9.5993\n",
      "Episode 205/1000 - Policy loss: 0.0131, Value loss: 10.7632, Entropy: 9.0456\n",
      "Episode 206/1000 - Min reward: -10.7927, Max reward: 0.0759, Mean reward: -1.2406\n",
      "Episode 206/1000 - Policy loss: -0.1883, Value loss: 8.7198, Entropy: 9.0284\n",
      "Episode 207/1000 - Min reward: -11.4441, Max reward: 0.0759, Mean reward: -1.5309\n",
      "Episode 207/1000 - Policy loss: 0.0390, Value loss: 14.3799, Entropy: 9.0259\n",
      "Episode 208/1000 - Min reward: -12.8574, Max reward: 0.0876, Mean reward: -3.2976\n",
      "Episode 208/1000 - Policy loss: -0.0323, Value loss: 9.4017, Entropy: 9.0280\n",
      "Episode 209/1000 - Min reward: -7.8490, Max reward: 0.2694, Mean reward: -0.8611\n",
      "Episode 209/1000 - Policy loss: 0.0845, Value loss: 16.4224, Entropy: 9.0322\n",
      "Episode 210/1000 - Min reward: -9.4015, Max reward: 0.0759, Mean reward: -2.2720\n",
      "Episode 210/1000 - Policy loss: -0.1161, Value loss: 22.4165, Entropy: 9.0494\n",
      "Episode 211/1000 - Min reward: -14.7016, Max reward: 0.0759, Mean reward: -6.0538\n",
      "Episode 211/1000 - Policy loss: -0.0638, Value loss: 6.0778, Entropy: 9.0560\n",
      "Episode 212/1000 - Min reward: -10.2872, Max reward: -0.0871, Mean reward: -5.5623\n",
      "Episode 212/1000 - Policy loss: -0.2391, Value loss: 32.3763, Entropy: 9.0542\n",
      "Episode 213/1000 - Min reward: -5.9094, Max reward: 0.0659, Mean reward: -2.4687\n",
      "Episode 213/1000 - Policy loss: -0.0516, Value loss: 54.7808, Entropy: 9.0443\n",
      "Episode 214/1000 - Min reward: -10.8977, Max reward: 0.0759, Mean reward: -1.7704\n",
      "Episode 214/1000 - Policy loss: 0.1786, Value loss: 12.5941, Entropy: 9.0273\n",
      "Episode 215/1000 - Min reward: -15.5693, Max reward: 0.9913, Mean reward: -1.2492\n",
      "Episode 215/1000 - Policy loss: -0.1842, Value loss: 5.6433, Entropy: 9.0148\n",
      "Episode 216/1000 - Min reward: -22.6582, Max reward: 0.0759, Mean reward: -4.6372\n",
      "Episode 216/1000 - Policy loss: -0.1823, Value loss: 10.1326, Entropy: 9.0145\n",
      "Episode 217/1000 - Min reward: -25.0310, Max reward: -0.1256, Mean reward: -8.5379\n",
      "Episode 217/1000 - Policy loss: -0.3080, Value loss: 2.7145, Entropy: 9.0291\n",
      "Episode 218/1000 - Min reward: -15.7844, Max reward: 0.0759, Mean reward: -5.9234\n",
      "Episode 218/1000 - Policy loss: -0.1301, Value loss: 5.8417, Entropy: 9.0316\n",
      "Episode 219/1000 - Min reward: -15.1363, Max reward: -3.1368, Mean reward: -8.7002\n",
      "Episode 219/1000 - Policy loss: 0.0715, Value loss: 24.5431, Entropy: 9.0292\n",
      "Episode 220/1000 - Min reward: -16.5246, Max reward: 0.0759, Mean reward: -4.9648\n",
      "Episode 220/1000 - Policy loss: -0.0879, Value loss: 8.2400, Entropy: 9.0303\n",
      "Episode 221/1000 - Min reward: -21.5221, Max reward: -1.0249, Mean reward: -9.6348\n",
      "Episode 221/1000 - Policy loss: 0.0201, Value loss: 11.0349, Entropy: 9.0393\n",
      "Episode 222/1000 - Min reward: -12.3115, Max reward: -1.0512, Mean reward: -4.9676\n",
      "Episode 222/1000 - Policy loss: -0.0325, Value loss: 22.5880, Entropy: 9.0401\n",
      "Episode 223/1000 - Min reward: -18.4185, Max reward: -2.2424, Mean reward: -9.9532\n",
      "Episode 223/1000 - Policy loss: -0.2244, Value loss: 7.5167, Entropy: 9.0412\n",
      "Episode 224/1000 - Min reward: -24.6517, Max reward: -1.4064, Mean reward: -7.9504\n",
      "Episode 224/1000 - Policy loss: 0.0080, Value loss: 6.2185, Entropy: 9.0361\n",
      "Episode 225/1000 - Min reward: -19.4971, Max reward: -1.1119, Mean reward: -5.5566\n",
      "Episode 225/1000 - Policy loss: -0.0219, Value loss: 12.5371, Entropy: 9.0380\n",
      "Episode 226/1000 - Min reward: -4.3172, Max reward: 0.7812, Mean reward: -0.2315\n",
      "Episode 226/1000 - Policy loss: -0.1005, Value loss: 19.4645, Entropy: 9.0364\n",
      "Episode 227/1000 - Min reward: -17.7517, Max reward: 0.0957, Mean reward: -3.4764\n",
      "Episode 227/1000 - Policy loss: -0.0820, Value loss: 34.6610, Entropy: 9.0262\n",
      "Episode 228/1000 - Min reward: -19.0189, Max reward: -0.8247, Mean reward: -5.9867\n",
      "Episode 228/1000 - Policy loss: -0.1007, Value loss: 5.1934, Entropy: 9.0215\n",
      "Episode 229/1000 - Min reward: -13.3319, Max reward: -0.6763, Mean reward: -4.7996\n",
      "Episode 229/1000 - Policy loss: -0.1456, Value loss: 10.6014, Entropy: 9.0265\n",
      "Episode 230/1000 - Min reward: -14.9352, Max reward: 0.2741, Mean reward: -1.3061\n",
      "Episode 230/1000 - Policy loss: -0.1897, Value loss: 12.7298, Entropy: 9.0433\n",
      "Episode 231/1000 - Min reward: -22.1440, Max reward: -0.0471, Mean reward: -6.4807\n",
      "Episode 231/1000 - Policy loss: -0.1121, Value loss: 5.8675, Entropy: 9.0551\n",
      "Episode 232/1000 - Min reward: -10.6565, Max reward: -1.4064, Mean reward: -4.5097\n",
      "Episode 232/1000 - Policy loss: -0.0606, Value loss: 4.8849, Entropy: 9.0529\n",
      "Episode 233/1000 - Min reward: -13.7891, Max reward: -2.4489, Mean reward: -7.8471\n",
      "Episode 233/1000 - Policy loss: -0.1224, Value loss: 3.8451, Entropy: 9.0413\n",
      "Episode 234/1000 - Min reward: -15.1724, Max reward: 0.0755, Mean reward: -4.3421\n",
      "Episode 234/1000 - Policy loss: -0.1037, Value loss: 11.7689, Entropy: 9.0403\n",
      "Episode 235/1000 - Min reward: -13.9622, Max reward: -0.0817, Mean reward: -4.0295\n",
      "Episode 235/1000 - Policy loss: 0.0021, Value loss: 9.0773, Entropy: 9.0487\n",
      "Episode 236/1000 - Min reward: -9.7353, Max reward: -1.7592, Mean reward: -5.3270\n",
      "Episode 236/1000 - Policy loss: -0.1591, Value loss: 44.2461, Entropy: 9.0481\n",
      "Episode 237/1000 - Min reward: -20.6330, Max reward: -2.7911, Mean reward: -8.7018\n",
      "Episode 237/1000 - Policy loss: -0.0812, Value loss: 10.3617, Entropy: 9.0442\n",
      "Episode 238/1000 - Min reward: -17.6860, Max reward: -3.6834, Mean reward: -9.2536\n",
      "Episode 238/1000 - Policy loss: -0.0441, Value loss: 11.9774, Entropy: 9.0505\n",
      "Episode 239/1000 - Min reward: -9.6343, Max reward: 0.9945, Mean reward: -0.1794\n",
      "Episode 239/1000 - Policy loss: -0.1453, Value loss: 4.2429, Entropy: 9.0586\n",
      "Episode 240/1000 - Min reward: -15.1645, Max reward: -1.5724, Mean reward: -8.0490\n",
      "Episode 240/1000 - Policy loss: -0.1952, Value loss: 23.5594, Entropy: 9.0665\n",
      "Episode 241/1000 - Min reward: -17.2822, Max reward: -0.5096, Mean reward: -8.7006\n",
      "Episode 241/1000 - Policy loss: 0.2637, Value loss: 13.5303, Entropy: 9.0568\n",
      "Episode 242/1000 - Min reward: -19.6692, Max reward: -4.8261, Mean reward: -9.5748\n",
      "Episode 242/1000 - Policy loss: -0.1764, Value loss: 15.1026, Entropy: 9.0451\n",
      "Episode 243/1000 - Min reward: -12.8930, Max reward: 0.8974, Mean reward: -1.0070\n",
      "Episode 243/1000 - Policy loss: -0.2498, Value loss: 9.4392, Entropy: 9.0422\n",
      "Episode 244/1000 - Min reward: -18.2305, Max reward: 0.0747, Mean reward: -3.7236\n",
      "Episode 244/1000 - Policy loss: -0.0223, Value loss: 13.0606, Entropy: 9.0400\n",
      "Episode 245/1000 - Min reward: -18.2713, Max reward: 0.0163, Mean reward: -6.1331\n",
      "Episode 245/1000 - Policy loss: -0.1044, Value loss: 14.5772, Entropy: 9.0345\n",
      "Episode 246/1000 - Min reward: -12.1344, Max reward: 0.0759, Mean reward: -3.4975\n",
      "Episode 246/1000 - Policy loss: -0.1470, Value loss: 23.7847, Entropy: 9.0274\n",
      "Episode 247/1000 - Min reward: -11.2993, Max reward: 0.0759, Mean reward: -1.2026\n",
      "Episode 247/1000 - Policy loss: 0.0821, Value loss: 14.5242, Entropy: 9.0154\n",
      "Episode 248/1000 - Min reward: -14.8655, Max reward: 0.0745, Mean reward: -3.8021\n",
      "Episode 248/1000 - Policy loss: -0.0802, Value loss: 17.0779, Entropy: 9.0133\n",
      "Episode 249/1000 - Min reward: -16.9943, Max reward: -1.6756, Mean reward: -7.7987\n",
      "Episode 249/1000 - Policy loss: 0.0560, Value loss: 19.2723, Entropy: 9.0093\n",
      "Episode 250/1000 - Min reward: -9.1405, Max reward: 0.0759, Mean reward: -1.7846\n",
      "Episode 250/1000 - Policy loss: 0.0554, Value loss: 34.2332, Entropy: 9.0176\n",
      "Episode 251/1000 - Min reward: -16.2261, Max reward: -0.8522, Mean reward: -6.4638\n",
      "Episode 251/1000 - Policy loss: -0.1896, Value loss: 17.7958, Entropy: 9.0292\n",
      "Episode 252/1000 - Min reward: -24.7652, Max reward: -0.0128, Mean reward: -6.0032\n",
      "Episode 252/1000 - Policy loss: -0.0023, Value loss: 12.4048, Entropy: 9.0344\n",
      "Episode 253/1000 - Min reward: -23.5342, Max reward: 0.9959, Mean reward: -1.8729\n",
      "Episode 253/1000 - Policy loss: -0.0902, Value loss: 5.7537, Entropy: 9.0399\n",
      "Episode 254/1000 - Min reward: -14.1941, Max reward: 0.0988, Mean reward: -2.9661\n",
      "Episode 254/1000 - Policy loss: -0.1319, Value loss: 13.3451, Entropy: 9.0447\n",
      "Episode 255/1000 - Min reward: -23.6154, Max reward: -0.8764, Mean reward: -5.5991\n",
      "Episode 255/1000 - Policy loss: -0.2738, Value loss: 22.7110, Entropy: 9.0560\n",
      "Episode 256/1000 - Min reward: -19.4006, Max reward: 0.0025, Mean reward: -5.3656\n",
      "Episode 256/1000 - Policy loss: 0.0283, Value loss: 11.8808, Entropy: 9.0712\n",
      "Episode 257/1000 - Min reward: -8.8666, Max reward: 0.0759, Mean reward: -2.9996\n",
      "Episode 257/1000 - Policy loss: -0.1706, Value loss: 29.5296, Entropy: 9.0737\n",
      "Episode 258/1000 - Min reward: -11.1528, Max reward: 0.0757, Mean reward: -2.3874\n",
      "Episode 258/1000 - Policy loss: 0.1199, Value loss: 12.1591, Entropy: 9.0577\n",
      "Episode 259/1000 - Min reward: -5.1263, Max reward: -0.3213, Mean reward: -2.5088\n",
      "Episode 259/1000 - Policy loss: -0.1336, Value loss: 7.0432, Entropy: 9.0394\n",
      "Episode 260/1000 - Min reward: -7.6223, Max reward: 0.0938, Mean reward: -1.5413\n",
      "Episode 260/1000 - Policy loss: -0.0504, Value loss: 15.5444, Entropy: 9.0303\n",
      "Episode 261/1000 - Min reward: -5.3826, Max reward: 0.9896, Mean reward: -0.1346\n",
      "Episode 261/1000 - Policy loss: -0.6209, Value loss: 28.3319, Entropy: 9.0347\n",
      "Episode 262/1000 - Min reward: -16.1595, Max reward: 0.7389, Mean reward: -1.3013\n",
      "Episode 262/1000 - Policy loss: 0.0031, Value loss: 10.9446, Entropy: 9.0404\n",
      "Episode 263/1000 - Min reward: -18.1243, Max reward: 0.9825, Mean reward: -1.8185\n",
      "Episode 263/1000 - Policy loss: 0.2848, Value loss: 8.1791, Entropy: 9.0498\n",
      "Episode 264/1000 - Min reward: -11.7313, Max reward: 0.0448, Mean reward: -3.4060\n",
      "Episode 264/1000 - Policy loss: -0.1093, Value loss: 44.4345, Entropy: 9.0578\n",
      "Episode 265/1000 - Min reward: -7.0388, Max reward: -0.9572, Mean reward: -3.9268\n",
      "Episode 265/1000 - Policy loss: -0.3061, Value loss: 26.7198, Entropy: 9.0652\n",
      "Episode 266/1000 - Min reward: -4.5405, Max reward: -0.0089, Mean reward: -1.7818\n",
      "Episode 266/1000 - Policy loss: -0.0706, Value loss: 23.8152, Entropy: 9.0692\n",
      "Episode 267/1000 - Min reward: -24.7807, Max reward: -0.3234, Mean reward: -8.2771\n",
      "Episode 267/1000 - Policy loss: -0.1303, Value loss: 38.9608, Entropy: 9.0742\n",
      "Episode 268/1000 - Min reward: -13.9484, Max reward: -0.5444, Mean reward: -2.8272\n",
      "Episode 268/1000 - Policy loss: -0.0432, Value loss: 21.4858, Entropy: 9.0815\n",
      "Episode 269/1000 - Min reward: -22.2197, Max reward: 0.9994, Mean reward: -3.4628\n",
      "Episode 269/1000 - Policy loss: 0.1439, Value loss: 10.1650, Entropy: 9.0942\n",
      "Episode 270/1000 - Min reward: -10.4861, Max reward: 0.0755, Mean reward: -4.0003\n",
      "Episode 270/1000 - Policy loss: -0.0544, Value loss: 19.7388, Entropy: 9.1078\n",
      "Episode 271/1000 - Min reward: -14.1312, Max reward: 0.5238, Mean reward: -1.3458\n",
      "Episode 271/1000 - Policy loss: 0.1343, Value loss: 2.8821, Entropy: 9.1191\n",
      "Episode 272/1000 - Min reward: -19.7015, Max reward: 0.9956, Mean reward: -2.3498\n",
      "Episode 272/1000 - Policy loss: -0.0487, Value loss: 5.5294, Entropy: 9.1243\n",
      "Episode 273/1000 - Min reward: -5.1078, Max reward: 0.2906, Mean reward: -0.1723\n",
      "Episode 273/1000 - Policy loss: -0.0746, Value loss: 7.7734, Entropy: 9.1224\n",
      "Episode 274/1000 - Min reward: -25.3386, Max reward: 0.1942, Mean reward: -5.4954\n",
      "Episode 274/1000 - Policy loss: 0.0002, Value loss: 16.9331, Entropy: 9.1205\n",
      "Episode 275/1000 - Min reward: -29.2207, Max reward: 0.7831, Mean reward: -3.0488\n",
      "Episode 275/1000 - Policy loss: 0.2337, Value loss: 7.7854, Entropy: 9.1283\n",
      "Episode 276/1000 - Min reward: -13.7600, Max reward: 0.1059, Mean reward: -2.1463\n",
      "Episode 276/1000 - Policy loss: -0.0620, Value loss: 16.7897, Entropy: 9.1325\n",
      "Episode 277/1000 - Min reward: -25.2164, Max reward: -2.2889, Mean reward: -7.4416\n",
      "Episode 277/1000 - Policy loss: -0.1112, Value loss: 12.4537, Entropy: 9.1336\n",
      "Episode 278/1000 - Min reward: -13.0002, Max reward: 0.4781, Mean reward: -2.4362\n",
      "Episode 278/1000 - Policy loss: -0.4231, Value loss: 4.9956, Entropy: 9.1344\n",
      "Episode 279/1000 - Min reward: -14.3192, Max reward: 0.9803, Mean reward: -1.7879\n",
      "Episode 279/1000 - Policy loss: 0.1361, Value loss: 5.8855, Entropy: 9.1431\n",
      "Episode 280/1000 - Min reward: -17.3081, Max reward: 0.2614, Mean reward: -2.2073\n",
      "Episode 280/1000 - Policy loss: -0.3036, Value loss: 14.4947, Entropy: 9.1508\n",
      "Episode 281/1000 - Min reward: -18.5495, Max reward: -0.2968, Mean reward: -3.8060\n",
      "Episode 281/1000 - Policy loss: 0.1376, Value loss: 12.4580, Entropy: 9.1507\n",
      "Episode 282/1000 - Min reward: -11.3229, Max reward: 0.1100, Mean reward: -4.0665\n",
      "Episode 282/1000 - Policy loss: -0.1485, Value loss: 27.4255, Entropy: 9.1477\n",
      "Episode 283/1000 - Min reward: -5.3898, Max reward: 0.6775, Mean reward: -0.5956\n",
      "Episode 283/1000 - Policy loss: -0.2118, Value loss: 13.7986, Entropy: 9.1492\n",
      "Episode 284/1000 - Min reward: -19.7478, Max reward: -6.2981, Mean reward: -9.8848\n",
      "Episode 284/1000 - Policy loss: -0.2966, Value loss: 22.0021, Entropy: 9.1641\n",
      "Episode 285/1000 - Min reward: -20.3674, Max reward: -0.1384, Mean reward: -4.6939\n",
      "Episode 285/1000 - Policy loss: -0.4514, Value loss: 6.0995, Entropy: 9.1680\n",
      "Episode 286/1000 - Min reward: -4.1687, Max reward: 0.9499, Mean reward: -0.8328\n",
      "Episode 286/1000 - Policy loss: -0.0529, Value loss: 113.7365, Entropy: 9.1675\n",
      "Episode 287/1000 - Min reward: -6.6078, Max reward: 0.9884, Mean reward: 0.0656\n",
      "Episode 287/1000 - Policy loss: -0.1049, Value loss: 4.4003, Entropy: 9.1759\n",
      "Episode 288/1000 - Min reward: -3.9433, Max reward: 0.8517, Mean reward: -0.1369\n",
      "Episode 288/1000 - Policy loss: -0.1835, Value loss: 2.6810, Entropy: 9.1861\n",
      "Episode 289/1000 - Min reward: -11.3852, Max reward: 0.9993, Mean reward: -0.9151\n",
      "Episode 289/1000 - Policy loss: -0.0115, Value loss: 1.8396, Entropy: 9.1818\n",
      "Episode 290/1000 - Min reward: -11.0318, Max reward: 0.5487, Mean reward: -0.5581\n",
      "Episode 290/1000 - Policy loss: -0.0458, Value loss: 3.6511, Entropy: 9.1932\n",
      "Episode 291/1000 - Min reward: -18.3160, Max reward: 0.0868, Mean reward: -3.4481\n",
      "Episode 291/1000 - Policy loss: -0.1805, Value loss: 10.9713, Entropy: 9.1988\n",
      "Episode 292/1000 - Min reward: -12.0445, Max reward: 0.9999, Mean reward: -0.4702\n",
      "Episode 292/1000 - Policy loss: 0.1969, Value loss: 16.5140, Entropy: 9.1951\n",
      "Episode 293/1000 - Min reward: -14.2674, Max reward: 0.1543, Mean reward: -3.5917\n",
      "Episode 293/1000 - Policy loss: -0.1633, Value loss: 16.4803, Entropy: 9.1898\n",
      "Episode 294/1000 - Min reward: -13.9657, Max reward: -1.2430, Mean reward: -5.1770\n",
      "Episode 294/1000 - Policy loss: 0.0687, Value loss: 15.6507, Entropy: 9.1862\n",
      "Episode 295/1000 - Min reward: -18.7387, Max reward: 0.0859, Mean reward: -3.9230\n",
      "Episode 295/1000 - Policy loss: 0.0063, Value loss: 14.7998, Entropy: 9.1899\n",
      "Episode 296/1000 - Min reward: -14.5581, Max reward: 0.9493, Mean reward: -1.6406\n",
      "Episode 296/1000 - Policy loss: -0.1439, Value loss: 10.9228, Entropy: 9.1953\n",
      "Episode 297/1000 - Min reward: -9.5592, Max reward: 0.1290, Mean reward: -3.7451\n",
      "Episode 297/1000 - Policy loss: 0.0389, Value loss: 18.6371, Entropy: 9.1942\n",
      "Episode 298/1000 - Min reward: -16.6786, Max reward: -0.5274, Mean reward: -4.0204\n",
      "Episode 298/1000 - Policy loss: -0.0618, Value loss: 2.7505, Entropy: 9.2003\n",
      "Episode 299/1000 - Min reward: -2.1532, Max reward: 0.9871, Mean reward: 0.3274\n",
      "Episode 299/1000 - Policy loss: -0.0776, Value loss: 49.1634, Entropy: 9.2070\n",
      "Episode 300/1000 - Min reward: -20.6650, Max reward: 0.1461, Mean reward: -4.3727\n",
      "Episode 300/1000 - Policy loss: -0.2189, Value loss: 14.5248, Entropy: 9.2072\n",
      "Episode 301/1000 - Min reward: -16.0179, Max reward: 0.9818, Mean reward: -2.3201\n",
      "Episode 301/1000 - Policy loss: -0.2189, Value loss: 5.4995, Entropy: 9.2104\n",
      "Episode 302/1000 - Min reward: -20.9041, Max reward: 0.1801, Mean reward: -3.4737\n",
      "Episode 302/1000 - Policy loss: -0.0792, Value loss: 35.3287, Entropy: 9.2147\n",
      "Episode 303/1000 - Min reward: -18.3322, Max reward: -3.0758, Mean reward: -7.3287\n",
      "Episode 303/1000 - Policy loss: -0.0575, Value loss: 24.4419, Entropy: 9.2263\n",
      "Episode 304/1000 - Min reward: -12.6427, Max reward: 0.8465, Mean reward: -1.8626\n",
      "Episode 304/1000 - Policy loss: 0.0268, Value loss: 4.9758, Entropy: 9.2249\n",
      "Episode 305/1000 - Min reward: -8.9118, Max reward: 0.9874, Mean reward: -0.1805\n",
      "Episode 305/1000 - Policy loss: -0.0745, Value loss: 13.7489, Entropy: 9.2244\n",
      "Episode 306/1000 - Min reward: -18.0551, Max reward: 0.9729, Mean reward: -1.7201\n",
      "Episode 306/1000 - Policy loss: -0.2242, Value loss: 15.7925, Entropy: 9.2136\n",
      "Episode 307/1000 - Min reward: -7.9693, Max reward: 0.6077, Mean reward: -0.6943\n",
      "Episode 307/1000 - Policy loss: -0.0753, Value loss: 12.2275, Entropy: 9.2203\n",
      "Episode 308/1000 - Min reward: -16.7379, Max reward: 0.5687, Mean reward: -1.7965\n",
      "Episode 308/1000 - Policy loss: -0.0770, Value loss: 6.9126, Entropy: 9.2158\n",
      "Episode 309/1000 - Min reward: -13.2649, Max reward: 0.9998, Mean reward: -0.3980\n",
      "Episode 309/1000 - Policy loss: -0.2801, Value loss: 12.8544, Entropy: 9.2041\n",
      "Episode 310/1000 - Min reward: -28.2852, Max reward: 0.9960, Mean reward: -2.8993\n",
      "Episode 310/1000 - Policy loss: 0.0182, Value loss: 4.7630, Entropy: 9.2028\n",
      "Episode 311/1000 - Min reward: -20.2499, Max reward: 1.0000, Mean reward: -2.1089\n",
      "Episode 311/1000 - Policy loss: -0.0310, Value loss: 6.8345, Entropy: 9.2016\n",
      "Episode 312/1000 - Min reward: -9.4297, Max reward: 0.9645, Mean reward: -0.5859\n",
      "Episode 312/1000 - Policy loss: -0.0230, Value loss: 1.7463, Entropy: 9.2034\n",
      "Episode 313/1000 - Min reward: -11.8445, Max reward: 0.8748, Mean reward: -0.7253\n",
      "Episode 313/1000 - Policy loss: -0.0303, Value loss: 3.0876, Entropy: 9.2071\n",
      "Episode 314/1000 - Min reward: -6.6374, Max reward: 0.8964, Mean reward: -0.2080\n",
      "Episode 314/1000 - Policy loss: -0.0805, Value loss: 1.2676, Entropy: 9.2094\n",
      "Episode 315/1000 - Min reward: -10.6331, Max reward: 0.8659, Mean reward: -0.8450\n",
      "Episode 315/1000 - Policy loss: -0.2001, Value loss: 10.3657, Entropy: 9.2253\n",
      "Episode 316/1000 - Min reward: -16.2638, Max reward: 1.0000, Mean reward: -0.8777\n",
      "Episode 316/1000 - Policy loss: -0.2902, Value loss: 9.7361, Entropy: 9.2231\n",
      "Episode 317/1000 - Min reward: -7.2247, Max reward: 1.0000, Mean reward: 0.4723\n",
      "Episode 317/1000 - Policy loss: -0.1961, Value loss: 3.1551, Entropy: 9.2211\n",
      "Episode 318/1000 - Min reward: -18.9465, Max reward: 0.9983, Mean reward: -0.9284\n",
      "Episode 318/1000 - Policy loss: -0.0951, Value loss: 3.2844, Entropy: 9.2253\n",
      "Episode 319/1000 - Min reward: -16.7804, Max reward: 0.9999, Mean reward: -1.9059\n",
      "Episode 319/1000 - Policy loss: 0.2283, Value loss: 4.1809, Entropy: 9.2330\n",
      "Episode 320/1000 - Min reward: 0.8198, Max reward: 1.0000, Mean reward: 0.9949\n",
      "Episode 320/1000 - Policy loss: 0.0667, Value loss: 1.3999, Entropy: 9.2409\n",
      "Episode 321/1000 - Min reward: -1.7223, Max reward: 1.0000, Mean reward: 0.8987\n",
      "Episode 321/1000 - Policy loss: -0.1033, Value loss: 1.5684, Entropy: 9.2475\n",
      "Episode 322/1000 - Min reward: -13.7942, Max reward: 0.9999, Mean reward: -0.3022\n",
      "Episode 322/1000 - Policy loss: 0.1578, Value loss: 9.2808, Entropy: 9.2535\n",
      "Episode 323/1000 - Min reward: -18.5562, Max reward: 0.9871, Mean reward: -2.4052\n",
      "Episode 323/1000 - Policy loss: -0.0911, Value loss: 18.9719, Entropy: 9.2595\n",
      "Episode 324/1000 - Min reward: -18.8531, Max reward: 0.9997, Mean reward: -1.9950\n",
      "Episode 324/1000 - Policy loss: -0.0220, Value loss: 11.1242, Entropy: 9.2569\n",
      "Episode 325/1000 - Min reward: -14.9617, Max reward: 1.0000, Mean reward: -0.2707\n",
      "Episode 325/1000 - Policy loss: -0.0179, Value loss: 5.5875, Entropy: 9.2520\n",
      "Episode 326/1000 - Min reward: -7.8371, Max reward: 0.9920, Mean reward: 0.1242\n",
      "Episode 326/1000 - Policy loss: -0.2956, Value loss: 6.5849, Entropy: 9.2463\n",
      "Episode 327/1000 - Min reward: -27.8594, Max reward: 0.9815, Mean reward: -5.2795\n",
      "Episode 327/1000 - Policy loss: -0.0585, Value loss: 6.8099, Entropy: 9.2391\n",
      "Episode 328/1000 - Min reward: -12.2790, Max reward: 0.2624, Mean reward: -2.9080\n",
      "Episode 328/1000 - Policy loss: -0.2157, Value loss: 38.4568, Entropy: 9.2440\n",
      "Episode 329/1000 - Min reward: -5.0308, Max reward: 1.0000, Mean reward: 0.6681\n",
      "Episode 329/1000 - Policy loss: -0.2307, Value loss: 3.5659, Entropy: 9.2551\n",
      "Episode 330/1000 - Min reward: -7.1215, Max reward: 0.9837, Mean reward: 0.1230\n",
      "Episode 330/1000 - Policy loss: -0.0830, Value loss: 6.4268, Entropy: 9.2539\n",
      "Episode 331/1000 - Min reward: -7.2317, Max reward: 1.0000, Mean reward: 0.0892\n",
      "Episode 331/1000 - Policy loss: -0.0893, Value loss: 3.0854, Entropy: 9.2451\n",
      "Episode 332/1000 - Min reward: -15.9878, Max reward: 0.9988, Mean reward: -2.3559\n",
      "Episode 332/1000 - Policy loss: -0.2153, Value loss: 7.6084, Entropy: 9.2272\n",
      "Episode 333/1000 - Min reward: -17.0733, Max reward: 1.0000, Mean reward: -0.5323\n",
      "Episode 333/1000 - Policy loss: 0.1382, Value loss: 8.6865, Entropy: 9.2209\n",
      "Episode 334/1000 - Min reward: -22.5679, Max reward: 0.9580, Mean reward: -9.0388\n",
      "Episode 334/1000 - Policy loss: 0.0780, Value loss: 10.8590, Entropy: 9.2220\n",
      "Episode 335/1000 - Min reward: -3.4864, Max reward: 0.9954, Mean reward: 0.6717\n",
      "Episode 335/1000 - Policy loss: -0.0502, Value loss: 3.5297, Entropy: 9.2094\n",
      "Episode 336/1000 - Min reward: -4.9953, Max reward: 1.0000, Mean reward: 0.7238\n",
      "Episode 336/1000 - Policy loss: 0.1289, Value loss: 5.2664, Entropy: 9.2024\n",
      "Episode 337/1000 - Min reward: -6.0985, Max reward: 1.0000, Mean reward: 0.2350\n",
      "Episode 337/1000 - Policy loss: -0.0804, Value loss: 2.4467, Entropy: 9.2017\n",
      "Episode 338/1000 - Min reward: -17.3144, Max reward: 1.0000, Mean reward: -0.9678\n",
      "Episode 338/1000 - Policy loss: -0.2214, Value loss: 24.4582, Entropy: 9.1984\n",
      "Episode 339/1000 - Min reward: -9.5817, Max reward: 0.8643, Mean reward: -0.8882\n",
      "Episode 339/1000 - Policy loss: -0.1593, Value loss: 5.7950, Entropy: 9.1995\n",
      "Episode 340/1000 - Min reward: -4.0119, Max reward: 1.0000, Mean reward: 0.5520\n",
      "Episode 340/1000 - Policy loss: -0.0271, Value loss: 6.5270, Entropy: 9.2038\n",
      "Episode 341/1000 - Min reward: -7.9646, Max reward: 1.0000, Mean reward: 0.2806\n",
      "Episode 341/1000 - Policy loss: -0.0873, Value loss: 3.0024, Entropy: 9.2183\n",
      "Episode 342/1000 - Min reward: -8.0278, Max reward: 1.0000, Mean reward: 0.6043\n",
      "Episode 342/1000 - Policy loss: -0.0711, Value loss: 3.9471, Entropy: 9.2324\n",
      "Episode 343/1000 - Min reward: -15.0390, Max reward: 0.7844, Mean reward: -2.1113\n",
      "Episode 343/1000 - Policy loss: -0.1947, Value loss: 38.3620, Entropy: 9.2362\n",
      "Episode 344/1000 - Min reward: -27.9886, Max reward: 0.1749, Mean reward: -11.3224\n",
      "Episode 344/1000 - Policy loss: -0.0601, Value loss: 3.1802, Entropy: 9.2372\n",
      "Episode 345/1000 - Min reward: -21.4049, Max reward: 0.9896, Mean reward: -3.1557\n",
      "Episode 345/1000 - Policy loss: -0.1589, Value loss: 6.4727, Entropy: 9.2280\n",
      "Episode 346/1000 - Min reward: -20.4260, Max reward: 0.9106, Mean reward: -5.1574\n",
      "Episode 346/1000 - Policy loss: -0.0022, Value loss: 4.3355, Entropy: 9.2298\n",
      "Episode 347/1000 - Min reward: -24.2095, Max reward: 1.0000, Mean reward: -1.8252\n",
      "Episode 347/1000 - Policy loss: -0.1438, Value loss: 5.1043, Entropy: 9.2251\n",
      "Episode 348/1000 - Min reward: -19.9629, Max reward: 0.9981, Mean reward: -3.7416\n",
      "Episode 348/1000 - Policy loss: 0.0821, Value loss: 4.8667, Entropy: 9.2086\n",
      "Episode 349/1000 - Min reward: -22.7003, Max reward: 0.9965, Mean reward: -3.3058\n",
      "Episode 349/1000 - Policy loss: -0.1043, Value loss: 0.7515, Entropy: 9.2048\n",
      "Episode 350/1000 - Min reward: -23.5119, Max reward: 1.0000, Mean reward: -3.7737\n",
      "Episode 350/1000 - Policy loss: 0.0303, Value loss: 0.7140, Entropy: 9.1952\n",
      "Episode 351/1000 - Min reward: -1.0548, Max reward: 1.0000, Mean reward: 0.8624\n",
      "Episode 351/1000 - Policy loss: -0.0235, Value loss: 3.7806, Entropy: 9.1849\n",
      "Episode 352/1000 - Min reward: -18.1192, Max reward: 0.9989, Mean reward: -1.2136\n",
      "Episode 352/1000 - Policy loss: 0.0706, Value loss: 8.0685, Entropy: 9.1864\n",
      "Episode 353/1000 - Min reward: -12.5554, Max reward: 1.0000, Mean reward: 0.1676\n",
      "Episode 353/1000 - Policy loss: 0.1712, Value loss: 5.3316, Entropy: 9.1878\n",
      "Episode 354/1000 - Min reward: -15.0901, Max reward: 1.0000, Mean reward: -0.6631\n",
      "Episode 354/1000 - Policy loss: -0.1820, Value loss: 3.9190, Entropy: 9.1826\n",
      "Episode 355/1000 - Min reward: -9.2688, Max reward: 0.9986, Mean reward: -0.5423\n",
      "Episode 355/1000 - Policy loss: 0.1874, Value loss: 5.3855, Entropy: 9.1889\n",
      "Episode 356/1000 - Min reward: -14.7650, Max reward: 1.0000, Mean reward: -1.0515\n",
      "Episode 356/1000 - Policy loss: -0.2380, Value loss: 15.3933, Entropy: 9.1838\n",
      "Episode 357/1000 - Min reward: -17.0641, Max reward: 0.9998, Mean reward: -2.0600\n",
      "Episode 357/1000 - Policy loss: 0.0423, Value loss: 13.0122, Entropy: 9.1787\n",
      "Episode 358/1000 - Min reward: -14.2447, Max reward: 1.0000, Mean reward: -0.2363\n",
      "Episode 358/1000 - Policy loss: -0.2062, Value loss: 2.3937, Entropy: 9.1775\n",
      "Episode 359/1000 - Min reward: -17.6379, Max reward: 1.0000, Mean reward: -2.6588\n",
      "Episode 359/1000 - Policy loss: -0.3628, Value loss: 3.1702, Entropy: 9.1690\n",
      "Episode 360/1000 - Min reward: -19.0029, Max reward: -2.6416, Mean reward: -7.2731\n",
      "Episode 360/1000 - Policy loss: -0.2073, Value loss: 17.6713, Entropy: 9.1689\n",
      "Episode 361/1000 - Min reward: -25.5278, Max reward: 0.8559, Mean reward: -5.4502\n",
      "Episode 361/1000 - Policy loss: 0.0102, Value loss: 7.9382, Entropy: 9.1775\n",
      "Episode 362/1000 - Min reward: -15.0316, Max reward: 0.3184, Mean reward: -2.1174\n",
      "Episode 362/1000 - Policy loss: -0.3612, Value loss: 13.9420, Entropy: 9.1667\n",
      "Episode 363/1000 - Min reward: -14.6512, Max reward: 0.9998, Mean reward: -1.1491\n",
      "Episode 363/1000 - Policy loss: -0.0812, Value loss: 13.1524, Entropy: 9.1627\n",
      "Episode 364/1000 - Min reward: -28.0513, Max reward: 0.9999, Mean reward: -4.7878\n",
      "Episode 364/1000 - Policy loss: -0.2688, Value loss: 2.5618, Entropy: 9.1703\n",
      "Episode 365/1000 - Min reward: -17.7868, Max reward: 1.0000, Mean reward: -0.8231\n",
      "Episode 365/1000 - Policy loss: -0.2002, Value loss: 2.8736, Entropy: 9.1656\n",
      "Episode 366/1000 - Min reward: -10.8510, Max reward: 0.9999, Mean reward: -0.0145\n",
      "Episode 366/1000 - Policy loss: 0.2984, Value loss: 3.3219, Entropy: 9.1674\n",
      "Episode 367/1000 - Min reward: -18.6542, Max reward: 0.9148, Mean reward: -4.0697\n",
      "Episode 367/1000 - Policy loss: -0.0277, Value loss: 17.6726, Entropy: 9.1675\n",
      "Episode 368/1000 - Min reward: -11.7425, Max reward: 1.0000, Mean reward: -0.0812\n",
      "Episode 368/1000 - Policy loss: -0.0317, Value loss: 3.7093, Entropy: 9.1694\n",
      "Episode 369/1000 - Min reward: -9.6419, Max reward: 1.0000, Mean reward: -0.1008\n",
      "Episode 369/1000 - Policy loss: 0.0597, Value loss: 1.9931, Entropy: 9.1659\n",
      "Episode 370/1000 - Min reward: -12.7434, Max reward: 1.0000, Mean reward: -0.0760\n",
      "Episode 370/1000 - Policy loss: 0.1773, Value loss: 4.1093, Entropy: 9.1612\n",
      "Episode 371/1000 - Min reward: -9.2258, Max reward: 1.0000, Mean reward: 0.4587\n",
      "Episode 371/1000 - Policy loss: -0.2374, Value loss: 3.3161, Entropy: 9.1657\n",
      "Episode 372/1000 - Min reward: -11.3086, Max reward: 1.0000, Mean reward: 0.2529\n",
      "Episode 372/1000 - Policy loss: -0.3909, Value loss: 3.4182, Entropy: 9.1681\n",
      "Episode 373/1000 - Min reward: -23.7614, Max reward: 0.7734, Mean reward: -5.1658\n",
      "Episode 373/1000 - Policy loss: -0.2459, Value loss: 5.3699, Entropy: 9.1576\n",
      "Episode 374/1000 - Min reward: -24.7211, Max reward: 0.9940, Mean reward: -7.1303\n",
      "Episode 374/1000 - Policy loss: -0.2587, Value loss: 11.7929, Entropy: 9.1386\n",
      "Episode 375/1000 - Min reward: -14.3909, Max reward: 1.0000, Mean reward: -0.6282\n",
      "Episode 375/1000 - Policy loss: 0.1523, Value loss: 2.4909, Entropy: 9.1316\n",
      "Episode 376/1000 - Min reward: -16.9678, Max reward: 0.9998, Mean reward: -2.8870\n",
      "Episode 376/1000 - Policy loss: -0.1571, Value loss: 3.3094, Entropy: 9.1355\n",
      "Episode 377/1000 - Min reward: -16.1652, Max reward: 1.0000, Mean reward: -1.0846\n",
      "Episode 377/1000 - Policy loss: 0.0265, Value loss: 2.6051, Entropy: 9.1411\n",
      "Episode 378/1000 - Min reward: -0.6476, Max reward: 1.0000, Mean reward: 0.9254\n",
      "Episode 378/1000 - Policy loss: 0.0857, Value loss: 4.4161, Entropy: 9.1514\n",
      "Episode 379/1000 - Min reward: -11.0062, Max reward: 0.9994, Mean reward: -0.1572\n",
      "Episode 379/1000 - Policy loss: -0.0240, Value loss: 6.1596, Entropy: 9.1613\n",
      "Episode 380/1000 - Min reward: -10.0731, Max reward: 0.9997, Mean reward: -0.1001\n",
      "Episode 380/1000 - Policy loss: -0.1898, Value loss: 1.4683, Entropy: 9.1620\n",
      "Episode 381/1000 - Min reward: -29.2184, Max reward: 0.9970, Mean reward: -5.0234\n",
      "Episode 381/1000 - Policy loss: 0.1045, Value loss: 7.0689, Entropy: 9.1648\n",
      "Episode 382/1000 - Min reward: -12.9035, Max reward: 0.9994, Mean reward: -0.4584\n",
      "Episode 382/1000 - Policy loss: 0.0563, Value loss: 5.9944, Entropy: 9.1673\n",
      "Episode 383/1000 - Min reward: -3.1410, Max reward: 1.0000, Mean reward: 0.8097\n",
      "Episode 383/1000 - Policy loss: -0.3310, Value loss: 1.7026, Entropy: 9.1748\n",
      "Episode 384/1000 - Min reward: -14.9047, Max reward: 0.9784, Mean reward: -0.6534\n",
      "Episode 384/1000 - Policy loss: 0.0254, Value loss: 12.3657, Entropy: 9.1726\n",
      "Episode 385/1000 - Min reward: -13.6038, Max reward: 0.9951, Mean reward: -0.4547\n",
      "Episode 385/1000 - Policy loss: -0.0467, Value loss: 8.5503, Entropy: 9.1613\n",
      "Episode 386/1000 - Min reward: -1.8522, Max reward: 0.9840, Mean reward: 0.7517\n",
      "Episode 386/1000 - Policy loss: -0.0349, Value loss: 6.1241, Entropy: 9.1649\n",
      "Episode 387/1000 - Min reward: -7.6898, Max reward: 0.9999, Mean reward: 0.4737\n",
      "Episode 387/1000 - Policy loss: -0.2425, Value loss: 0.4483, Entropy: 9.1821\n",
      "Episode 388/1000 - Min reward: -14.4387, Max reward: 1.0000, Mean reward: -0.3370\n",
      "Episode 388/1000 - Policy loss: 0.0592, Value loss: 8.4331, Entropy: 9.1869\n",
      "Episode 389/1000 - Min reward: -30.2454, Max reward: 0.9999, Mean reward: -4.4399\n",
      "Episode 389/1000 - Policy loss: 0.0156, Value loss: 3.9375, Entropy: 9.1965\n",
      "Episode 390/1000 - Min reward: -17.5305, Max reward: 0.9899, Mean reward: -2.0293\n",
      "Episode 390/1000 - Policy loss: -0.0029, Value loss: 5.1305, Entropy: 9.2002\n",
      "Episode 391/1000 - Min reward: -18.5456, Max reward: 0.9836, Mean reward: -3.5580\n",
      "Episode 391/1000 - Policy loss: -0.1342, Value loss: 1.8858, Entropy: 9.1961\n",
      "Episode 392/1000 - Min reward: -15.2023, Max reward: 0.9783, Mean reward: -2.0594\n",
      "Episode 392/1000 - Policy loss: -0.1804, Value loss: 2.4477, Entropy: 9.1820\n",
      "Episode 393/1000 - Min reward: -15.3249, Max reward: 0.9285, Mean reward: -1.5268\n",
      "Episode 393/1000 - Policy loss: -0.0032, Value loss: 9.3310, Entropy: 9.1527\n",
      "Episode 394/1000 - Min reward: -3.3896, Max reward: 0.9999, Mean reward: 0.8208\n",
      "Episode 394/1000 - Policy loss: -0.2088, Value loss: 2.7049, Entropy: 9.1287\n",
      "Episode 395/1000 - Min reward: -9.4265, Max reward: 1.0000, Mean reward: 0.3992\n",
      "Episode 395/1000 - Policy loss: -0.1223, Value loss: 11.4115, Entropy: 9.1178\n",
      "Episode 396/1000 - Min reward: -16.5841, Max reward: 0.9996, Mean reward: -1.1727\n",
      "Episode 396/1000 - Policy loss: -0.0508, Value loss: 2.0158, Entropy: 9.1256\n",
      "Episode 397/1000 - Min reward: -14.9808, Max reward: 0.9899, Mean reward: -2.9234\n",
      "Episode 397/1000 - Policy loss: 0.0605, Value loss: 12.4029, Entropy: 9.1209\n",
      "Episode 398/1000 - Min reward: -10.7658, Max reward: 0.9921, Mean reward: 0.3204\n",
      "Episode 398/1000 - Policy loss: -0.4178, Value loss: 1.3870, Entropy: 9.1033\n",
      "Episode 399/1000 - Min reward: -3.7535, Max reward: 0.9622, Mean reward: 0.1962\n",
      "Episode 399/1000 - Policy loss: -0.0868, Value loss: 25.2221, Entropy: 9.0900\n",
      "Episode 400/1000 - Min reward: -5.8921, Max reward: 1.0000, Mean reward: 0.7102\n",
      "Episode 400/1000 - Policy loss: -0.0509, Value loss: 3.3104, Entropy: 9.0896\n",
      "Episode 401/1000 - Min reward: -14.4029, Max reward: 0.9998, Mean reward: -0.5889\n",
      "Episode 401/1000 - Policy loss: 0.0194, Value loss: 1.3264, Entropy: 9.0861\n",
      "Episode 402/1000 - Min reward: -8.7734, Max reward: 1.0000, Mean reward: 0.4606\n",
      "Episode 402/1000 - Policy loss: 0.3007, Value loss: 2.2628, Entropy: 9.0829\n",
      "Episode 403/1000 - Min reward: -16.6594, Max reward: 0.9993, Mean reward: -1.1380\n",
      "Episode 403/1000 - Policy loss: -0.0184, Value loss: 2.5462, Entropy: 9.0862\n",
      "Episode 404/1000 - Min reward: -2.1627, Max reward: 1.0000, Mean reward: 0.8828\n",
      "Episode 404/1000 - Policy loss: -0.1591, Value loss: 2.0397, Entropy: 9.0754\n",
      "Episode 405/1000 - Min reward: -19.6749, Max reward: 0.6925, Mean reward: -2.3859\n",
      "Episode 405/1000 - Policy loss: 0.0901, Value loss: 2.6332, Entropy: 9.0670\n",
      "Episode 406/1000 - Min reward: -6.2134, Max reward: 0.9999, Mean reward: -0.1567\n",
      "Episode 406/1000 - Policy loss: -0.0056, Value loss: 3.9995, Entropy: 9.0638\n",
      "Episode 407/1000 - Min reward: -15.8530, Max reward: 0.9976, Mean reward: -1.4375\n",
      "Episode 407/1000 - Policy loss: 0.0558, Value loss: 10.1760, Entropy: 9.0585\n",
      "Episode 408/1000 - Min reward: -13.1067, Max reward: 0.7135, Mean reward: -3.0334\n",
      "Episode 408/1000 - Policy loss: 0.0660, Value loss: 13.3077, Entropy: 9.0521\n",
      "Episode 409/1000 - Min reward: -27.5480, Max reward: -0.7561, Mean reward: -8.6109\n",
      "Episode 409/1000 - Policy loss: -0.2485, Value loss: 8.7358, Entropy: 9.0547\n",
      "Episode 410/1000 - Min reward: -15.6209, Max reward: 0.9216, Mean reward: -0.8642\n",
      "Episode 410/1000 - Policy loss: -0.0043, Value loss: 5.3720, Entropy: 9.0615\n",
      "Episode 411/1000 - Min reward: -12.5503, Max reward: 0.9974, Mean reward: -0.2215\n",
      "Episode 411/1000 - Policy loss: -0.2333, Value loss: 3.7273, Entropy: 9.0633\n",
      "Episode 412/1000 - Min reward: -6.3144, Max reward: 1.0000, Mean reward: 0.5088\n",
      "Episode 412/1000 - Policy loss: 0.0656, Value loss: 6.5636, Entropy: 9.0583\n",
      "Episode 413/1000 - Min reward: -12.8849, Max reward: 0.1975, Mean reward: -2.2624\n",
      "Episode 413/1000 - Policy loss: 0.3001, Value loss: 14.6453, Entropy: 9.0657\n",
      "Episode 414/1000 - Min reward: -6.5687, Max reward: 0.9917, Mean reward: -0.3603\n",
      "Episode 414/1000 - Policy loss: -0.1073, Value loss: 11.6627, Entropy: 9.0757\n",
      "Episode 415/1000 - Min reward: -24.4195, Max reward: -2.3309, Mean reward: -8.7774\n",
      "Episode 415/1000 - Policy loss: 0.1247, Value loss: 57.8892, Entropy: 9.0888\n",
      "Episode 416/1000 - Min reward: -17.2438, Max reward: -0.4846, Mean reward: -4.7545\n",
      "Episode 416/1000 - Policy loss: -0.1939, Value loss: 78.0291, Entropy: 9.0932\n",
      "Episode 417/1000 - Min reward: -18.2259, Max reward: 1.0000, Mean reward: -1.1188\n",
      "Episode 417/1000 - Policy loss: -0.1879, Value loss: 6.7564, Entropy: 9.0989\n",
      "Episode 418/1000 - Min reward: -16.0791, Max reward: -1.0771, Mean reward: -5.0204\n",
      "Episode 418/1000 - Policy loss: 0.1812, Value loss: 57.5330, Entropy: 9.0918\n",
      "Episode 419/1000 - Min reward: -9.6182, Max reward: 1.0000, Mean reward: 0.3923\n",
      "Episode 419/1000 - Policy loss: -0.1851, Value loss: 3.9432, Entropy: 9.0894\n",
      "Episode 420/1000 - Min reward: -8.4077, Max reward: 0.5436, Mean reward: -1.3489\n",
      "Episode 420/1000 - Policy loss: -0.3921, Value loss: 40.5801, Entropy: 9.0815\n",
      "Episode 421/1000 - Min reward: -16.6893, Max reward: 0.5951, Mean reward: -2.7192\n",
      "Episode 421/1000 - Policy loss: -0.0202, Value loss: 35.6149, Entropy: 9.0761\n",
      "Episode 422/1000 - Min reward: -15.3310, Max reward: -0.7719, Mean reward: -5.0150\n",
      "Episode 422/1000 - Policy loss: -0.2622, Value loss: 26.3695, Entropy: 9.0800\n",
      "Episode 423/1000 - Min reward: -12.3599, Max reward: 0.4141, Mean reward: -2.9469\n",
      "Episode 423/1000 - Policy loss: 0.1286, Value loss: 47.0298, Entropy: 9.0838\n",
      "Episode 424/1000 - Min reward: -14.2507, Max reward: 1.0000, Mean reward: -1.3957\n",
      "Episode 424/1000 - Policy loss: -0.0746, Value loss: 11.9695, Entropy: 9.0950\n",
      "Episode 425/1000 - Min reward: -10.3716, Max reward: 0.2185, Mean reward: -1.8515\n",
      "Episode 425/1000 - Policy loss: -0.1652, Value loss: 7.6432, Entropy: 9.0962\n",
      "Episode 426/1000 - Min reward: -16.3717, Max reward: -1.6782, Mean reward: -5.8030\n",
      "Episode 426/1000 - Policy loss: 0.0087, Value loss: 12.2895, Entropy: 9.1066\n",
      "Episode 427/1000 - Min reward: -28.0230, Max reward: -3.4611, Mean reward: -10.5801\n",
      "Episode 427/1000 - Policy loss: 0.2684, Value loss: 36.8482, Entropy: 9.1152\n",
      "Episode 428/1000 - Min reward: -21.7660, Max reward: 0.9953, Mean reward: -1.2970\n",
      "Episode 428/1000 - Policy loss: 0.2183, Value loss: 16.1395, Entropy: 9.1135\n",
      "Episode 429/1000 - Min reward: -20.4274, Max reward: -2.2658, Mean reward: -6.6833\n",
      "Episode 429/1000 - Policy loss: -0.0392, Value loss: 24.0678, Entropy: 9.1116\n",
      "Episode 430/1000 - Min reward: -18.4806, Max reward: 0.9999, Mean reward: -1.0253\n",
      "Episode 430/1000 - Policy loss: -0.0586, Value loss: 4.6666, Entropy: 9.1067\n",
      "Episode 431/1000 - Min reward: -5.1519, Max reward: 1.0000, Mean reward: 0.5571\n",
      "Episode 431/1000 - Policy loss: -0.0245, Value loss: 9.2749, Entropy: 9.0998\n",
      "Episode 432/1000 - Min reward: -11.9633, Max reward: 0.9761, Mean reward: -1.3356\n",
      "Episode 432/1000 - Policy loss: -0.0459, Value loss: 11.1300, Entropy: 9.0983\n",
      "Episode 433/1000 - Min reward: -10.1371, Max reward: 0.5398, Mean reward: -3.7816\n",
      "Episode 433/1000 - Policy loss: -0.1457, Value loss: 39.0076, Entropy: 9.0981\n",
      "Episode 434/1000 - Min reward: -22.5959, Max reward: 0.0815, Mean reward: -6.0711\n",
      "Episode 434/1000 - Policy loss: 0.0285, Value loss: 18.7248, Entropy: 9.1056\n",
      "Episode 435/1000 - Min reward: -27.8693, Max reward: 0.1576, Mean reward: -7.7769\n",
      "Episode 435/1000 - Policy loss: -0.2444, Value loss: 8.2605, Entropy: 9.1117\n",
      "Episode 436/1000 - Min reward: -16.4895, Max reward: 0.9940, Mean reward: -1.1863\n",
      "Episode 436/1000 - Policy loss: -0.0060, Value loss: 11.5286, Entropy: 9.1185\n",
      "Episode 437/1000 - Min reward: -10.4275, Max reward: 0.8934, Mean reward: -0.6083\n",
      "Episode 437/1000 - Policy loss: -0.0527, Value loss: 10.2470, Entropy: 9.1226\n",
      "Episode 438/1000 - Min reward: -6.3591, Max reward: 0.8762, Mean reward: 0.1477\n",
      "Episode 438/1000 - Policy loss: 0.0268, Value loss: 11.6794, Entropy: 9.1159\n",
      "Episode 439/1000 - Min reward: -13.8123, Max reward: 0.1545, Mean reward: -2.5742\n",
      "Episode 439/1000 - Policy loss: 0.0445, Value loss: 9.9301, Entropy: 9.1023\n",
      "Episode 440/1000 - Min reward: -23.4576, Max reward: 0.1148, Mean reward: -3.6476\n",
      "Episode 440/1000 - Policy loss: -0.2580, Value loss: 8.9052, Entropy: 9.1058\n",
      "Episode 441/1000 - Min reward: -14.2327, Max reward: 0.1963, Mean reward: -3.1954\n",
      "Episode 441/1000 - Policy loss: 0.1649, Value loss: 19.8661, Entropy: 9.1312\n",
      "Episode 442/1000 - Min reward: -15.2480, Max reward: 0.9909, Mean reward: -2.2684\n",
      "Episode 442/1000 - Policy loss: -0.3329, Value loss: 12.0784, Entropy: 9.1447\n",
      "Episode 443/1000 - Min reward: -15.6507, Max reward: -0.3463, Mean reward: -3.8881\n",
      "Episode 443/1000 - Policy loss: -0.1390, Value loss: 8.6049, Entropy: 9.1391\n",
      "Episode 444/1000 - Min reward: -9.5102, Max reward: 0.9741, Mean reward: -0.0241\n",
      "Episode 444/1000 - Policy loss: -0.0474, Value loss: 6.9837, Entropy: 9.1498\n",
      "Episode 445/1000 - Min reward: -14.5270, Max reward: 1.0000, Mean reward: -0.1819\n",
      "Episode 445/1000 - Policy loss: -0.1410, Value loss: 4.5609, Entropy: 9.1577\n",
      "Episode 446/1000 - Min reward: -29.4940, Max reward: 0.9427, Mean reward: -4.7409\n",
      "Episode 446/1000 - Policy loss: 0.1805, Value loss: 10.9204, Entropy: 9.1681\n",
      "Episode 447/1000 - Min reward: -8.9053, Max reward: 1.0000, Mean reward: 0.3154\n",
      "Episode 447/1000 - Policy loss: -0.1748, Value loss: 2.8594, Entropy: 9.1622\n",
      "Episode 448/1000 - Min reward: -14.3913, Max reward: 0.9996, Mean reward: -1.9918\n",
      "Episode 448/1000 - Policy loss: -0.3427, Value loss: 8.9009, Entropy: 9.1536\n",
      "Episode 449/1000 - Min reward: -11.9807, Max reward: 0.5303, Mean reward: -3.3735\n",
      "Episode 449/1000 - Policy loss: -0.2470, Value loss: 23.8619, Entropy: 9.1570\n",
      "Episode 450/1000 - Min reward: -11.8209, Max reward: 0.9990, Mean reward: -2.3973\n",
      "Episode 450/1000 - Policy loss: -0.1099, Value loss: 15.1134, Entropy: 9.1548\n",
      "Episode 451/1000 - Min reward: -18.1662, Max reward: 0.3450, Mean reward: -2.0383\n",
      "Episode 451/1000 - Policy loss: -0.0880, Value loss: 7.9462, Entropy: 9.1516\n",
      "Episode 452/1000 - Min reward: -14.5137, Max reward: 0.7416, Mean reward: -3.7031\n",
      "Episode 452/1000 - Policy loss: 0.0192, Value loss: 11.6351, Entropy: 9.1603\n",
      "Episode 453/1000 - Min reward: -9.3424, Max reward: 0.9658, Mean reward: -0.6330\n",
      "Episode 453/1000 - Policy loss: -0.0226, Value loss: 6.4968, Entropy: 9.1737\n",
      "Episode 454/1000 - Min reward: -11.6457, Max reward: 1.0000, Mean reward: 0.0368\n",
      "Episode 454/1000 - Policy loss: -0.1321, Value loss: 3.9340, Entropy: 9.1912\n",
      "Episode 455/1000 - Min reward: -7.5020, Max reward: 1.0000, Mean reward: 0.5207\n",
      "Episode 455/1000 - Policy loss: -0.0456, Value loss: 2.4876, Entropy: 9.1978\n",
      "Episode 456/1000 - Min reward: -24.4598, Max reward: 0.7574, Mean reward: -6.6069\n",
      "Episode 456/1000 - Policy loss: -0.0967, Value loss: 6.7413, Entropy: 9.2121\n",
      "Episode 457/1000 - Min reward: -1.5585, Max reward: 1.0000, Mean reward: 0.9115\n",
      "Episode 457/1000 - Policy loss: -0.1141, Value loss: 4.3979, Entropy: 9.2227\n",
      "Episode 458/1000 - Min reward: -22.6492, Max reward: 0.9996, Mean reward: -3.4773\n",
      "Episode 458/1000 - Policy loss: -0.1068, Value loss: 1.6552, Entropy: 9.2230\n",
      "Episode 459/1000 - Min reward: -16.5579, Max reward: 0.9974, Mean reward: -1.4008\n",
      "Episode 459/1000 - Policy loss: 0.0304, Value loss: 2.6389, Entropy: 9.2222\n",
      "Episode 460/1000 - Min reward: -27.6628, Max reward: 0.9997, Mean reward: -3.1501\n",
      "Episode 460/1000 - Policy loss: -0.1975, Value loss: 4.0248, Entropy: 9.2207\n",
      "Episode 461/1000 - Min reward: -10.9750, Max reward: 1.0000, Mean reward: -0.2769\n",
      "Episode 461/1000 - Policy loss: -0.1842, Value loss: 2.7338, Entropy: 9.2157\n",
      "Episode 462/1000 - Min reward: -25.2858, Max reward: 0.9987, Mean reward: -3.8727\n",
      "Episode 462/1000 - Policy loss: -0.0412, Value loss: 2.1277, Entropy: 9.2071\n",
      "Episode 463/1000 - Min reward: -14.9528, Max reward: 0.9995, Mean reward: -3.1493\n",
      "Episode 463/1000 - Policy loss: 0.0930, Value loss: 18.5377, Entropy: 9.2036\n",
      "Episode 464/1000 - Min reward: -15.5431, Max reward: 1.0000, Mean reward: -0.9779\n",
      "Episode 464/1000 - Policy loss: -0.0770, Value loss: 2.0883, Entropy: 9.1978\n",
      "Episode 465/1000 - Min reward: -21.9033, Max reward: 1.0000, Mean reward: -0.8298\n",
      "Episode 465/1000 - Policy loss: -0.2235, Value loss: 6.8157, Entropy: 9.2009\n",
      "Episode 466/1000 - Min reward: -18.6870, Max reward: 0.9259, Mean reward: -2.5966\n",
      "Episode 466/1000 - Policy loss: -0.1170, Value loss: 13.3343, Entropy: 9.1950\n",
      "Episode 467/1000 - Min reward: -14.5005, Max reward: 1.0000, Mean reward: -1.2772\n",
      "Episode 467/1000 - Policy loss: -0.0446, Value loss: 1.2808, Entropy: 9.1912\n",
      "Episode 468/1000 - Min reward: -11.6971, Max reward: 1.0000, Mean reward: 0.0711\n",
      "Episode 468/1000 - Policy loss: -0.2520, Value loss: 2.9677, Entropy: 9.1886\n",
      "Episode 469/1000 - Min reward: -24.8536, Max reward: 1.0000, Mean reward: -2.7715\n",
      "Episode 469/1000 - Policy loss: -0.3156, Value loss: 6.2619, Entropy: 9.1900\n",
      "Episode 470/1000 - Min reward: -9.2322, Max reward: 1.0000, Mean reward: 0.3273\n",
      "Episode 470/1000 - Policy loss: -0.1583, Value loss: 2.1850, Entropy: 9.2005\n",
      "Episode 471/1000 - Min reward: -14.8162, Max reward: 1.0000, Mean reward: -0.8066\n",
      "Episode 471/1000 - Policy loss: -0.1629, Value loss: 3.9121, Entropy: 9.2062\n",
      "Episode 472/1000 - Min reward: -24.1860, Max reward: 0.9997, Mean reward: -4.2854\n",
      "Episode 472/1000 - Policy loss: 0.1586, Value loss: 13.0912, Entropy: 9.2021\n",
      "Episode 473/1000 - Min reward: -21.8077, Max reward: 1.0000, Mean reward: -1.4813\n",
      "Episode 473/1000 - Policy loss: -0.2859, Value loss: 4.2635, Entropy: 9.1972\n",
      "Episode 474/1000 - Min reward: -3.2077, Max reward: 0.9981, Mean reward: 0.4999\n",
      "Episode 474/1000 - Policy loss: -0.2212, Value loss: 37.4174, Entropy: 9.1978\n",
      "Episode 475/1000 - Min reward: -19.9993, Max reward: -2.3781, Mean reward: -7.3918\n",
      "Episode 475/1000 - Policy loss: -0.1856, Value loss: 15.2186, Entropy: 9.2029\n",
      "Episode 476/1000 - Min reward: -23.3735, Max reward: 0.9948, Mean reward: -2.3426\n",
      "Episode 476/1000 - Policy loss: -0.1285, Value loss: 5.3030, Entropy: 9.2029\n",
      "Episode 477/1000 - Min reward: -10.7240, Max reward: 0.9989, Mean reward: -1.5307\n",
      "Episode 477/1000 - Policy loss: -0.1292, Value loss: 14.2925, Entropy: 9.2032\n",
      "Episode 478/1000 - Min reward: -21.4901, Max reward: 0.9641, Mean reward: -2.7510\n",
      "Episode 478/1000 - Policy loss: 0.0907, Value loss: 7.4951, Entropy: 9.1983\n",
      "Episode 479/1000 - Min reward: -10.5311, Max reward: 0.9963, Mean reward: 0.1613\n",
      "Episode 479/1000 - Policy loss: 0.2976, Value loss: 5.2254, Entropy: 9.2008\n",
      "Episode 480/1000 - Min reward: -16.5595, Max reward: 0.9877, Mean reward: -2.4924\n",
      "Episode 480/1000 - Policy loss: 0.0215, Value loss: 6.5479, Entropy: 9.2003\n",
      "Episode 481/1000 - Min reward: -16.0941, Max reward: 0.9997, Mean reward: -1.0454\n",
      "Episode 481/1000 - Policy loss: -0.0821, Value loss: 5.0289, Entropy: 9.1882\n",
      "Episode 482/1000 - Min reward: -14.1759, Max reward: 0.9941, Mean reward: -1.5921\n",
      "Episode 482/1000 - Policy loss: 0.0932, Value loss: 4.4072, Entropy: 9.1845\n",
      "Episode 483/1000 - Min reward: -3.5049, Max reward: 0.9548, Mean reward: 0.3984\n",
      "Episode 483/1000 - Policy loss: 0.0622, Value loss: 4.1400, Entropy: 9.1854\n",
      "Episode 484/1000 - Min reward: -17.7861, Max reward: 0.9998, Mean reward: -0.8578\n",
      "Episode 484/1000 - Policy loss: -0.2358, Value loss: 5.3763, Entropy: 9.1742\n",
      "Episode 485/1000 - Min reward: -25.4423, Max reward: 1.0000, Mean reward: -3.6290\n",
      "Episode 485/1000 - Policy loss: -0.0476, Value loss: 2.6261, Entropy: 9.1682\n",
      "Episode 486/1000 - Min reward: -10.6740, Max reward: 1.0000, Mean reward: 0.2743\n",
      "Episode 486/1000 - Policy loss: -0.1217, Value loss: 2.6827, Entropy: 9.1748\n",
      "Episode 487/1000 - Min reward: -15.6892, Max reward: 1.0000, Mean reward: -1.0451\n",
      "Episode 487/1000 - Policy loss: -0.1717, Value loss: 1.1233, Entropy: 9.1728\n",
      "Episode 488/1000 - Min reward: -15.4769, Max reward: 0.5205, Mean reward: -2.1499\n",
      "Episode 488/1000 - Policy loss: -0.3236, Value loss: 15.1296, Entropy: 9.1644\n",
      "Episode 489/1000 - Min reward: -13.1861, Max reward: 0.9955, Mean reward: -0.8750\n",
      "Episode 489/1000 - Policy loss: 0.1651, Value loss: 5.6975, Entropy: 9.1590\n",
      "Episode 490/1000 - Min reward: -14.1515, Max reward: 0.9999, Mean reward: -0.5516\n",
      "Episode 490/1000 - Policy loss: -0.0278, Value loss: 2.8462, Entropy: 9.1532\n",
      "Episode 491/1000 - Min reward: -9.4559, Max reward: 0.9916, Mean reward: 0.1765\n",
      "Episode 491/1000 - Policy loss: -0.2466, Value loss: 2.2684, Entropy: 9.1587\n",
      "Episode 492/1000 - Min reward: -12.9326, Max reward: 0.9736, Mean reward: -0.3383\n",
      "Episode 492/1000 - Policy loss: -0.1701, Value loss: 2.6977, Entropy: 9.1594\n",
      "Episode 493/1000 - Min reward: -11.8997, Max reward: 0.8559, Mean reward: -1.5414\n",
      "Episode 493/1000 - Policy loss: 0.0463, Value loss: 9.9622, Entropy: 9.1540\n",
      "Episode 494/1000 - Min reward: -25.2514, Max reward: 0.9999, Mean reward: -1.8579\n",
      "Episode 494/1000 - Policy loss: -0.1315, Value loss: 2.4617, Entropy: 9.1491\n",
      "Episode 495/1000 - Min reward: -5.9460, Max reward: 1.0000, Mean reward: 0.7452\n",
      "Episode 495/1000 - Policy loss: 0.0522, Value loss: 4.5431, Entropy: 9.1505\n",
      "Episode 496/1000 - Min reward: -12.6787, Max reward: 0.9834, Mean reward: -0.1437\n",
      "Episode 496/1000 - Policy loss: 0.0164, Value loss: 4.6507, Entropy: 9.1515\n",
      "Episode 497/1000 - Min reward: -18.0385, Max reward: 0.9998, Mean reward: -1.9080\n",
      "Episode 497/1000 - Policy loss: 0.0700, Value loss: 8.0488, Entropy: 9.1587\n",
      "Episode 498/1000 - Min reward: -17.5987, Max reward: 0.9999, Mean reward: -0.7146\n",
      "Episode 498/1000 - Policy loss: -0.0561, Value loss: 9.5555, Entropy: 9.1597\n",
      "Episode 499/1000 - Min reward: -11.1560, Max reward: 1.0000, Mean reward: 0.1610\n",
      "Episode 499/1000 - Policy loss: 0.0230, Value loss: 2.5654, Entropy: 9.1545\n",
      "Episode 500/1000 - Min reward: -16.9390, Max reward: 1.0000, Mean reward: -0.1300\n",
      "Episode 500/1000 - Policy loss: -0.0831, Value loss: 7.3293, Entropy: 9.1488\n",
      "Episode 501/1000 - Min reward: -22.4878, Max reward: 0.4812, Mean reward: -4.1232\n",
      "Episode 501/1000 - Policy loss: 0.1284, Value loss: 14.0347, Entropy: 9.1463\n",
      "Episode 502/1000 - Min reward: -15.1178, Max reward: -3.2089, Mean reward: -6.3519\n",
      "Episode 502/1000 - Policy loss: 0.2787, Value loss: 21.9127, Entropy: 9.1427\n",
      "Episode 503/1000 - Min reward: -17.9970, Max reward: 0.0893, Mean reward: -3.9176\n",
      "Episode 503/1000 - Policy loss: -0.1713, Value loss: 7.8267, Entropy: 9.1360\n",
      "Episode 504/1000 - Min reward: -22.9362, Max reward: 1.0000, Mean reward: -1.4004\n",
      "Episode 504/1000 - Policy loss: -0.0071, Value loss: 16.0678, Entropy: 9.1350\n",
      "Episode 505/1000 - Min reward: -4.8375, Max reward: 0.9997, Mean reward: 0.6956\n",
      "Episode 505/1000 - Policy loss: 0.0726, Value loss: 9.9270, Entropy: 9.1399\n",
      "Episode 506/1000 - Min reward: -8.1346, Max reward: 0.9942, Mean reward: -0.2358\n",
      "Episode 506/1000 - Policy loss: -0.2855, Value loss: 7.0992, Entropy: 9.1419\n",
      "Episode 507/1000 - Min reward: -4.5161, Max reward: 1.0000, Mean reward: 0.7871\n",
      "Episode 507/1000 - Policy loss: 0.0035, Value loss: 2.9444, Entropy: 9.1379\n",
      "Episode 508/1000 - Min reward: -13.6352, Max reward: 0.8804, Mean reward: -1.3933\n",
      "Episode 508/1000 - Policy loss: 0.0837, Value loss: 8.5372, Entropy: 9.1321\n",
      "Episode 509/1000 - Min reward: -15.7166, Max reward: 0.1934, Mean reward: -3.5774\n",
      "Episode 509/1000 - Policy loss: -0.0714, Value loss: 5.8494, Entropy: 9.1313\n",
      "Episode 510/1000 - Min reward: -15.2764, Max reward: 0.9972, Mean reward: -1.1216\n",
      "Episode 510/1000 - Policy loss: -0.3157, Value loss: 6.3487, Entropy: 9.1327\n",
      "Episode 511/1000 - Min reward: -6.1898, Max reward: 0.9993, Mean reward: 0.4385\n",
      "Episode 511/1000 - Policy loss: -0.1378, Value loss: 2.3598, Entropy: 9.1240\n",
      "Episode 512/1000 - Min reward: -15.0374, Max reward: 0.9999, Mean reward: -0.2932\n",
      "Episode 512/1000 - Policy loss: -0.1310, Value loss: 2.7854, Entropy: 9.1120\n",
      "Episode 513/1000 - Min reward: -22.6294, Max reward: -0.6620, Mean reward: -6.4496\n",
      "Episode 513/1000 - Policy loss: -0.2050, Value loss: 8.7316, Entropy: 9.0986\n",
      "Episode 514/1000 - Min reward: -27.2276, Max reward: 0.2084, Mean reward: -5.5135\n",
      "Episode 514/1000 - Policy loss: -0.1033, Value loss: 4.3025, Entropy: 9.0867\n",
      "Episode 515/1000 - Min reward: -16.2171, Max reward: -2.5419, Mean reward: -4.8806\n",
      "Episode 515/1000 - Policy loss: -0.1620, Value loss: 14.7389, Entropy: 9.0797\n",
      "Episode 516/1000 - Min reward: -17.9790, Max reward: 0.9997, Mean reward: -1.3081\n",
      "Episode 516/1000 - Policy loss: 0.0060, Value loss: 14.9977, Entropy: 9.0720\n",
      "Episode 517/1000 - Min reward: -16.7508, Max reward: 0.9979, Mean reward: -0.6162\n",
      "Episode 517/1000 - Policy loss: -0.1000, Value loss: 2.4036, Entropy: 9.0737\n",
      "Episode 518/1000 - Min reward: -17.6656, Max reward: 0.9997, Mean reward: -1.0926\n",
      "Episode 518/1000 - Policy loss: 0.0851, Value loss: 11.9779, Entropy: 9.0816\n",
      "Episode 519/1000 - Min reward: -14.0615, Max reward: 0.6970, Mean reward: -2.1752\n",
      "Episode 519/1000 - Policy loss: -0.0841, Value loss: 24.0506, Entropy: 9.0863\n",
      "Episode 520/1000 - Min reward: -15.8044, Max reward: 0.7833, Mean reward: -2.8704\n",
      "Episode 520/1000 - Policy loss: -0.4229, Value loss: 5.7069, Entropy: 9.0897\n",
      "Episode 521/1000 - Min reward: -12.8230, Max reward: 0.7275, Mean reward: -1.6558\n",
      "Episode 521/1000 - Policy loss: -0.0745, Value loss: 13.2716, Entropy: 9.0951\n",
      "Episode 522/1000 - Min reward: -24.0957, Max reward: 0.9702, Mean reward: -2.8559\n",
      "Episode 522/1000 - Policy loss: -0.1561, Value loss: 2.9033, Entropy: 9.1097\n",
      "Episode 523/1000 - Min reward: -12.5295, Max reward: 0.9997, Mean reward: 0.0300\n",
      "Episode 523/1000 - Policy loss: -0.0613, Value loss: 2.9762, Entropy: 9.1117\n",
      "Episode 524/1000 - Min reward: -10.3400, Max reward: 1.0000, Mean reward: 0.3912\n",
      "Episode 524/1000 - Policy loss: -0.2051, Value loss: 2.4031, Entropy: 9.0961\n",
      "Episode 525/1000 - Min reward: -15.0820, Max reward: -0.3314, Mean reward: -4.4566\n",
      "Episode 525/1000 - Policy loss: -0.2554, Value loss: 6.2302, Entropy: 9.0783\n",
      "Episode 526/1000 - Min reward: -8.0136, Max reward: 0.9999, Mean reward: 0.2070\n",
      "Episode 526/1000 - Policy loss: -0.1086, Value loss: 2.3245, Entropy: 9.0612\n",
      "Episode 527/1000 - Min reward: -24.3133, Max reward: 0.9999, Mean reward: -3.6604\n",
      "Episode 527/1000 - Policy loss: -0.1270, Value loss: 4.4052, Entropy: 9.0510\n",
      "Episode 528/1000 - Min reward: -20.1077, Max reward: 0.6775, Mean reward: -2.3052\n",
      "Episode 528/1000 - Policy loss: 0.0009, Value loss: 15.1154, Entropy: 9.0451\n",
      "Episode 529/1000 - Min reward: -15.0695, Max reward: 0.9636, Mean reward: -0.5206\n",
      "Episode 529/1000 - Policy loss: -0.0374, Value loss: 14.4336, Entropy: 9.0509\n",
      "Episode 530/1000 - Min reward: -13.0666, Max reward: 1.0000, Mean reward: 0.2150\n",
      "Episode 530/1000 - Policy loss: -0.1491, Value loss: 3.0462, Entropy: 9.0580\n",
      "Episode 531/1000 - Min reward: -8.9137, Max reward: 1.0000, Mean reward: 0.3693\n",
      "Episode 531/1000 - Policy loss: -0.2124, Value loss: 5.4838, Entropy: 9.0604\n",
      "Episode 532/1000 - Min reward: -8.3143, Max reward: 0.6120, Mean reward: -0.2793\n",
      "Episode 532/1000 - Policy loss: -0.1483, Value loss: 5.7134, Entropy: 9.0546\n",
      "Episode 533/1000 - Min reward: -20.4216, Max reward: 0.9989, Mean reward: -2.4922\n",
      "Episode 533/1000 - Policy loss: -0.0342, Value loss: 18.0797, Entropy: 9.0503\n",
      "Episode 534/1000 - Min reward: -26.5762, Max reward: 0.5557, Mean reward: -4.1165\n",
      "Episode 534/1000 - Policy loss: 0.0389, Value loss: 14.5336, Entropy: 9.0558\n",
      "Episode 535/1000 - Min reward: -8.6960, Max reward: 1.0000, Mean reward: 0.4687\n",
      "Episode 535/1000 - Policy loss: -0.0538, Value loss: 4.9498, Entropy: 9.0549\n",
      "Episode 536/1000 - Min reward: -24.6343, Max reward: 1.0000, Mean reward: -2.9716\n",
      "Episode 536/1000 - Policy loss: -0.0098, Value loss: 7.0410, Entropy: 9.0533\n",
      "Episode 537/1000 - Min reward: -20.4201, Max reward: 1.0000, Mean reward: -1.1153\n",
      "Episode 537/1000 - Policy loss: 0.0391, Value loss: 5.6375, Entropy: 9.0596\n",
      "Episode 538/1000 - Min reward: -20.5428, Max reward: -0.4989, Mean reward: -4.5051\n",
      "Episode 538/1000 - Policy loss: -0.2679, Value loss: 13.2578, Entropy: 9.0627\n",
      "Episode 539/1000 - Min reward: -18.0379, Max reward: 0.9930, Mean reward: -0.8002\n",
      "Episode 539/1000 - Policy loss: -0.0902, Value loss: 12.3207, Entropy: 9.0590\n",
      "Episode 540/1000 - Min reward: -22.4823, Max reward: 0.9998, Mean reward: -2.0155\n",
      "Episode 540/1000 - Policy loss: 0.2985, Value loss: 6.1067, Entropy: 9.0405\n",
      "Episode 541/1000 - Min reward: -19.2539, Max reward: -2.3107, Mean reward: -6.9772\n",
      "Episode 541/1000 - Policy loss: -0.1670, Value loss: 15.0410, Entropy: 9.0382\n",
      "Episode 542/1000 - Min reward: -18.5321, Max reward: 0.9857, Mean reward: -2.6517\n",
      "Episode 542/1000 - Policy loss: -0.3101, Value loss: 13.9540, Entropy: 9.0607\n",
      "Episode 543/1000 - Min reward: -14.4817, Max reward: 0.3938, Mean reward: -1.9258\n",
      "Episode 543/1000 - Policy loss: 0.1008, Value loss: 1.9994, Entropy: 9.0654\n",
      "Episode 544/1000 - Min reward: -23.8842, Max reward: -1.0092, Mean reward: -6.0966\n",
      "Episode 544/1000 - Policy loss: -0.1160, Value loss: 6.2445, Entropy: 9.0662\n",
      "Episode 545/1000 - Min reward: -19.0848, Max reward: 0.2265, Mean reward: -2.1682\n",
      "Episode 545/1000 - Policy loss: 0.0315, Value loss: 8.4452, Entropy: 9.0775\n",
      "Episode 546/1000 - Min reward: -9.5445, Max reward: 0.9999, Mean reward: 0.1099\n",
      "Episode 546/1000 - Policy loss: 0.0718, Value loss: 5.9590, Entropy: 9.0867\n",
      "Episode 547/1000 - Min reward: -17.6969, Max reward: 0.7209, Mean reward: -1.4815\n",
      "Episode 547/1000 - Policy loss: 0.1275, Value loss: 8.4378, Entropy: 9.0813\n",
      "Episode 548/1000 - Min reward: -16.2336, Max reward: -4.5361, Mean reward: -8.1711\n",
      "Episode 548/1000 - Policy loss: 0.1021, Value loss: 32.0160, Entropy: 9.0737\n",
      "Episode 549/1000 - Min reward: -17.2360, Max reward: -4.8727, Mean reward: -8.3300\n",
      "Episode 549/1000 - Policy loss: -0.0698, Value loss: 0.8580, Entropy: 9.0687\n",
      "Episode 550/1000 - Min reward: -19.3319, Max reward: 0.5782, Mean reward: -2.2316\n",
      "Episode 550/1000 - Policy loss: 0.1647, Value loss: 8.8224, Entropy: 9.0616\n",
      "Episode 551/1000 - Min reward: -10.9912, Max reward: 0.9964, Mean reward: 0.2394\n",
      "Episode 551/1000 - Policy loss: -0.0612, Value loss: 3.8722, Entropy: 9.0581\n",
      "Episode 552/1000 - Min reward: -18.8120, Max reward: 0.9904, Mean reward: -0.9084\n",
      "Episode 552/1000 - Policy loss: -0.1338, Value loss: 13.4634, Entropy: 9.0559\n",
      "Episode 553/1000 - Min reward: -21.4741, Max reward: 0.8483, Mean reward: -3.3741\n",
      "Episode 553/1000 - Policy loss: 0.0646, Value loss: 11.4927, Entropy: 9.0554\n",
      "Episode 554/1000 - Min reward: -18.3486, Max reward: 0.9760, Mean reward: -1.5169\n",
      "Episode 554/1000 - Policy loss: 0.0472, Value loss: 15.4499, Entropy: 9.0491\n",
      "Episode 555/1000 - Min reward: -13.2400, Max reward: 1.0000, Mean reward: 0.0091\n",
      "Episode 555/1000 - Policy loss: 0.0034, Value loss: 7.3892, Entropy: 9.0456\n",
      "Episode 556/1000 - Min reward: -14.8000, Max reward: -1.9107, Mean reward: -7.4767\n",
      "Episode 556/1000 - Policy loss: -0.2788, Value loss: 43.0697, Entropy: 9.0520\n",
      "Episode 557/1000 - Min reward: -17.8124, Max reward: 0.9981, Mean reward: -1.3445\n",
      "Episode 557/1000 - Policy loss: 0.0142, Value loss: 9.9150, Entropy: 9.0394\n",
      "Episode 558/1000 - Min reward: -2.1458, Max reward: 1.0000, Mean reward: 0.8663\n",
      "Episode 558/1000 - Policy loss: -0.0306, Value loss: 12.8081, Entropy: 9.0390\n",
      "Episode 559/1000 - Min reward: -22.3989, Max reward: 0.1081, Mean reward: -3.9252\n",
      "Episode 559/1000 - Policy loss: -0.2273, Value loss: 20.6268, Entropy: 9.0410\n",
      "Episode 560/1000 - Min reward: -20.1134, Max reward: -1.1128, Mean reward: -5.0767\n",
      "Episode 560/1000 - Policy loss: -0.0937, Value loss: 8.7323, Entropy: 9.0196\n",
      "Episode 561/1000 - Min reward: -13.5194, Max reward: -4.8427, Mean reward: -7.1749\n",
      "Episode 561/1000 - Policy loss: -0.1370, Value loss: 12.2161, Entropy: 8.9870\n",
      "Episode 562/1000 - Min reward: -6.2801, Max reward: 0.9989, Mean reward: 0.6455\n",
      "Episode 562/1000 - Policy loss: -0.0548, Value loss: 3.5442, Entropy: 8.9743\n",
      "Episode 563/1000 - Min reward: -21.2164, Max reward: 0.1142, Mean reward: -5.6788\n",
      "Episode 563/1000 - Policy loss: 0.0740, Value loss: 10.9586, Entropy: 8.9788\n",
      "Episode 564/1000 - Min reward: -9.9997, Max reward: 0.9987, Mean reward: 0.2740\n",
      "Episode 564/1000 - Policy loss: 0.1175, Value loss: 5.6384, Entropy: 8.9868\n",
      "Episode 565/1000 - Min reward: -14.1038, Max reward: 0.2287, Mean reward: -3.2814\n",
      "Episode 565/1000 - Policy loss: -0.0400, Value loss: 16.2430, Entropy: 8.9861\n",
      "Episode 566/1000 - Min reward: -7.3700, Max reward: 1.0000, Mean reward: 0.6826\n",
      "Episode 566/1000 - Policy loss: 0.0871, Value loss: 1.3331, Entropy: 8.9763\n",
      "Episode 567/1000 - Min reward: -14.9403, Max reward: -1.2757, Mean reward: -5.0079\n",
      "Episode 567/1000 - Policy loss: 0.0847, Value loss: 45.2786, Entropy: 8.9736\n",
      "Episode 568/1000 - Min reward: -12.5992, Max reward: -4.3295, Mean reward: -6.5803\n",
      "Episode 568/1000 - Policy loss: -0.2061, Value loss: 6.2013, Entropy: 8.9798\n",
      "Episode 569/1000 - Min reward: -15.6422, Max reward: 0.9466, Mean reward: -1.0429\n",
      "Episode 569/1000 - Policy loss: -0.2254, Value loss: 3.3695, Entropy: 8.9815\n",
      "Episode 570/1000 - Min reward: -11.1992, Max reward: 0.9997, Mean reward: -0.1547\n",
      "Episode 570/1000 - Policy loss: 0.1434, Value loss: 1.4629, Entropy: 8.9754\n",
      "Episode 571/1000 - Min reward: -8.6791, Max reward: 0.9990, Mean reward: 0.3005\n",
      "Episode 571/1000 - Policy loss: -0.1909, Value loss: 5.3235, Entropy: 8.9796\n",
      "Episode 572/1000 - Min reward: -24.6581, Max reward: -0.1212, Mean reward: -5.6730\n",
      "Episode 572/1000 - Policy loss: -0.1294, Value loss: 17.6381, Entropy: 8.9909\n",
      "Episode 573/1000 - Min reward: -9.1224, Max reward: 0.9780, Mean reward: -0.0300\n",
      "Episode 573/1000 - Policy loss: -0.1726, Value loss: 16.3349, Entropy: 8.9950\n",
      "Episode 574/1000 - Min reward: -13.2540, Max reward: 0.9999, Mean reward: -0.1638\n",
      "Episode 574/1000 - Policy loss: 0.0575, Value loss: 2.4089, Entropy: 8.9944\n",
      "Episode 575/1000 - Min reward: -13.0338, Max reward: 0.1218, Mean reward: -3.6271\n",
      "Episode 575/1000 - Policy loss: -0.2108, Value loss: 23.6408, Entropy: 9.0126\n",
      "Episode 576/1000 - Min reward: -13.1016, Max reward: 0.9112, Mean reward: -1.9933\n",
      "Episode 576/1000 - Policy loss: 0.0164, Value loss: 6.8819, Entropy: 9.0130\n",
      "Episode 577/1000 - Min reward: -15.2412, Max reward: 0.1025, Mean reward: -4.0853\n",
      "Episode 577/1000 - Policy loss: -0.0299, Value loss: 16.8820, Entropy: 9.0132\n",
      "Episode 578/1000 - Min reward: -20.6795, Max reward: 0.9351, Mean reward: -2.7566\n",
      "Episode 578/1000 - Policy loss: -0.1110, Value loss: 17.8275, Entropy: 9.0320\n",
      "Episode 579/1000 - Min reward: -6.9987, Max reward: 0.1052, Mean reward: -1.7941\n",
      "Episode 579/1000 - Policy loss: -0.0840, Value loss: 3.6315, Entropy: 9.0408\n",
      "Episode 580/1000 - Min reward: -7.8816, Max reward: 0.9994, Mean reward: 0.5726\n",
      "Episode 580/1000 - Policy loss: -0.1097, Value loss: 2.1154, Entropy: 9.0297\n",
      "Episode 581/1000 - Min reward: -13.5613, Max reward: 0.9929, Mean reward: -0.3573\n",
      "Episode 581/1000 - Policy loss: -0.2303, Value loss: 12.5474, Entropy: 9.0343\n",
      "Episode 582/1000 - Min reward: -26.5099, Max reward: 0.3520, Mean reward: -4.2261\n",
      "Episode 582/1000 - Policy loss: -0.0313, Value loss: 9.4077, Entropy: 9.0396\n",
      "Episode 583/1000 - Min reward: -14.8810, Max reward: 1.0000, Mean reward: -0.3319\n",
      "Episode 583/1000 - Policy loss: -0.2325, Value loss: 2.8475, Entropy: 9.0379\n",
      "Episode 584/1000 - Min reward: -16.1470, Max reward: 1.0000, Mean reward: -1.5223\n",
      "Episode 584/1000 - Policy loss: -0.1384, Value loss: 8.2034, Entropy: 9.0373\n",
      "Episode 585/1000 - Min reward: 0.8229, Max reward: 0.9999, Mean reward: 0.9820\n",
      "Episode 585/1000 - Policy loss: -0.1331, Value loss: 17.3847, Entropy: 9.0312\n",
      "Episode 586/1000 - Min reward: -11.3010, Max reward: 0.3536, Mean reward: -1.7199\n",
      "Episode 586/1000 - Policy loss: -0.0465, Value loss: 9.0277, Entropy: 9.0300\n",
      "Episode 587/1000 - Min reward: -16.7183, Max reward: 0.9987, Mean reward: -1.2116\n",
      "Episode 587/1000 - Policy loss: 0.2403, Value loss: 10.0617, Entropy: 9.0361\n",
      "Episode 588/1000 - Min reward: -2.5300, Max reward: 1.0000, Mean reward: 0.8464\n",
      "Episode 588/1000 - Policy loss: 0.2061, Value loss: 5.3270, Entropy: 9.0363\n",
      "Episode 589/1000 - Min reward: -8.8497, Max reward: 1.0000, Mean reward: 0.3494\n",
      "Episode 589/1000 - Policy loss: -0.1112, Value loss: 0.5483, Entropy: 9.0359\n",
      "Episode 590/1000 - Min reward: -18.8911, Max reward: 0.5693, Mean reward: -2.2754\n",
      "Episode 590/1000 - Policy loss: -0.2008, Value loss: 8.7636, Entropy: 9.0443\n",
      "Episode 591/1000 - Min reward: -24.9875, Max reward: 0.9918, Mean reward: -3.0149\n",
      "Episode 591/1000 - Policy loss: 0.0073, Value loss: 19.1206, Entropy: 9.0566\n",
      "Episode 592/1000 - Min reward: -19.6147, Max reward: 0.9994, Mean reward: -1.9039\n",
      "Episode 592/1000 - Policy loss: -0.0200, Value loss: 5.1781, Entropy: 9.0536\n",
      "Episode 593/1000 - Min reward: -13.3245, Max reward: 0.9997, Mean reward: -0.1722\n",
      "Episode 593/1000 - Policy loss: -0.1347, Value loss: 1.6492, Entropy: 9.0505\n",
      "Episode 594/1000 - Min reward: -22.8824, Max reward: 0.9772, Mean reward: -2.6113\n",
      "Episode 594/1000 - Policy loss: 0.0239, Value loss: 2.4276, Entropy: 9.0555\n",
      "Episode 595/1000 - Min reward: -7.2515, Max reward: 0.9077, Mean reward: -0.1122\n",
      "Episode 595/1000 - Policy loss: -0.0269, Value loss: 14.6566, Entropy: 9.0542\n",
      "Episode 596/1000 - Min reward: -25.3853, Max reward: 0.3872, Mean reward: -4.7910\n",
      "Episode 596/1000 - Policy loss: 0.1218, Value loss: 13.2987, Entropy: 9.0481\n",
      "Episode 597/1000 - Min reward: -27.9541, Max reward: 0.9521, Mean reward: -4.1646\n",
      "Episode 597/1000 - Policy loss: -0.3858, Value loss: 7.8508, Entropy: 9.0526\n",
      "Episode 598/1000 - Min reward: -8.3776, Max reward: 0.9959, Mean reward: 0.2740\n",
      "Episode 598/1000 - Policy loss: 0.0076, Value loss: 17.1161, Entropy: 9.0543\n",
      "Episode 599/1000 - Min reward: -15.4422, Max reward: 0.9999, Mean reward: -1.5091\n",
      "Episode 599/1000 - Policy loss: -0.1381, Value loss: 13.5121, Entropy: 9.0524\n",
      "Episode 600/1000 - Min reward: -10.1195, Max reward: 0.4131, Mean reward: -3.0907\n",
      "Episode 600/1000 - Policy loss: -0.3529, Value loss: 21.8748, Entropy: 9.0490\n",
      "Episode 601/1000 - Min reward: -23.8023, Max reward: -5.2099, Mean reward: -8.7771\n",
      "Episode 601/1000 - Policy loss: -0.1155, Value loss: 14.7698, Entropy: 9.0563\n",
      "Episode 602/1000 - Min reward: -14.1802, Max reward: 0.9491, Mean reward: -2.2136\n",
      "Episode 602/1000 - Policy loss: 0.0678, Value loss: 14.1004, Entropy: 9.0650\n",
      "Episode 603/1000 - Min reward: -22.5807, Max reward: 0.8001, Mean reward: -4.1125\n",
      "Episode 603/1000 - Policy loss: 0.0915, Value loss: 18.9751, Entropy: 9.0670\n",
      "Episode 604/1000 - Min reward: -14.6488, Max reward: 0.9073, Mean reward: -1.2607\n",
      "Episode 604/1000 - Policy loss: -0.1169, Value loss: 11.9069, Entropy: 9.0595\n",
      "Episode 605/1000 - Min reward: -10.0395, Max reward: 0.9850, Mean reward: 0.2302\n",
      "Episode 605/1000 - Policy loss: 0.1775, Value loss: 6.8302, Entropy: 9.0408\n",
      "Episode 606/1000 - Min reward: -9.6718, Max reward: 0.7376, Mean reward: -0.2378\n",
      "Episode 606/1000 - Policy loss: -0.2402, Value loss: 5.9506, Entropy: 9.0320\n",
      "Episode 607/1000 - Min reward: -19.1318, Max reward: -0.9052, Mean reward: -4.7461\n",
      "Episode 607/1000 - Policy loss: -0.0775, Value loss: 6.1970, Entropy: 9.0268\n",
      "Episode 608/1000 - Min reward: -17.2901, Max reward: 0.4981, Mean reward: -2.6976\n",
      "Episode 608/1000 - Policy loss: -0.0419, Value loss: 22.3369, Entropy: 9.0291\n",
      "Episode 609/1000 - Min reward: -16.6450, Max reward: 0.3508, Mean reward: -3.6646\n",
      "Episode 609/1000 - Policy loss: 0.0768, Value loss: 10.1928, Entropy: 9.0398\n",
      "Episode 610/1000 - Min reward: -20.2472, Max reward: 0.7604, Mean reward: -2.4770\n",
      "Episode 610/1000 - Policy loss: -0.2052, Value loss: 0.8807, Entropy: 9.0508\n",
      "Episode 611/1000 - Min reward: -16.5861, Max reward: 0.5001, Mean reward: -1.4338\n",
      "Episode 611/1000 - Policy loss: 0.0431, Value loss: 1.9097, Entropy: 9.0541\n",
      "Episode 612/1000 - Min reward: -23.8475, Max reward: 0.1198, Mean reward: -5.5706\n",
      "Episode 612/1000 - Policy loss: 0.0454, Value loss: 17.7410, Entropy: 9.0503\n",
      "Episode 613/1000 - Min reward: -22.1772, Max reward: -1.8225, Mean reward: -10.4286\n",
      "Episode 613/1000 - Policy loss: -0.0828, Value loss: 14.2888, Entropy: 9.0387\n",
      "Episode 614/1000 - Min reward: -14.2080, Max reward: 0.9093, Mean reward: -1.1323\n",
      "Episode 614/1000 - Policy loss: 0.1162, Value loss: 3.2638, Entropy: 9.0271\n",
      "Episode 615/1000 - Min reward: -22.6980, Max reward: -0.6839, Mean reward: -7.0216\n",
      "Episode 615/1000 - Policy loss: 0.0406, Value loss: 7.5800, Entropy: 9.0163\n",
      "Episode 616/1000 - Min reward: -10.7361, Max reward: 0.9972, Mean reward: 0.0826\n",
      "Episode 616/1000 - Policy loss: 0.0505, Value loss: 7.4997, Entropy: 9.0196\n",
      "Episode 617/1000 - Min reward: -14.2781, Max reward: 0.9739, Mean reward: -1.5865\n",
      "Episode 617/1000 - Policy loss: 0.0953, Value loss: 1.3603, Entropy: 9.0251\n",
      "Episode 618/1000 - Min reward: -16.4898, Max reward: 0.9922, Mean reward: -0.8937\n",
      "Episode 618/1000 - Policy loss: 0.0578, Value loss: 13.0045, Entropy: 9.0407\n",
      "Episode 619/1000 - Min reward: 0.1440, Max reward: 1.0000, Mean reward: 0.9594\n",
      "Episode 619/1000 - Policy loss: 0.0660, Value loss: 9.1314, Entropy: 9.0529\n",
      "Episode 620/1000 - Min reward: -21.1606, Max reward: 0.9750, Mean reward: -2.5743\n",
      "Episode 620/1000 - Policy loss: -0.2025, Value loss: 19.5300, Entropy: 9.0477\n",
      "Episode 621/1000 - Min reward: -4.7666, Max reward: 0.9985, Mean reward: 0.6956\n",
      "Episode 621/1000 - Policy loss: -0.2853, Value loss: 5.7035, Entropy: 9.0487\n",
      "Episode 622/1000 - Min reward: -21.4334, Max reward: 0.0942, Mean reward: -4.0549\n",
      "Episode 622/1000 - Policy loss: 0.1318, Value loss: 9.0828, Entropy: 9.0492\n",
      "Episode 623/1000 - Min reward: -15.7536, Max reward: 0.8138, Mean reward: -1.2851\n",
      "Episode 623/1000 - Policy loss: -0.2972, Value loss: 11.7567, Entropy: 9.0618\n",
      "Episode 624/1000 - Min reward: -13.4960, Max reward: 1.0000, Mean reward: -0.3941\n",
      "Episode 624/1000 - Policy loss: 0.0208, Value loss: 6.4211, Entropy: 9.0612\n",
      "Episode 625/1000 - Min reward: -4.2444, Max reward: 1.0000, Mean reward: 0.7803\n",
      "Episode 625/1000 - Policy loss: 0.0994, Value loss: 7.8171, Entropy: 9.0652\n",
      "Episode 626/1000 - Min reward: -13.2351, Max reward: 0.9999, Mean reward: -0.2070\n",
      "Episode 626/1000 - Policy loss: -0.1158, Value loss: 6.9859, Entropy: 9.0756\n",
      "Episode 627/1000 - Min reward: -12.2688, Max reward: 0.9997, Mean reward: 0.0777\n",
      "Episode 627/1000 - Policy loss: -0.0624, Value loss: 7.0951, Entropy: 9.0803\n",
      "Episode 628/1000 - Min reward: -19.5065, Max reward: 0.9990, Mean reward: -2.0755\n",
      "Episode 628/1000 - Policy loss: -0.3071, Value loss: 3.0853, Entropy: 9.0815\n",
      "Episode 629/1000 - Min reward: 0.3302, Max reward: 0.9920, Mean reward: 0.8073\n",
      "Episode 629/1000 - Policy loss: 0.1461, Value loss: 110.1189, Entropy: 9.0833\n",
      "Episode 630/1000 - Min reward: -12.7155, Max reward: 0.9984, Mean reward: -0.4284\n",
      "Episode 630/1000 - Policy loss: -0.2946, Value loss: 3.1969, Entropy: 9.0862\n",
      "Episode 631/1000 - Min reward: -16.1677, Max reward: 0.9891, Mean reward: -1.0708\n",
      "Episode 631/1000 - Policy loss: -0.0520, Value loss: 3.8707, Entropy: 9.0889\n",
      "Episode 632/1000 - Min reward: -16.6107, Max reward: 0.5550, Mean reward: -4.1694\n",
      "Episode 632/1000 - Policy loss: 0.1020, Value loss: 57.2507, Entropy: 9.1107\n",
      "Episode 633/1000 - Min reward: -9.9307, Max reward: 0.2141, Mean reward: -3.7861\n",
      "Episode 633/1000 - Policy loss: 0.0342, Value loss: 43.4942, Entropy: 9.1268\n",
      "Episode 634/1000 - Min reward: -19.0400, Max reward: -0.8413, Mean reward: -3.9901\n",
      "Episode 634/1000 - Policy loss: -0.1280, Value loss: 13.6372, Entropy: 9.1321\n",
      "Episode 635/1000 - Min reward: -14.8553, Max reward: 0.9999, Mean reward: -0.1436\n",
      "Episode 635/1000 - Policy loss: -0.1906, Value loss: 4.4570, Entropy: 9.1366\n",
      "Episode 636/1000 - Min reward: -15.4303, Max reward: 0.9995, Mean reward: -1.6799\n",
      "Episode 636/1000 - Policy loss: -0.1314, Value loss: 11.5705, Entropy: 9.1283\n",
      "Episode 637/1000 - Min reward: -21.6639, Max reward: 0.9858, Mean reward: -2.3468\n",
      "Episode 637/1000 - Policy loss: -0.2023, Value loss: 3.4073, Entropy: 9.1083\n",
      "Episode 638/1000 - Min reward: -23.1172, Max reward: 0.9999, Mean reward: -1.1870\n",
      "Episode 638/1000 - Policy loss: -0.2207, Value loss: 4.2427, Entropy: 9.0918\n",
      "Episode 639/1000 - Min reward: -23.5104, Max reward: 0.8348, Mean reward: -2.9676\n",
      "Episode 639/1000 - Policy loss: -0.0991, Value loss: 2.3131, Entropy: 9.0842\n",
      "Episode 640/1000 - Min reward: -12.0378, Max reward: 0.9951, Mean reward: 0.0465\n",
      "Episode 640/1000 - Policy loss: 0.0339, Value loss: 6.9097, Entropy: 9.0777\n",
      "Episode 641/1000 - Min reward: -18.3616, Max reward: 0.9844, Mean reward: -1.0079\n",
      "Episode 641/1000 - Policy loss: 0.1715, Value loss: 10.9673, Entropy: 9.0728\n",
      "Episode 642/1000 - Min reward: -19.9657, Max reward: 0.7096, Mean reward: -3.4132\n",
      "Episode 642/1000 - Policy loss: -0.1662, Value loss: 4.6330, Entropy: 9.0768\n",
      "Episode 643/1000 - Min reward: -2.0196, Max reward: 1.0000, Mean reward: 0.8691\n",
      "Episode 643/1000 - Policy loss: 0.0774, Value loss: 2.7449, Entropy: 9.0741\n",
      "Episode 644/1000 - Min reward: 0.9692, Max reward: 1.0000, Mean reward: 0.9982\n",
      "Episode 644/1000 - Policy loss: -0.3117, Value loss: 4.0862, Entropy: 9.0680\n",
      "Episode 645/1000 - Min reward: -10.0431, Max reward: 0.8824, Mean reward: -1.1007\n",
      "Episode 645/1000 - Policy loss: -0.1686, Value loss: 10.7593, Entropy: 9.0638\n",
      "Episode 646/1000 - Min reward: -3.5104, Max reward: 0.9999, Mean reward: 0.8322\n",
      "Episode 646/1000 - Policy loss: 0.0110, Value loss: 6.5681, Entropy: 9.0630\n",
      "Episode 647/1000 - Min reward: -16.0168, Max reward: -0.9711, Mean reward: -4.3727\n",
      "Episode 647/1000 - Policy loss: -0.0043, Value loss: 16.5592, Entropy: 9.0774\n",
      "Episode 648/1000 - Min reward: -16.9986, Max reward: 0.9622, Mean reward: -1.5193\n",
      "Episode 648/1000 - Policy loss: -0.2892, Value loss: 3.0983, Entropy: 9.0899\n",
      "Episode 649/1000 - Min reward: -23.0872, Max reward: 0.9647, Mean reward: -2.0272\n",
      "Episode 649/1000 - Policy loss: 0.0075, Value loss: 4.7985, Entropy: 9.0863\n",
      "Episode 650/1000 - Min reward: -25.2768, Max reward: -1.4631, Mean reward: -10.0572\n",
      "Episode 650/1000 - Policy loss: -0.0857, Value loss: 3.1391, Entropy: 9.0800\n",
      "Episode 651/1000 - Min reward: -15.3948, Max reward: 0.9935, Mean reward: -0.2194\n",
      "Episode 651/1000 - Policy loss: -0.2138, Value loss: 10.4913, Entropy: 9.0748\n",
      "Episode 652/1000 - Min reward: -24.2445, Max reward: 0.9783, Mean reward: -3.1632\n",
      "Episode 652/1000 - Policy loss: 0.0281, Value loss: 5.6295, Entropy: 9.0771\n",
      "Episode 653/1000 - Min reward: 0.8106, Max reward: 1.0000, Mean reward: 0.9953\n",
      "Episode 653/1000 - Policy loss: 0.0776, Value loss: 3.8579, Entropy: 9.0822\n",
      "Episode 654/1000 - Min reward: -14.7678, Max reward: 0.0789, Mean reward: -3.3275\n",
      "Episode 654/1000 - Policy loss: 0.2516, Value loss: 20.7705, Entropy: 9.0761\n",
      "Episode 655/1000 - Min reward: -10.1199, Max reward: 0.9414, Mean reward: -0.5184\n",
      "Episode 655/1000 - Policy loss: -0.2329, Value loss: 4.7250, Entropy: 9.0640\n",
      "Episode 656/1000 - Min reward: -3.8813, Max reward: 1.0000, Mean reward: 0.7089\n",
      "Episode 656/1000 - Policy loss: 0.0897, Value loss: 13.8279, Entropy: 9.0559\n",
      "Episode 657/1000 - Min reward: -20.4650, Max reward: 0.9999, Mean reward: -1.2931\n",
      "Episode 657/1000 - Policy loss: 0.2426, Value loss: 10.6338, Entropy: 9.0449\n",
      "Episode 658/1000 - Min reward: -18.2824, Max reward: 1.0000, Mean reward: -0.8409\n",
      "Episode 658/1000 - Policy loss: -0.2109, Value loss: 5.1880, Entropy: 9.0354\n",
      "Episode 659/1000 - Min reward: 0.1835, Max reward: 1.0000, Mean reward: 0.9679\n",
      "Episode 659/1000 - Policy loss: -0.0816, Value loss: 3.6287, Entropy: 9.0446\n",
      "Episode 660/1000 - Min reward: -18.8752, Max reward: 0.9965, Mean reward: -1.0854\n",
      "Episode 660/1000 - Policy loss: 0.0608, Value loss: 14.9189, Entropy: 9.0507\n",
      "Episode 661/1000 - Min reward: -18.8210, Max reward: 0.9767, Mean reward: -5.5617\n",
      "Episode 661/1000 - Policy loss: -0.0141, Value loss: 3.9916, Entropy: 9.0485\n",
      "Episode 662/1000 - Min reward: -10.9453, Max reward: 0.9776, Mean reward: -0.9598\n",
      "Episode 662/1000 - Policy loss: -0.1514, Value loss: 4.8166, Entropy: 9.0545\n",
      "Episode 663/1000 - Min reward: -11.9790, Max reward: 1.0000, Mean reward: -0.9639\n",
      "Episode 663/1000 - Policy loss: 0.0799, Value loss: 6.1643, Entropy: 9.0612\n",
      "Episode 664/1000 - Min reward: 0.4332, Max reward: 1.0000, Mean reward: 0.9860\n",
      "Episode 664/1000 - Policy loss: 0.0355, Value loss: 8.0436, Entropy: 9.0699\n",
      "Episode 665/1000 - Min reward: -14.4596, Max reward: 0.5311, Mean reward: -3.3620\n",
      "Episode 665/1000 - Policy loss: -0.0296, Value loss: 4.9081, Entropy: 9.0774\n",
      "Episode 666/1000 - Min reward: -12.5056, Max reward: 0.8222, Mean reward: -1.0274\n",
      "Episode 666/1000 - Policy loss: 0.1423, Value loss: 11.8977, Entropy: 9.0722\n",
      "Episode 667/1000 - Min reward: -21.0445, Max reward: 0.7958, Mean reward: -7.4512\n",
      "Episode 667/1000 - Policy loss: -0.1415, Value loss: 5.4395, Entropy: 9.0697\n",
      "Episode 668/1000 - Min reward: -17.6340, Max reward: 0.5878, Mean reward: -4.5726\n",
      "Episode 668/1000 - Policy loss: 0.0206, Value loss: 1.8416, Entropy: 9.0769\n",
      "Episode 669/1000 - Min reward: -18.3515, Max reward: 0.3905, Mean reward: -4.7999\n",
      "Episode 669/1000 - Policy loss: -0.1997, Value loss: 8.8861, Entropy: 9.0822\n",
      "Episode 670/1000 - Min reward: -10.5878, Max reward: 0.3803, Mean reward: -2.1440\n",
      "Episode 670/1000 - Policy loss: -0.0588, Value loss: 24.0986, Entropy: 9.0892\n",
      "Episode 671/1000 - Min reward: -22.2796, Max reward: 0.9881, Mean reward: -1.1850\n",
      "Episode 671/1000 - Policy loss: 0.0311, Value loss: 8.9677, Entropy: 9.1097\n",
      "Episode 672/1000 - Min reward: -16.9611, Max reward: 0.9636, Mean reward: -4.4285\n",
      "Episode 672/1000 - Policy loss: -0.2473, Value loss: 3.7501, Entropy: 9.1161\n",
      "Episode 673/1000 - Min reward: -6.6993, Max reward: 0.1054, Mean reward: -1.6461\n",
      "Episode 673/1000 - Policy loss: 0.0375, Value loss: 8.4843, Entropy: 9.1187\n",
      "Episode 674/1000 - Min reward: -18.4091, Max reward: 0.9852, Mean reward: -4.0669\n",
      "Episode 674/1000 - Policy loss: -0.2571, Value loss: 4.7994, Entropy: 9.1167\n",
      "Episode 675/1000 - Min reward: -19.8138, Max reward: 0.5508, Mean reward: -5.6547\n",
      "Episode 675/1000 - Policy loss: -0.1457, Value loss: 4.5479, Entropy: 9.1011\n",
      "Episode 676/1000 - Min reward: -0.8711, Max reward: 0.9900, Mean reward: 0.6960\n",
      "Episode 676/1000 - Policy loss: -0.1706, Value loss: 8.9589, Entropy: 9.0972\n",
      "Episode 677/1000 - Min reward: -6.2155, Max reward: 0.9984, Mean reward: 0.6000\n",
      "Episode 677/1000 - Policy loss: -0.1226, Value loss: 13.5206, Entropy: 9.0994\n",
      "Episode 678/1000 - Min reward: -12.6023, Max reward: 0.9997, Mean reward: 0.0004\n",
      "Episode 678/1000 - Policy loss: 0.0349, Value loss: 1.8774, Entropy: 9.0947\n",
      "Episode 679/1000 - Min reward: -9.2738, Max reward: 0.9544, Mean reward: 0.1482\n",
      "Episode 679/1000 - Policy loss: 0.1134, Value loss: 4.3802, Entropy: 9.0905\n",
      "Episode 680/1000 - Min reward: -11.2625, Max reward: 0.9986, Mean reward: -0.2111\n",
      "Episode 680/1000 - Policy loss: -0.2871, Value loss: 2.0026, Entropy: 9.0779\n",
      "Episode 681/1000 - Min reward: -19.0043, Max reward: 0.9536, Mean reward: -2.1338\n",
      "Episode 681/1000 - Policy loss: -0.2004, Value loss: 8.4402, Entropy: 9.0724\n",
      "Episode 682/1000 - Min reward: -11.6309, Max reward: 1.0000, Mean reward: -1.0069\n",
      "Episode 682/1000 - Policy loss: -0.3106, Value loss: 20.0556, Entropy: 9.0846\n",
      "Episode 683/1000 - Min reward: -21.1043, Max reward: 0.9134, Mean reward: -1.7493\n",
      "Episode 683/1000 - Policy loss: 0.0934, Value loss: 15.3379, Entropy: 9.0889\n",
      "Episode 684/1000 - Min reward: -17.2409, Max reward: -0.3440, Mean reward: -6.7204\n",
      "Episode 684/1000 - Policy loss: 0.0930, Value loss: 13.0800, Entropy: 9.0902\n",
      "Episode 685/1000 - Min reward: -23.4020, Max reward: 0.9936, Mean reward: -4.0276\n",
      "Episode 685/1000 - Policy loss: -0.0778, Value loss: 5.3682, Entropy: 9.0860\n",
      "Episode 686/1000 - Min reward: -20.7569, Max reward: 0.9874, Mean reward: -5.0542\n",
      "Episode 686/1000 - Policy loss: -0.0361, Value loss: 18.8082, Entropy: 9.0841\n",
      "Episode 687/1000 - Min reward: -12.2019, Max reward: 0.0760, Mean reward: -3.4416\n",
      "Episode 687/1000 - Policy loss: -0.0420, Value loss: 13.1977, Entropy: 9.0881\n",
      "Episode 688/1000 - Min reward: -13.5965, Max reward: 0.3922, Mean reward: -3.1152\n",
      "Episode 688/1000 - Policy loss: -0.3032, Value loss: 16.9964, Entropy: 9.0881\n",
      "Episode 689/1000 - Min reward: -13.6126, Max reward: -0.1340, Mean reward: -5.2120\n",
      "Episode 689/1000 - Policy loss: -0.0302, Value loss: 11.3075, Entropy: 9.0827\n",
      "Episode 690/1000 - Min reward: -17.9419, Max reward: 0.9964, Mean reward: -0.4235\n",
      "Episode 690/1000 - Policy loss: -0.1005, Value loss: 7.1624, Entropy: 9.0772\n",
      "Episode 691/1000 - Min reward: -14.4425, Max reward: 0.1179, Mean reward: -5.6957\n",
      "Episode 691/1000 - Policy loss: -0.1380, Value loss: 13.8543, Entropy: 9.0782\n",
      "Episode 692/1000 - Min reward: -28.3878, Max reward: 0.9162, Mean reward: -6.5640\n",
      "Episode 692/1000 - Policy loss: -0.0182, Value loss: 16.0238, Entropy: 9.0796\n",
      "Episode 693/1000 - Min reward: -23.6844, Max reward: 0.9259, Mean reward: -4.2573\n",
      "Episode 693/1000 - Policy loss: 0.0727, Value loss: 22.0860, Entropy: 9.0887\n",
      "Episode 694/1000 - Min reward: -13.2657, Max reward: 0.9997, Mean reward: 0.1612\n",
      "Episode 694/1000 - Policy loss: -0.1264, Value loss: 11.9387, Entropy: 9.0909\n",
      "Episode 695/1000 - Min reward: -10.5687, Max reward: 0.9575, Mean reward: -1.1814\n",
      "Episode 695/1000 - Policy loss: 0.0026, Value loss: 12.0467, Entropy: 9.0909\n",
      "Episode 696/1000 - Min reward: -18.1836, Max reward: 0.9913, Mean reward: -0.7066\n",
      "Episode 696/1000 - Policy loss: -0.1678, Value loss: 7.2046, Entropy: 9.0978\n",
      "Episode 697/1000 - Min reward: -9.4133, Max reward: 0.9983, Mean reward: -0.1281\n",
      "Episode 697/1000 - Policy loss: -0.1028, Value loss: 7.2208, Entropy: 9.0960\n",
      "Episode 698/1000 - Min reward: -10.9884, Max reward: -0.1677, Mean reward: -2.7553\n",
      "Episode 698/1000 - Policy loss: -0.2225, Value loss: 2.3878, Entropy: 9.0968\n",
      "Episode 699/1000 - Min reward: -21.9492, Max reward: 0.8239, Mean reward: -6.4433\n",
      "Episode 699/1000 - Policy loss: -0.0700, Value loss: 2.0175, Entropy: 9.1050\n",
      "Episode 700/1000 - Min reward: -15.0824, Max reward: 0.9998, Mean reward: -0.3182\n",
      "Episode 700/1000 - Policy loss: 0.1416, Value loss: 8.1518, Entropy: 9.0979\n",
      "Episode 701/1000 - Min reward: -17.0058, Max reward: 0.9998, Mean reward: -1.2078\n",
      "Episode 701/1000 - Policy loss: -0.2559, Value loss: 5.0767, Entropy: 9.0916\n",
      "Episode 702/1000 - Min reward: -14.1298, Max reward: 0.9772, Mean reward: -0.5835\n",
      "Episode 702/1000 - Policy loss: -0.0143, Value loss: 3.1420, Entropy: 9.0817\n",
      "Episode 703/1000 - Min reward: -18.5451, Max reward: -5.8435, Mean reward: -12.6165\n",
      "Episode 703/1000 - Policy loss: 0.1121, Value loss: 11.4084, Entropy: 9.0752\n",
      "Episode 704/1000 - Min reward: -18.3912, Max reward: 0.7515, Mean reward: -2.3627\n",
      "Episode 704/1000 - Policy loss: -0.0696, Value loss: 6.9280, Entropy: 9.0872\n",
      "Episode 705/1000 - Min reward: -16.9027, Max reward: 0.1574, Mean reward: -4.7668\n",
      "Episode 705/1000 - Policy loss: -0.3141, Value loss: 5.0705, Entropy: 9.1011\n",
      "Episode 706/1000 - Min reward: -12.0090, Max reward: 0.9656, Mean reward: -1.0789\n",
      "Episode 706/1000 - Policy loss: -0.0104, Value loss: 8.6655, Entropy: 9.1054\n",
      "Episode 707/1000 - Min reward: -7.1723, Max reward: 0.9935, Mean reward: 0.3885\n",
      "Episode 707/1000 - Policy loss: 0.0869, Value loss: 3.5169, Entropy: 9.1043\n",
      "Episode 708/1000 - Min reward: -14.6571, Max reward: 0.9997, Mean reward: -0.8138\n",
      "Episode 708/1000 - Policy loss: -0.2072, Value loss: 13.3475, Entropy: 9.1013\n",
      "Episode 709/1000 - Min reward: -23.3582, Max reward: 0.1993, Mean reward: -5.4580\n",
      "Episode 709/1000 - Policy loss: -0.0110, Value loss: 6.9124, Entropy: 9.1101\n",
      "Episode 710/1000 - Min reward: -17.2324, Max reward: -3.4647, Mean reward: -7.6683\n",
      "Episode 710/1000 - Policy loss: 0.0901, Value loss: 8.3232, Entropy: 9.1110\n",
      "Episode 711/1000 - Min reward: -19.7654, Max reward: 0.9868, Mean reward: -2.7132\n",
      "Episode 711/1000 - Policy loss: 0.0610, Value loss: 6.3058, Entropy: 9.1100\n",
      "Episode 712/1000 - Min reward: -27.3083, Max reward: -0.4881, Mean reward: -9.5369\n",
      "Episode 712/1000 - Policy loss: -0.0406, Value loss: 2.3129, Entropy: 9.1103\n",
      "Episode 713/1000 - Min reward: -16.5060, Max reward: 0.2903, Mean reward: -2.0245\n",
      "Episode 713/1000 - Policy loss: 0.1868, Value loss: 9.2430, Entropy: 9.1034\n",
      "Episode 714/1000 - Min reward: -14.5877, Max reward: 0.9848, Mean reward: -1.3382\n",
      "Episode 714/1000 - Policy loss: 0.0168, Value loss: 3.2211, Entropy: 9.0934\n",
      "Episode 715/1000 - Min reward: -15.9758, Max reward: 0.3027, Mean reward: -2.9032\n",
      "Episode 715/1000 - Policy loss: -0.1300, Value loss: 13.4948, Entropy: 9.0863\n",
      "Episode 716/1000 - Min reward: -18.8405, Max reward: 0.9969, Mean reward: -0.8326\n",
      "Episode 716/1000 - Policy loss: -0.1101, Value loss: 2.9385, Entropy: 9.0871\n",
      "Episode 717/1000 - Min reward: -21.1126, Max reward: 0.4358, Mean reward: -2.9258\n",
      "Episode 717/1000 - Policy loss: -0.1504, Value loss: 20.6622, Entropy: 9.0893\n",
      "Episode 718/1000 - Min reward: -10.9929, Max reward: 0.9924, Mean reward: -0.0728\n",
      "Episode 718/1000 - Policy loss: 0.1652, Value loss: 4.9957, Entropy: 9.0923\n",
      "Episode 719/1000 - Min reward: -6.8813, Max reward: 0.3305, Mean reward: -0.6317\n",
      "Episode 719/1000 - Policy loss: 0.0570, Value loss: 14.2295, Entropy: 9.1049\n",
      "Episode 720/1000 - Min reward: -9.2566, Max reward: 0.9991, Mean reward: -0.1732\n",
      "Episode 720/1000 - Policy loss: -0.2906, Value loss: 5.5087, Entropy: 9.1142\n",
      "Episode 721/1000 - Min reward: -15.3736, Max reward: -0.0437, Mean reward: -3.5306\n",
      "Episode 721/1000 - Policy loss: -0.4264, Value loss: 10.8285, Entropy: 9.1185\n",
      "Episode 722/1000 - Min reward: -15.2011, Max reward: -2.5190, Mean reward: -7.5360\n",
      "Episode 722/1000 - Policy loss: -0.1924, Value loss: 9.1856, Entropy: 9.1054\n",
      "Episode 723/1000 - Min reward: -20.5036, Max reward: 0.9990, Mean reward: -0.7882\n",
      "Episode 723/1000 - Policy loss: -0.0754, Value loss: 3.3254, Entropy: 9.0993\n",
      "Episode 724/1000 - Min reward: 0.4062, Max reward: 0.9997, Mean reward: 0.9607\n",
      "Episode 724/1000 - Policy loss: -0.0483, Value loss: 8.8355, Entropy: 9.0973\n",
      "Episode 725/1000 - Min reward: -15.8630, Max reward: 0.1671, Mean reward: -3.6506\n",
      "Episode 725/1000 - Policy loss: 0.0854, Value loss: 40.7003, Entropy: 9.0938\n",
      "Episode 726/1000 - Min reward: -10.7633, Max reward: 0.9948, Mean reward: 0.0310\n",
      "Episode 726/1000 - Policy loss: 0.0278, Value loss: 5.8797, Entropy: 9.0921\n",
      "Episode 727/1000 - Min reward: -1.6957, Max reward: 1.0000, Mean reward: 0.8911\n",
      "Episode 727/1000 - Policy loss: 0.0882, Value loss: 8.1285, Entropy: 9.0846\n",
      "Episode 728/1000 - Min reward: -7.2776, Max reward: 0.9896, Mean reward: 0.5679\n",
      "Episode 728/1000 - Policy loss: -0.2825, Value loss: 3.9654, Entropy: 9.0906\n",
      "Episode 729/1000 - Min reward: -25.1624, Max reward: -4.0090, Mean reward: -9.5943\n",
      "Episode 729/1000 - Policy loss: -0.0355, Value loss: 11.2719, Entropy: 9.0935\n",
      "Episode 730/1000 - Min reward: -5.1863, Max reward: 0.9720, Mean reward: -1.7011\n",
      "Episode 730/1000 - Policy loss: 0.0632, Value loss: 78.1216, Entropy: 9.0958\n",
      "Episode 731/1000 - Min reward: -20.7604, Max reward: 0.1289, Mean reward: -3.4996\n",
      "Episode 731/1000 - Policy loss: 0.0759, Value loss: 12.6064, Entropy: 9.1004\n",
      "Episode 732/1000 - Min reward: -20.1811, Max reward: 0.3671, Mean reward: -5.0915\n",
      "Episode 732/1000 - Policy loss: -0.0773, Value loss: 19.5605, Entropy: 9.0976\n",
      "Episode 733/1000 - Min reward: -19.8930, Max reward: 0.9923, Mean reward: -3.1409\n",
      "Episode 733/1000 - Policy loss: -0.1593, Value loss: 4.3455, Entropy: 9.1013\n",
      "Episode 734/1000 - Min reward: -4.9184, Max reward: 0.9999, Mean reward: 0.5579\n",
      "Episode 734/1000 - Policy loss: -0.0143, Value loss: 7.1215, Entropy: 9.0972\n",
      "Episode 735/1000 - Min reward: -20.6326, Max reward: 0.2029, Mean reward: -3.5036\n",
      "Episode 735/1000 - Policy loss: -0.1661, Value loss: 16.5285, Entropy: 9.0866\n",
      "Episode 736/1000 - Min reward: -18.1055, Max reward: -3.1602, Mean reward: -7.4078\n",
      "Episode 736/1000 - Policy loss: -0.3447, Value loss: 11.3337, Entropy: 9.0669\n",
      "Episode 737/1000 - Min reward: -11.3457, Max reward: 0.9999, Mean reward: -0.0078\n",
      "Episode 737/1000 - Policy loss: 0.0859, Value loss: 5.2403, Entropy: 9.0420\n",
      "Episode 738/1000 - Min reward: -11.4105, Max reward: 0.1956, Mean reward: -1.6271\n",
      "Episode 738/1000 - Policy loss: -0.0378, Value loss: 8.4843, Entropy: 9.0354\n",
      "Episode 739/1000 - Min reward: -16.2579, Max reward: -1.8641, Mean reward: -7.8435\n",
      "Episode 739/1000 - Policy loss: -0.1272, Value loss: 26.4664, Entropy: 9.0375\n",
      "Episode 740/1000 - Min reward: -28.8108, Max reward: 0.8487, Mean reward: -3.8155\n",
      "Episode 740/1000 - Policy loss: -0.1145, Value loss: 3.1461, Entropy: 9.0468\n",
      "Episode 741/1000 - Min reward: -8.0958, Max reward: 0.9954, Mean reward: -0.3607\n",
      "Episode 741/1000 - Policy loss: 0.2198, Value loss: 7.2635, Entropy: 9.0438\n",
      "Episode 742/1000 - Min reward: -14.6278, Max reward: 0.5126, Mean reward: -2.5405\n",
      "Episode 742/1000 - Policy loss: 0.0383, Value loss: 7.9610, Entropy: 9.0385\n",
      "Episode 743/1000 - Min reward: -14.2625, Max reward: 0.9993, Mean reward: -1.0068\n",
      "Episode 743/1000 - Policy loss: 0.1478, Value loss: 4.7537, Entropy: 9.0357\n",
      "Episode 744/1000 - Min reward: -18.4749, Max reward: -5.0010, Mean reward: -8.6053\n",
      "Episode 744/1000 - Policy loss: 0.0384, Value loss: 7.1281, Entropy: 9.0257\n",
      "Episode 745/1000 - Min reward: -12.7897, Max reward: 0.9675, Mean reward: -3.9300\n",
      "Episode 745/1000 - Policy loss: -0.2900, Value loss: 65.5865, Entropy: 9.0180\n",
      "Episode 746/1000 - Min reward: -19.7483, Max reward: 0.9984, Mean reward: -1.7523\n",
      "Episode 746/1000 - Policy loss: -0.0838, Value loss: 6.3615, Entropy: 9.0122\n",
      "Episode 747/1000 - Min reward: -9.0065, Max reward: 1.0000, Mean reward: 0.3379\n",
      "Episode 747/1000 - Policy loss: -0.0475, Value loss: 5.2405, Entropy: 9.0107\n",
      "Episode 748/1000 - Min reward: -9.6965, Max reward: 0.9998, Mean reward: 0.4519\n",
      "Episode 748/1000 - Policy loss: -0.1385, Value loss: 2.3954, Entropy: 9.0138\n",
      "Episode 749/1000 - Min reward: -23.5030, Max reward: -0.0850, Mean reward: -6.1779\n",
      "Episode 749/1000 - Policy loss: -0.1318, Value loss: 6.2850, Entropy: 9.0209\n",
      "Episode 750/1000 - Min reward: -9.3125, Max reward: 0.9993, Mean reward: 0.1969\n",
      "Episode 750/1000 - Policy loss: 0.0839, Value loss: 6.3685, Entropy: 9.0183\n",
      "Episode 751/1000 - Min reward: -2.4926, Max reward: 0.9990, Mean reward: 0.8545\n",
      "Episode 751/1000 - Policy loss: 0.0941, Value loss: 8.9014, Entropy: 9.0519\n",
      "Episode 752/1000 - Min reward: -19.3428, Max reward: 0.6776, Mean reward: -2.8589\n",
      "Episode 752/1000 - Policy loss: -0.2305, Value loss: 3.1662, Entropy: 9.0883\n",
      "Episode 753/1000 - Min reward: -16.5495, Max reward: 0.9708, Mean reward: -2.1463\n",
      "Episode 753/1000 - Policy loss: -0.0899, Value loss: 2.6909, Entropy: 9.1075\n",
      "Episode 754/1000 - Min reward: -20.0852, Max reward: 0.9959, Mean reward: -1.8325\n",
      "Episode 754/1000 - Policy loss: -0.1119, Value loss: 2.5565, Entropy: 9.1097\n",
      "Episode 755/1000 - Min reward: -22.5088, Max reward: 0.9797, Mean reward: -2.3158\n",
      "Episode 755/1000 - Policy loss: -0.0751, Value loss: 2.1482, Entropy: 9.1004\n",
      "Episode 756/1000 - Min reward: -12.5373, Max reward: 0.2179, Mean reward: -3.3706\n",
      "Episode 756/1000 - Policy loss: -0.0172, Value loss: 42.9817, Entropy: 9.1075\n",
      "Episode 757/1000 - Min reward: -14.5023, Max reward: 0.9742, Mean reward: -0.7730\n",
      "Episode 757/1000 - Policy loss: 0.0582, Value loss: 19.1378, Entropy: 9.1208\n",
      "Episode 758/1000 - Min reward: -18.4041, Max reward: 0.9974, Mean reward: -1.5060\n",
      "Episode 758/1000 - Policy loss: -0.1010, Value loss: 6.1440, Entropy: 9.1256\n",
      "Episode 759/1000 - Min reward: -22.2327, Max reward: 0.9994, Mean reward: -3.2071\n",
      "Episode 759/1000 - Policy loss: -0.3518, Value loss: 2.5106, Entropy: 9.1220\n",
      "Episode 760/1000 - Min reward: -14.4062, Max reward: 0.7336, Mean reward: -2.4747\n",
      "Episode 760/1000 - Policy loss: 0.0562, Value loss: 16.0960, Entropy: 9.1204\n",
      "Episode 761/1000 - Min reward: -15.4170, Max reward: -0.5670, Mean reward: -3.7388\n",
      "Episode 761/1000 - Policy loss: -0.2065, Value loss: 6.3423, Entropy: 9.1283\n",
      "Episode 762/1000 - Min reward: -16.5800, Max reward: 0.9998, Mean reward: -0.4718\n",
      "Episode 762/1000 - Policy loss: 0.0124, Value loss: 6.2392, Entropy: 9.1263\n",
      "Episode 763/1000 - Min reward: -22.4569, Max reward: 0.6538, Mean reward: -1.4637\n",
      "Episode 763/1000 - Policy loss: -0.0901, Value loss: 3.9628, Entropy: 9.1087\n",
      "Episode 764/1000 - Min reward: -19.8138, Max reward: 0.9627, Mean reward: -1.2961\n",
      "Episode 764/1000 - Policy loss: -0.1617, Value loss: 7.5483, Entropy: 9.1088\n",
      "Episode 765/1000 - Min reward: -15.9781, Max reward: -0.3227, Mean reward: -4.5241\n",
      "Episode 765/1000 - Policy loss: -0.2229, Value loss: 6.8470, Entropy: 9.1085\n",
      "Episode 766/1000 - Min reward: -7.8027, Max reward: 0.9788, Mean reward: -0.4158\n",
      "Episode 766/1000 - Policy loss: 0.1428, Value loss: 4.0216, Entropy: 9.1054\n",
      "Episode 767/1000 - Min reward: -14.1314, Max reward: 0.4201, Mean reward: -2.1527\n",
      "Episode 767/1000 - Policy loss: -0.0251, Value loss: 12.9325, Entropy: 9.0928\n",
      "Episode 768/1000 - Min reward: -21.7599, Max reward: 0.2059, Mean reward: -3.1800\n",
      "Episode 768/1000 - Policy loss: -0.2296, Value loss: 19.6235, Entropy: 9.0849\n",
      "Episode 769/1000 - Min reward: -22.3658, Max reward: 0.9942, Mean reward: -1.7925\n",
      "Episode 769/1000 - Policy loss: 0.0691, Value loss: 5.0317, Entropy: 9.0786\n",
      "Episode 770/1000 - Min reward: -11.6292, Max reward: 0.9998, Mean reward: -0.8811\n",
      "Episode 770/1000 - Policy loss: -0.1492, Value loss: 19.3700, Entropy: 9.0784\n",
      "Episode 771/1000 - Min reward: -22.1305, Max reward: -4.2085, Mean reward: -7.5332\n",
      "Episode 771/1000 - Policy loss: -0.2705, Value loss: 3.5815, Entropy: 9.0728\n",
      "Episode 772/1000 - Min reward: -24.9610, Max reward: 0.9591, Mean reward: -2.2243\n",
      "Episode 772/1000 - Policy loss: 0.0452, Value loss: 3.7101, Entropy: 9.0724\n",
      "Episode 773/1000 - Min reward: -16.2615, Max reward: 0.3006, Mean reward: -2.0980\n",
      "Episode 773/1000 - Policy loss: -0.1893, Value loss: 4.0560, Entropy: 9.0792\n",
      "Episode 774/1000 - Min reward: -19.1233, Max reward: 0.8411, Mean reward: -1.5389\n",
      "Episode 774/1000 - Policy loss: -0.0471, Value loss: 13.4025, Entropy: 9.0825\n",
      "Episode 775/1000 - Min reward: -1.4750, Max reward: 0.9959, Mean reward: 0.2953\n",
      "Episode 775/1000 - Policy loss: -0.1802, Value loss: 84.0429, Entropy: 9.0793\n",
      "Episode 776/1000 - Min reward: -14.8998, Max reward: 0.9664, Mean reward: -1.5043\n",
      "Episode 776/1000 - Policy loss: -0.1402, Value loss: 11.1210, Entropy: 9.0853\n",
      "Episode 777/1000 - Min reward: -25.7038, Max reward: 0.1355, Mean reward: -4.5979\n",
      "Episode 777/1000 - Policy loss: -0.3440, Value loss: 7.6008, Entropy: 9.0764\n",
      "Episode 778/1000 - Min reward: -15.3853, Max reward: 0.3345, Mean reward: -2.1538\n",
      "Episode 778/1000 - Policy loss: -0.1977, Value loss: 2.5847, Entropy: 9.0685\n",
      "Episode 779/1000 - Min reward: -22.2930, Max reward: 0.9855, Mean reward: -5.5638\n",
      "Episode 779/1000 - Policy loss: -0.2330, Value loss: 3.9677, Entropy: 9.0602\n",
      "Episode 780/1000 - Min reward: -9.3362, Max reward: 0.7418, Mean reward: -1.3494\n",
      "Episode 780/1000 - Policy loss: -0.1120, Value loss: 17.9435, Entropy: 9.0477\n",
      "Episode 781/1000 - Min reward: -14.4561, Max reward: 0.1183, Mean reward: -5.7526\n",
      "Episode 781/1000 - Policy loss: -0.1204, Value loss: 8.1243, Entropy: 9.0416\n",
      "Episode 782/1000 - Min reward: -21.7547, Max reward: 0.0696, Mean reward: -4.6859\n",
      "Episode 782/1000 - Policy loss: -0.0630, Value loss: 17.1771, Entropy: 9.0474\n",
      "Episode 783/1000 - Min reward: -7.0166, Max reward: 1.0000, Mean reward: 0.5132\n",
      "Episode 783/1000 - Policy loss: 0.0325, Value loss: 3.3445, Entropy: 9.0560\n",
      "Episode 784/1000 - Min reward: -11.1896, Max reward: 0.9810, Mean reward: -0.6659\n",
      "Episode 784/1000 - Policy loss: -0.1721, Value loss: 4.8534, Entropy: 9.0430\n",
      "Episode 785/1000 - Min reward: -12.9863, Max reward: 0.9933, Mean reward: -0.6711\n",
      "Episode 785/1000 - Policy loss: -0.0663, Value loss: 8.6577, Entropy: 9.0207\n",
      "Episode 786/1000 - Min reward: -20.8568, Max reward: -1.2351, Mean reward: -4.8228\n",
      "Episode 786/1000 - Policy loss: -0.1026, Value loss: 4.0255, Entropy: 9.0069\n",
      "Episode 787/1000 - Min reward: -13.7982, Max reward: 0.9834, Mean reward: -0.6930\n",
      "Episode 787/1000 - Policy loss: -0.0729, Value loss: 18.2574, Entropy: 8.9960\n",
      "Episode 788/1000 - Min reward: -13.6110, Max reward: 0.9982, Mean reward: -0.0952\n",
      "Episode 788/1000 - Policy loss: -0.2252, Value loss: 7.6925, Entropy: 8.9976\n",
      "Episode 789/1000 - Min reward: -15.3597, Max reward: 0.9710, Mean reward: -0.6376\n",
      "Episode 789/1000 - Policy loss: 0.0186, Value loss: 3.9412, Entropy: 8.9924\n",
      "Episode 790/1000 - Min reward: -4.2520, Max reward: 0.9975, Mean reward: 0.5590\n",
      "Episode 790/1000 - Policy loss: -0.1366, Value loss: 2.3179, Entropy: 8.9863\n",
      "Episode 791/1000 - Min reward: -9.4046, Max reward: 0.9998, Mean reward: 0.5299\n",
      "Episode 791/1000 - Policy loss: -0.2172, Value loss: 7.5253, Entropy: 8.9789\n",
      "Episode 792/1000 - Min reward: -20.8627, Max reward: 0.5209, Mean reward: -2.3988\n",
      "Episode 792/1000 - Policy loss: -0.1322, Value loss: 8.4268, Entropy: 8.9663\n",
      "Episode 793/1000 - Min reward: -7.0486, Max reward: 0.9369, Mean reward: -1.3810\n",
      "Episode 793/1000 - Policy loss: -0.0382, Value loss: 26.2099, Entropy: 8.9677\n",
      "Episode 794/1000 - Min reward: -10.6536, Max reward: 0.9045, Mean reward: -1.1592\n",
      "Episode 794/1000 - Policy loss: -0.2301, Value loss: 4.6466, Entropy: 8.9673\n",
      "Episode 795/1000 - Min reward: -18.6558, Max reward: 0.9783, Mean reward: -1.6776\n",
      "Episode 795/1000 - Policy loss: -0.1060, Value loss: 12.0068, Entropy: 8.9821\n",
      "Episode 796/1000 - Min reward: -14.1017, Max reward: 0.9772, Mean reward: -0.3074\n",
      "Episode 796/1000 - Policy loss: -0.1265, Value loss: 13.5266, Entropy: 8.9914\n",
      "Episode 797/1000 - Min reward: -12.9227, Max reward: 0.9773, Mean reward: -0.7457\n",
      "Episode 797/1000 - Policy loss: 0.0210, Value loss: 16.4457, Entropy: 9.0000\n",
      "Episode 798/1000 - Min reward: -8.1488, Max reward: 0.9974, Mean reward: 0.0168\n",
      "Episode 798/1000 - Policy loss: -0.0486, Value loss: 6.1823, Entropy: 9.0036\n",
      "Episode 799/1000 - Min reward: -26.2168, Max reward: 0.9994, Mean reward: -2.3874\n",
      "Episode 799/1000 - Policy loss: 0.1940, Value loss: 10.6887, Entropy: 9.0015\n",
      "Episode 800/1000 - Min reward: -10.6158, Max reward: 1.0000, Mean reward: -0.1970\n",
      "Episode 800/1000 - Policy loss: 0.1211, Value loss: 7.5327, Entropy: 8.9996\n",
      "Episode 801/1000 - Min reward: -15.0023, Max reward: 0.9688, Mean reward: -3.3057\n",
      "Episode 801/1000 - Policy loss: 0.0232, Value loss: 17.0826, Entropy: 8.9943\n",
      "Episode 802/1000 - Min reward: -24.3343, Max reward: 0.9980, Mean reward: -3.2426\n",
      "Episode 802/1000 - Policy loss: 0.0138, Value loss: 9.8950, Entropy: 8.9814\n",
      "Episode 803/1000 - Min reward: -11.2619, Max reward: 0.9901, Mean reward: -2.0834\n",
      "Episode 803/1000 - Policy loss: -0.1668, Value loss: 18.6772, Entropy: 8.9831\n",
      "Episode 804/1000 - Min reward: -6.0440, Max reward: 0.9964, Mean reward: 0.3985\n",
      "Episode 804/1000 - Policy loss: -0.0581, Value loss: 4.9299, Entropy: 8.9842\n",
      "Episode 805/1000 - Min reward: -14.6798, Max reward: 0.9993, Mean reward: -0.2519\n",
      "Episode 805/1000 - Policy loss: 0.1152, Value loss: 4.0078, Entropy: 8.9725\n",
      "Episode 806/1000 - Min reward: -27.2883, Max reward: 1.0000, Mean reward: -3.4323\n",
      "Episode 806/1000 - Policy loss: 0.0185, Value loss: 8.5165, Entropy: 8.9664\n",
      "Episode 807/1000 - Min reward: -24.9033, Max reward: 0.8817, Mean reward: -5.5301\n",
      "Episode 807/1000 - Policy loss: 0.0100, Value loss: 4.8549, Entropy: 8.9634\n",
      "Episode 808/1000 - Min reward: -18.7687, Max reward: 0.9939, Mean reward: -1.3429\n",
      "Episode 808/1000 - Policy loss: -0.0389, Value loss: 7.1640, Entropy: 8.9597\n",
      "Episode 809/1000 - Min reward: -15.2210, Max reward: 0.9981, Mean reward: -1.5316\n",
      "Episode 809/1000 - Policy loss: -0.2069, Value loss: 3.2753, Entropy: 8.9726\n",
      "Episode 810/1000 - Min reward: -3.9295, Max reward: 0.9811, Mean reward: 0.4484\n",
      "Episode 810/1000 - Policy loss: -0.0641, Value loss: 12.6911, Entropy: 8.9787\n",
      "Episode 811/1000 - Min reward: -19.2720, Max reward: 0.9996, Mean reward: -1.5271\n",
      "Episode 811/1000 - Policy loss: 0.0441, Value loss: 2.1147, Entropy: 8.9715\n",
      "Episode 812/1000 - Min reward: -4.6339, Max reward: 0.9740, Mean reward: 0.6133\n",
      "Episode 812/1000 - Policy loss: -0.1625, Value loss: 4.5493, Entropy: 8.9582\n",
      "Episode 813/1000 - Min reward: -3.8070, Max reward: 0.9794, Mean reward: -0.2624\n",
      "Episode 813/1000 - Policy loss: -0.0392, Value loss: 40.4199, Entropy: 8.9547\n",
      "Episode 814/1000 - Min reward: -9.4369, Max reward: 0.9998, Mean reward: 0.1520\n",
      "Episode 814/1000 - Policy loss: -0.0130, Value loss: 4.5923, Entropy: 8.9619\n",
      "Episode 815/1000 - Min reward: 0.7237, Max reward: 0.9999, Mean reward: 0.9835\n",
      "Episode 815/1000 - Policy loss: 0.0047, Value loss: 7.0871, Entropy: 8.9724\n",
      "Episode 816/1000 - Min reward: -3.5869, Max reward: 0.8988, Mean reward: 0.0635\n",
      "Episode 816/1000 - Policy loss: -0.2815, Value loss: 12.1155, Entropy: 8.9819\n",
      "Episode 817/1000 - Min reward: -13.2471, Max reward: 0.9899, Mean reward: -0.5768\n",
      "Episode 817/1000 - Policy loss: 0.0144, Value loss: 12.1469, Entropy: 8.9765\n",
      "Episode 818/1000 - Min reward: -2.6629, Max reward: 0.9997, Mean reward: 0.7051\n",
      "Episode 818/1000 - Policy loss: 0.1567, Value loss: 4.8198, Entropy: 8.9669\n",
      "Episode 819/1000 - Min reward: -13.1119, Max reward: 0.9928, Mean reward: -1.6593\n",
      "Episode 819/1000 - Policy loss: 0.0779, Value loss: 20.9906, Entropy: 8.9584\n",
      "Episode 820/1000 - Min reward: -14.5014, Max reward: 0.9996, Mean reward: -0.0688\n",
      "Episode 820/1000 - Policy loss: 0.0233, Value loss: 15.1833, Entropy: 8.9547\n",
      "Episode 821/1000 - Min reward: -16.1574, Max reward: 0.9862, Mean reward: -3.5658\n",
      "Episode 821/1000 - Policy loss: -0.1311, Value loss: 19.3163, Entropy: 8.9577\n",
      "Episode 822/1000 - Min reward: -18.2685, Max reward: 0.9964, Mean reward: -2.8863\n",
      "Episode 822/1000 - Policy loss: -0.0299, Value loss: 3.9345, Entropy: 8.9564\n",
      "Episode 823/1000 - Min reward: -25.6457, Max reward: 0.9992, Mean reward: -2.7518\n",
      "Episode 823/1000 - Policy loss: -0.0152, Value loss: 6.0770, Entropy: 8.9574\n",
      "Episode 824/1000 - Min reward: -0.0771, Max reward: 0.8099, Mean reward: 0.3511\n",
      "Episode 824/1000 - Policy loss: -0.1170, Value loss: 19.1792, Entropy: 8.9602\n",
      "Episode 825/1000 - Min reward: -12.4732, Max reward: 0.9802, Mean reward: 0.0744\n",
      "Episode 825/1000 - Policy loss: 0.0350, Value loss: 1.7373, Entropy: 8.9563\n",
      "Episode 826/1000 - Min reward: -17.6241, Max reward: 0.9999, Mean reward: -1.1365\n",
      "Episode 826/1000 - Policy loss: -0.1231, Value loss: 0.6970, Entropy: 8.9551\n",
      "Episode 827/1000 - Min reward: -10.5280, Max reward: 0.9617, Mean reward: -0.3549\n",
      "Episode 827/1000 - Policy loss: -0.0632, Value loss: 2.4697, Entropy: 8.9613\n",
      "Episode 828/1000 - Min reward: -19.3148, Max reward: 0.9846, Mean reward: -1.3394\n",
      "Episode 828/1000 - Policy loss: -0.3476, Value loss: 6.7509, Entropy: 8.9678\n",
      "Episode 829/1000 - Min reward: -20.9882, Max reward: 0.7912, Mean reward: -2.3223\n",
      "Episode 829/1000 - Policy loss: -0.2175, Value loss: 3.9059, Entropy: 8.9769\n",
      "Episode 830/1000 - Min reward: -19.2495, Max reward: 0.9807, Mean reward: -3.4955\n",
      "Episode 830/1000 - Policy loss: -0.2806, Value loss: 1.1872, Entropy: 8.9930\n",
      "Episode 831/1000 - Min reward: -24.1096, Max reward: 1.0000, Mean reward: -1.6894\n",
      "Episode 831/1000 - Policy loss: -0.1737, Value loss: 9.1270, Entropy: 8.9937\n",
      "Episode 832/1000 - Min reward: -14.1096, Max reward: 0.9941, Mean reward: -0.5295\n",
      "Episode 832/1000 - Policy loss: -0.2465, Value loss: 4.3832, Entropy: 8.9986\n",
      "Episode 833/1000 - Min reward: -5.8992, Max reward: 0.9998, Mean reward: 0.4604\n",
      "Episode 833/1000 - Policy loss: 0.0249, Value loss: 13.5615, Entropy: 9.0018\n",
      "Episode 834/1000 - Min reward: 0.0909, Max reward: 0.9996, Mean reward: 0.9160\n",
      "Episode 834/1000 - Policy loss: -0.1521, Value loss: 11.8255, Entropy: 9.0151\n",
      "Episode 835/1000 - Min reward: -17.8213, Max reward: 1.0000, Mean reward: -1.0518\n",
      "Episode 835/1000 - Policy loss: -0.1138, Value loss: 7.7442, Entropy: 9.0063\n",
      "Episode 836/1000 - Min reward: -30.5735, Max reward: 0.9983, Mean reward: -5.1266\n",
      "Episode 836/1000 - Policy loss: -0.1772, Value loss: 4.6139, Entropy: 8.9927\n",
      "Episode 837/1000 - Min reward: -12.5854, Max reward: 0.9971, Mean reward: -1.4820\n",
      "Episode 837/1000 - Policy loss: -0.1306, Value loss: 5.6512, Entropy: 8.9883\n",
      "Episode 838/1000 - Min reward: -14.6063, Max reward: 0.9550, Mean reward: -1.5069\n",
      "Episode 838/1000 - Policy loss: 0.0438, Value loss: 9.3472, Entropy: 8.9801\n",
      "Episode 839/1000 - Min reward: -21.5079, Max reward: 0.9847, Mean reward: -2.4359\n",
      "Episode 839/1000 - Policy loss: 0.0034, Value loss: 11.0469, Entropy: 8.9697\n",
      "Episode 840/1000 - Min reward: -14.2006, Max reward: 0.9969, Mean reward: -1.0789\n",
      "Episode 840/1000 - Policy loss: -0.1454, Value loss: 3.1018, Entropy: 8.9633\n",
      "Episode 841/1000 - Min reward: -4.8760, Max reward: 0.9992, Mean reward: -0.7216\n",
      "Episode 841/1000 - Policy loss: -0.1180, Value loss: 20.7693, Entropy: 8.9644\n",
      "Episode 842/1000 - Min reward: -11.9477, Max reward: 0.9998, Mean reward: -1.9165\n",
      "Episode 842/1000 - Policy loss: -0.2232, Value loss: 20.5642, Entropy: 8.9727\n",
      "Episode 843/1000 - Min reward: -15.7655, Max reward: 1.0000, Mean reward: -0.2618\n",
      "Episode 843/1000 - Policy loss: 0.0870, Value loss: 10.3626, Entropy: 8.9790\n",
      "Episode 844/1000 - Min reward: -4.9675, Max reward: 0.9992, Mean reward: 0.1005\n",
      "Episode 844/1000 - Policy loss: -0.0618, Value loss: 16.0955, Entropy: 8.9759\n",
      "Episode 845/1000 - Min reward: -16.7103, Max reward: 0.9908, Mean reward: -1.6534\n",
      "Episode 845/1000 - Policy loss: -0.4436, Value loss: 20.2023, Entropy: 8.9692\n",
      "Episode 846/1000 - Min reward: -17.3268, Max reward: 0.9974, Mean reward: -0.9058\n",
      "Episode 846/1000 - Policy loss: -0.0799, Value loss: 2.3241, Entropy: 8.9704\n",
      "Episode 847/1000 - Min reward: -19.8799, Max reward: 0.9959, Mean reward: -1.6825\n",
      "Episode 847/1000 - Policy loss: -0.0983, Value loss: 15.9241, Entropy: 8.9801\n",
      "Episode 848/1000 - Min reward: -3.7470, Max reward: 0.9995, Mean reward: 0.3787\n",
      "Episode 848/1000 - Policy loss: -0.0720, Value loss: 9.1454, Entropy: 8.9809\n",
      "Episode 849/1000 - Min reward: -23.4343, Max reward: 0.9998, Mean reward: -2.2325\n",
      "Episode 849/1000 - Policy loss: 0.0434, Value loss: 6.1805, Entropy: 8.9748\n",
      "Episode 850/1000 - Min reward: -12.2604, Max reward: 1.0000, Mean reward: 0.0524\n",
      "Episode 850/1000 - Policy loss: -0.2246, Value loss: 6.8847, Entropy: 8.9619\n",
      "Episode 851/1000 - Min reward: -9.2359, Max reward: 0.9629, Mean reward: 0.0546\n",
      "Episode 851/1000 - Policy loss: -0.1118, Value loss: 4.2840, Entropy: 8.9618\n",
      "Episode 852/1000 - Min reward: -16.3543, Max reward: 0.9995, Mean reward: -0.6486\n",
      "Episode 852/1000 - Policy loss: 0.0272, Value loss: 5.7390, Entropy: 8.9717\n",
      "Episode 853/1000 - Min reward: -22.6580, Max reward: 0.9904, Mean reward: -1.4765\n",
      "Episode 853/1000 - Policy loss: -0.0382, Value loss: 5.8354, Entropy: 8.9699\n",
      "Episode 854/1000 - Min reward: -12.1937, Max reward: 1.0000, Mean reward: 0.3663\n",
      "Episode 854/1000 - Policy loss: 0.1848, Value loss: 1.8485, Entropy: 8.9680\n",
      "Episode 855/1000 - Min reward: -9.7459, Max reward: 1.0000, Mean reward: 0.4552\n",
      "Episode 855/1000 - Policy loss: -0.2155, Value loss: 2.2244, Entropy: 8.9726\n",
      "Episode 856/1000 - Min reward: -24.3083, Max reward: 0.2289, Mean reward: -5.6325\n",
      "Episode 856/1000 - Policy loss: -0.1497, Value loss: 2.0894, Entropy: 8.9822\n",
      "Episode 857/1000 - Min reward: -5.1501, Max reward: 0.8579, Mean reward: -0.0007\n",
      "Episode 857/1000 - Policy loss: -0.0446, Value loss: 15.6820, Entropy: 8.9933\n",
      "Episode 858/1000 - Min reward: -15.8386, Max reward: 0.9969, Mean reward: -2.6080\n",
      "Episode 858/1000 - Policy loss: -0.1836, Value loss: 6.5293, Entropy: 8.9938\n",
      "Episode 859/1000 - Min reward: -11.7666, Max reward: 0.9995, Mean reward: 0.2748\n",
      "Episode 859/1000 - Policy loss: -0.3318, Value loss: 2.2542, Entropy: 8.9914\n",
      "Episode 860/1000 - Min reward: -4.0831, Max reward: 0.3234, Mean reward: -1.7061\n",
      "Episode 860/1000 - Policy loss: -0.3341, Value loss: 43.7982, Entropy: 8.9915\n",
      "Episode 861/1000 - Min reward: -13.3251, Max reward: 0.1079, Mean reward: -2.9988\n",
      "Episode 861/1000 - Policy loss: -0.1389, Value loss: 8.6572, Entropy: 8.9971\n",
      "Episode 862/1000 - Min reward: -7.3346, Max reward: 0.9998, Mean reward: 0.3551\n",
      "Episode 862/1000 - Policy loss: 0.0451, Value loss: 1.3826, Entropy: 8.9954\n",
      "Episode 863/1000 - Min reward: -13.1401, Max reward: 0.7984, Mean reward: -0.4554\n",
      "Episode 863/1000 - Policy loss: -0.1855, Value loss: 8.7959, Entropy: 8.9823\n",
      "Episode 864/1000 - Min reward: -8.1211, Max reward: 0.8798, Mean reward: -0.3320\n",
      "Episode 864/1000 - Policy loss: 0.0531, Value loss: 9.1510, Entropy: 8.9724\n",
      "Episode 865/1000 - Min reward: -1.0520, Max reward: 0.9891, Mean reward: 0.6383\n",
      "Episode 865/1000 - Policy loss: -0.0108, Value loss: 10.5212, Entropy: 8.9658\n",
      "Episode 866/1000 - Min reward: -9.3908, Max reward: 0.9997, Mean reward: 0.2590\n",
      "Episode 866/1000 - Policy loss: -0.1263, Value loss: 1.5612, Entropy: 8.9667\n",
      "Episode 867/1000 - Min reward: -17.5179, Max reward: 0.9663, Mean reward: -1.0922\n",
      "Episode 867/1000 - Policy loss: -0.3394, Value loss: 2.0655, Entropy: 8.9700\n",
      "Episode 868/1000 - Min reward: -13.1811, Max reward: 0.9768, Mean reward: -1.9582\n",
      "Episode 868/1000 - Policy loss: -0.0651, Value loss: 10.0757, Entropy: 8.9797\n",
      "Episode 869/1000 - Min reward: -3.3037, Max reward: 0.9999, Mean reward: 0.8537\n",
      "Episode 869/1000 - Policy loss: -0.2414, Value loss: 3.2503, Entropy: 8.9909\n",
      "Episode 870/1000 - Min reward: -7.9126, Max reward: 0.9950, Mean reward: 0.2274\n",
      "Episode 870/1000 - Policy loss: -0.0009, Value loss: 3.9843, Entropy: 9.0014\n",
      "Episode 871/1000 - Min reward: -17.0688, Max reward: 0.6533, Mean reward: -3.1487\n",
      "Episode 871/1000 - Policy loss: 0.0217, Value loss: 8.7401, Entropy: 9.0072\n",
      "Episode 872/1000 - Min reward: -12.5514, Max reward: 0.9766, Mean reward: -1.9133\n",
      "Episode 872/1000 - Policy loss: -0.2211, Value loss: 2.6378, Entropy: 9.0112\n",
      "Episode 873/1000 - Min reward: -11.4144, Max reward: 0.9915, Mean reward: 0.0050\n",
      "Episode 873/1000 - Policy loss: -0.0342, Value loss: 4.8764, Entropy: 9.0036\n",
      "Episode 874/1000 - Min reward: -11.6425, Max reward: 0.9861, Mean reward: -0.2800\n",
      "Episode 874/1000 - Policy loss: -0.0079, Value loss: 1.0208, Entropy: 8.9964\n",
      "Episode 875/1000 - Min reward: -18.8814, Max reward: 0.9960, Mean reward: -0.8430\n",
      "Episode 875/1000 - Policy loss: -0.0975, Value loss: 5.4114, Entropy: 8.9955\n",
      "Episode 876/1000 - Min reward: -17.4954, Max reward: 0.6036, Mean reward: -2.7695\n",
      "Episode 876/1000 - Policy loss: -0.1729, Value loss: 16.8569, Entropy: 9.0019\n",
      "Episode 877/1000 - Min reward: -14.6028, Max reward: 0.9670, Mean reward: -1.3786\n",
      "Episode 877/1000 - Policy loss: -0.0674, Value loss: 6.3651, Entropy: 9.0061\n",
      "Episode 878/1000 - Min reward: -18.0641, Max reward: 0.4254, Mean reward: -3.8384\n",
      "Episode 878/1000 - Policy loss: -0.1335, Value loss: 6.0757, Entropy: 9.0043\n",
      "Episode 879/1000 - Min reward: -18.2875, Max reward: 0.8970, Mean reward: -1.9382\n",
      "Episode 879/1000 - Policy loss: -0.2246, Value loss: 4.1308, Entropy: 9.0003\n",
      "Episode 880/1000 - Min reward: -13.2704, Max reward: 0.9138, Mean reward: -0.7294\n",
      "Episode 880/1000 - Policy loss: -0.1939, Value loss: 5.0840, Entropy: 8.9946\n",
      "Episode 881/1000 - Min reward: -14.2164, Max reward: 0.9730, Mean reward: -1.0402\n",
      "Episode 881/1000 - Policy loss: -0.0830, Value loss: 2.3743, Entropy: 8.9942\n",
      "Episode 882/1000 - Min reward: -18.4908, Max reward: 0.9999, Mean reward: -1.0023\n",
      "Episode 882/1000 - Policy loss: -0.3140, Value loss: 1.3181, Entropy: 9.0032\n",
      "Episode 883/1000 - Min reward: -17.5448, Max reward: -3.7360, Mean reward: -7.0301\n",
      "Episode 883/1000 - Policy loss: -0.0758, Value loss: 18.2090, Entropy: 9.0016\n",
      "Episode 884/1000 - Min reward: -8.6385, Max reward: 0.8937, Mean reward: -3.2735\n",
      "Episode 884/1000 - Policy loss: -0.4556, Value loss: 52.4794, Entropy: 8.9997\n",
      "Episode 885/1000 - Min reward: 0.2873, Max reward: 0.9999, Mean reward: 0.7973\n",
      "Episode 885/1000 - Policy loss: 0.1105, Value loss: 56.7275, Entropy: 8.9942\n",
      "Episode 886/1000 - Min reward: -17.4028, Max reward: 0.1173, Mean reward: -4.0297\n",
      "Episode 886/1000 - Policy loss: -0.0573, Value loss: 7.5887, Entropy: 8.9874\n",
      "Episode 887/1000 - Min reward: -20.0687, Max reward: 0.9782, Mean reward: -1.5352\n",
      "Episode 887/1000 - Policy loss: -0.1862, Value loss: 59.1180, Entropy: 8.9867\n",
      "Episode 888/1000 - Min reward: -15.8070, Max reward: 0.1780, Mean reward: -2.4745\n",
      "Episode 888/1000 - Policy loss: -0.1985, Value loss: 11.8152, Entropy: 8.9862\n",
      "Episode 889/1000 - Min reward: -16.7059, Max reward: 0.9204, Mean reward: -0.9624\n",
      "Episode 889/1000 - Policy loss: -0.2233, Value loss: 5.5742, Entropy: 8.9868\n",
      "Episode 890/1000 - Min reward: -14.2234, Max reward: 0.9855, Mean reward: -0.5225\n",
      "Episode 890/1000 - Policy loss: -0.0343, Value loss: 2.5753, Entropy: 8.9803\n",
      "Episode 891/1000 - Min reward: -18.0361, Max reward: -5.9466, Mean reward: -9.9142\n",
      "Episode 891/1000 - Policy loss: -0.0328, Value loss: 14.1166, Entropy: 8.9782\n",
      "Episode 892/1000 - Min reward: -26.8326, Max reward: 0.0137, Mean reward: -5.4343\n",
      "Episode 892/1000 - Policy loss: 0.0274, Value loss: 13.3721, Entropy: 8.9748\n",
      "Episode 893/1000 - Min reward: -20.3070, Max reward: 0.6666, Mean reward: -2.5835\n",
      "Episode 893/1000 - Policy loss: 0.2226, Value loss: 4.9807, Entropy: 8.9696\n",
      "Episode 894/1000 - Min reward: -21.3906, Max reward: 0.6397, Mean reward: -2.3825\n",
      "Episode 894/1000 - Policy loss: -0.2859, Value loss: 4.7131, Entropy: 8.9747\n",
      "Episode 895/1000 - Min reward: -1.4736, Max reward: 0.9987, Mean reward: 0.5052\n",
      "Episode 895/1000 - Policy loss: -0.2180, Value loss: 125.5984, Entropy: 8.9669\n",
      "Episode 896/1000 - Min reward: -20.4978, Max reward: -0.0734, Mean reward: -4.7976\n",
      "Episode 896/1000 - Policy loss: -0.0129, Value loss: 1.2141, Entropy: 8.9642\n",
      "Episode 897/1000 - Min reward: 0.2198, Max reward: 0.9541, Mean reward: 0.7350\n",
      "Episode 897/1000 - Policy loss: -0.0107, Value loss: 10.3631, Entropy: 8.9744\n",
      "Episode 898/1000 - Min reward: -10.8265, Max reward: 0.9986, Mean reward: 0.0015\n",
      "Episode 898/1000 - Policy loss: -0.0869, Value loss: 2.1476, Entropy: 8.9778\n",
      "Episode 899/1000 - Min reward: -8.3039, Max reward: -0.6795, Mean reward: -2.5625\n",
      "Episode 899/1000 - Policy loss: -0.0059, Value loss: 17.6146, Entropy: 8.9749\n",
      "Episode 900/1000 - Min reward: -10.8073, Max reward: 0.9852, Mean reward: -0.5543\n",
      "Episode 900/1000 - Policy loss: -0.1254, Value loss: 14.2143, Entropy: 8.9652\n",
      "Episode 901/1000 - Min reward: -9.3835, Max reward: 0.9100, Mean reward: 0.1238\n",
      "Episode 901/1000 - Policy loss: -0.0354, Value loss: 0.9243, Entropy: 8.9726\n",
      "Episode 902/1000 - Min reward: -22.0725, Max reward: -0.8440, Mean reward: -4.2575\n",
      "Episode 902/1000 - Policy loss: -0.2348, Value loss: 10.5503, Entropy: 8.9719\n",
      "Episode 903/1000 - Min reward: -9.6738, Max reward: 0.9671, Mean reward: 0.0947\n",
      "Episode 903/1000 - Policy loss: -0.0294, Value loss: 7.1618, Entropy: 8.9783\n",
      "Episode 904/1000 - Min reward: -14.0677, Max reward: 0.7096, Mean reward: -1.2205\n",
      "Episode 904/1000 - Policy loss: 0.1185, Value loss: 7.4589, Entropy: 8.9821\n",
      "Episode 905/1000 - Min reward: -11.4636, Max reward: 0.9908, Mean reward: -0.6419\n",
      "Episode 905/1000 - Policy loss: -0.2776, Value loss: 9.5884, Entropy: 8.9887\n",
      "Episode 906/1000 - Min reward: -12.6572, Max reward: 0.1238, Mean reward: -2.5666\n",
      "Episode 906/1000 - Policy loss: -0.1140, Value loss: 2.2842, Entropy: 8.9966\n",
      "Episode 907/1000 - Min reward: -12.2237, Max reward: 0.5495, Mean reward: -1.8528\n",
      "Episode 907/1000 - Policy loss: -0.1013, Value loss: 7.9028, Entropy: 8.9933\n",
      "Episode 908/1000 - Min reward: -7.2506, Max reward: 0.9906, Mean reward: -0.0695\n",
      "Episode 908/1000 - Policy loss: 0.0163, Value loss: 16.8500, Entropy: 8.9984\n",
      "Episode 909/1000 - Min reward: -19.3232, Max reward: 0.4558, Mean reward: -2.5620\n",
      "Episode 909/1000 - Policy loss: 0.0577, Value loss: 16.3825, Entropy: 9.0046\n",
      "Episode 910/1000 - Min reward: -12.6879, Max reward: 0.2922, Mean reward: -2.2592\n",
      "Episode 910/1000 - Policy loss: -0.2167, Value loss: 11.7432, Entropy: 9.0114\n",
      "Episode 911/1000 - Min reward: -23.9914, Max reward: 0.2120, Mean reward: -6.4858\n",
      "Episode 911/1000 - Policy loss: -0.1087, Value loss: 12.0248, Entropy: 9.0114\n",
      "Episode 912/1000 - Min reward: -11.4492, Max reward: 0.8494, Mean reward: -0.8911\n",
      "Episode 912/1000 - Policy loss: -0.0220, Value loss: 11.7515, Entropy: 8.9961\n",
      "Episode 913/1000 - Min reward: -19.6937, Max reward: 0.9984, Mean reward: -2.2516\n",
      "Episode 913/1000 - Policy loss: -0.1740, Value loss: 13.1605, Entropy: 8.9778\n",
      "Episode 914/1000 - Min reward: -14.5465, Max reward: 0.9777, Mean reward: -0.5697\n",
      "Episode 914/1000 - Policy loss: -0.1854, Value loss: 4.6544, Entropy: 8.9639\n",
      "Episode 915/1000 - Min reward: -1.8106, Max reward: 1.0000, Mean reward: 0.8252\n",
      "Episode 915/1000 - Policy loss: -0.0601, Value loss: 23.3900, Entropy: 8.9486\n",
      "Episode 916/1000 - Min reward: -14.7683, Max reward: -3.1784, Mean reward: -7.4890\n",
      "Episode 916/1000 - Policy loss: -0.0610, Value loss: 6.1934, Entropy: 8.9423\n",
      "Episode 917/1000 - Min reward: -24.9183, Max reward: 0.9995, Mean reward: -1.4861\n",
      "Episode 917/1000 - Policy loss: -0.0445, Value loss: 9.1526, Entropy: 8.9429\n",
      "Episode 918/1000 - Min reward: -5.0985, Max reward: 0.9391, Mean reward: 0.2980\n",
      "Episode 918/1000 - Policy loss: -0.2067, Value loss: 10.5150, Entropy: 8.9458\n",
      "Episode 919/1000 - Min reward: -6.1535, Max reward: 0.9975, Mean reward: 0.6539\n",
      "Episode 919/1000 - Policy loss: -0.0964, Value loss: 5.4821, Entropy: 8.9503\n",
      "Episode 920/1000 - Min reward: -3.4458, Max reward: 0.9995, Mean reward: 0.7151\n",
      "Episode 920/1000 - Policy loss: -0.0499, Value loss: 3.5937, Entropy: 8.9569\n",
      "Episode 921/1000 - Min reward: -16.4842, Max reward: 0.9859, Mean reward: -1.1139\n",
      "Episode 921/1000 - Policy loss: -0.1771, Value loss: 7.4278, Entropy: 8.9627\n",
      "Episode 922/1000 - Min reward: -5.8661, Max reward: 0.9985, Mean reward: 0.4506\n",
      "Episode 922/1000 - Policy loss: -0.1420, Value loss: 5.5010, Entropy: 8.9647\n",
      "Episode 923/1000 - Min reward: -14.8118, Max reward: 0.9665, Mean reward: -0.3261\n",
      "Episode 923/1000 - Policy loss: 0.0142, Value loss: 5.3166, Entropy: 8.9609\n",
      "Episode 924/1000 - Min reward: -4.2208, Max reward: 0.9917, Mean reward: 0.6641\n",
      "Episode 924/1000 - Policy loss: -0.0779, Value loss: 8.8059, Entropy: 8.9675\n",
      "Episode 925/1000 - Min reward: -14.8395, Max reward: 1.0000, Mean reward: -0.7479\n",
      "Episode 925/1000 - Policy loss: 0.1487, Value loss: 10.0932, Entropy: 8.9739\n",
      "Episode 926/1000 - Min reward: -13.5071, Max reward: 0.9765, Mean reward: -0.5697\n",
      "Episode 926/1000 - Policy loss: -0.2090, Value loss: 4.1088, Entropy: 8.9701\n",
      "Episode 927/1000 - Min reward: -13.5352, Max reward: 0.9586, Mean reward: -0.5868\n",
      "Episode 927/1000 - Policy loss: -0.3787, Value loss: 2.1349, Entropy: 8.9569\n",
      "Episode 928/1000 - Min reward: -14.2311, Max reward: 0.9572, Mean reward: -1.7799\n",
      "Episode 928/1000 - Policy loss: -0.1810, Value loss: 10.0821, Entropy: 8.9434\n",
      "Episode 929/1000 - Min reward: -13.0033, Max reward: -2.0959, Mean reward: -6.5829\n",
      "Episode 929/1000 - Policy loss: -0.1208, Value loss: 5.3013, Entropy: 8.9331\n",
      "Episode 930/1000 - Min reward: -10.6117, Max reward: 0.9010, Mean reward: -0.1489\n",
      "Episode 930/1000 - Policy loss: -0.4187, Value loss: 9.8704, Entropy: 8.9268\n",
      "Episode 931/1000 - Min reward: -19.3974, Max reward: 0.9229, Mean reward: -2.0646\n",
      "Episode 931/1000 - Policy loss: -0.1193, Value loss: 7.3785, Entropy: 8.9322\n",
      "Episode 932/1000 - Min reward: -15.9156, Max reward: 0.9595, Mean reward: -1.3137\n",
      "Episode 932/1000 - Policy loss: -0.1488, Value loss: 12.6115, Entropy: 8.9313\n",
      "Episode 933/1000 - Min reward: -23.8104, Max reward: 0.9840, Mean reward: -2.2087\n",
      "Episode 933/1000 - Policy loss: -0.2376, Value loss: 1.9439, Entropy: 8.9274\n",
      "Episode 934/1000 - Min reward: -24.4392, Max reward: 0.9900, Mean reward: -1.3684\n",
      "Episode 934/1000 - Policy loss: -0.3130, Value loss: 4.2270, Entropy: 8.9185\n",
      "Episode 935/1000 - Min reward: -22.5218, Max reward: 0.9508, Mean reward: -3.6920\n",
      "Episode 935/1000 - Policy loss: -0.1216, Value loss: 11.5762, Entropy: 8.9105\n",
      "Episode 936/1000 - Min reward: -22.6730, Max reward: 0.9812, Mean reward: -2.2427\n",
      "Episode 936/1000 - Policy loss: -0.1815, Value loss: 7.1901, Entropy: 8.9097\n",
      "Episode 937/1000 - Min reward: -15.5370, Max reward: 1.0000, Mean reward: -0.9335\n",
      "Episode 937/1000 - Policy loss: -0.1838, Value loss: 7.4182, Entropy: 8.9116\n",
      "Episode 938/1000 - Min reward: -17.9345, Max reward: 0.7606, Mean reward: -1.2547\n",
      "Episode 938/1000 - Policy loss: -0.2074, Value loss: 6.2332, Entropy: 8.9121\n",
      "Episode 939/1000 - Min reward: -14.6730, Max reward: 0.9994, Mean reward: -1.5579\n",
      "Episode 939/1000 - Policy loss: -0.2026, Value loss: 10.4932, Entropy: 8.9097\n",
      "Episode 940/1000 - Min reward: -11.0501, Max reward: 0.9865, Mean reward: -0.0771\n",
      "Episode 940/1000 - Policy loss: -0.2977, Value loss: 6.5383, Entropy: 8.9061\n",
      "Episode 941/1000 - Min reward: -15.3407, Max reward: 0.9754, Mean reward: -1.0317\n",
      "Episode 941/1000 - Policy loss: -0.0787, Value loss: 5.6742, Entropy: 8.9070\n",
      "Episode 942/1000 - Min reward: -22.2383, Max reward: 0.9884, Mean reward: -1.1828\n",
      "Episode 942/1000 - Policy loss: -0.1939, Value loss: 12.0200, Entropy: 8.9066\n",
      "Episode 943/1000 - Min reward: -15.8991, Max reward: 0.8296, Mean reward: -2.8163\n",
      "Episode 943/1000 - Policy loss: -0.0920, Value loss: 14.7975, Entropy: 8.9017\n",
      "Episode 944/1000 - Min reward: -16.2766, Max reward: 0.8452, Mean reward: -1.4297\n",
      "Episode 944/1000 - Policy loss: 0.0413, Value loss: 4.9304, Entropy: 8.8987\n",
      "Episode 945/1000 - Min reward: -18.0095, Max reward: 0.9672, Mean reward: -1.2836\n",
      "Episode 945/1000 - Policy loss: -0.2068, Value loss: 6.2068, Entropy: 8.9038\n",
      "Episode 946/1000 - Min reward: -23.4474, Max reward: 1.0000, Mean reward: -1.4651\n",
      "Episode 946/1000 - Policy loss: -0.0386, Value loss: 16.7500, Entropy: 8.9006\n",
      "Episode 947/1000 - Min reward: -14.0046, Max reward: -0.7088, Mean reward: -4.6414\n",
      "Episode 947/1000 - Policy loss: 0.0285, Value loss: 14.3731, Entropy: 8.9082\n",
      "Episode 948/1000 - Min reward: -15.1410, Max reward: 0.9560, Mean reward: -1.0808\n",
      "Episode 948/1000 - Policy loss: 0.0357, Value loss: 12.0722, Entropy: 8.9172\n",
      "Episode 949/1000 - Min reward: -24.2710, Max reward: 0.9661, Mean reward: -5.1076\n",
      "Episode 949/1000 - Policy loss: -0.0193, Value loss: 10.2288, Entropy: 8.9181\n",
      "Episode 950/1000 - Min reward: -22.2214, Max reward: 0.9090, Mean reward: -1.6620\n",
      "Episode 950/1000 - Policy loss: -0.1883, Value loss: 14.9576, Entropy: 8.9229\n",
      "Episode 951/1000 - Min reward: -19.8890, Max reward: 0.9994, Mean reward: -1.1377\n",
      "Episode 951/1000 - Policy loss: -0.2815, Value loss: 6.4550, Entropy: 8.9357\n",
      "Episode 952/1000 - Min reward: -20.3117, Max reward: 0.9812, Mean reward: -1.8448\n",
      "Episode 952/1000 - Policy loss: -0.1623, Value loss: 13.7310, Entropy: 8.9351\n",
      "Episode 953/1000 - Min reward: -12.0855, Max reward: 0.9918, Mean reward: -0.9543\n",
      "Episode 953/1000 - Policy loss: -0.1069, Value loss: 5.8459, Entropy: 8.9301\n",
      "Episode 954/1000 - Min reward: -23.0973, Max reward: 0.9035, Mean reward: -3.0007\n",
      "Episode 954/1000 - Policy loss: -0.0562, Value loss: 4.0833, Entropy: 8.9277\n",
      "Episode 955/1000 - Min reward: -12.5728, Max reward: 0.9241, Mean reward: -1.4525\n",
      "Episode 955/1000 - Policy loss: -0.1334, Value loss: 2.7486, Entropy: 8.9180\n",
      "Episode 956/1000 - Min reward: -12.2908, Max reward: 0.9897, Mean reward: -1.0372\n",
      "Episode 956/1000 - Policy loss: -0.1521, Value loss: 7.3175, Entropy: 8.9012\n",
      "Episode 957/1000 - Min reward: -5.2587, Max reward: 0.9988, Mean reward: 0.3417\n",
      "Episode 957/1000 - Policy loss: -0.0387, Value loss: 9.8828, Entropy: 8.8923\n",
      "Episode 958/1000 - Min reward: -14.8432, Max reward: 0.9977, Mean reward: -0.2766\n",
      "Episode 958/1000 - Policy loss: -0.2171, Value loss: 11.5800, Entropy: 8.9021\n",
      "Episode 959/1000 - Min reward: -21.7628, Max reward: 0.9791, Mean reward: -3.8674\n",
      "Episode 959/1000 - Policy loss: -0.0612, Value loss: 2.2918, Entropy: 8.9169\n",
      "Episode 960/1000 - Min reward: -12.8038, Max reward: 0.9898, Mean reward: -0.0217\n",
      "Episode 960/1000 - Policy loss: -0.0608, Value loss: 4.9545, Entropy: 8.9223\n",
      "Episode 961/1000 - Min reward: -20.6287, Max reward: 0.9724, Mean reward: -2.0159\n",
      "Episode 961/1000 - Policy loss: 0.0398, Value loss: 29.3128, Entropy: 8.9172\n",
      "Episode 962/1000 - Min reward: -26.7821, Max reward: 0.9980, Mean reward: -1.9206\n",
      "Episode 962/1000 - Policy loss: -0.1110, Value loss: 7.3689, Entropy: 8.9198\n",
      "Episode 963/1000 - Min reward: -12.4697, Max reward: 0.9996, Mean reward: 0.1509\n",
      "Episode 963/1000 - Policy loss: -0.1859, Value loss: 10.8468, Entropy: 8.9164\n",
      "Episode 964/1000 - Min reward: -7.4172, Max reward: 0.9982, Mean reward: -0.6062\n",
      "Episode 964/1000 - Policy loss: -0.1662, Value loss: 32.7807, Entropy: 8.9104\n",
      "Episode 965/1000 - Min reward: 0.7137, Max reward: 0.9979, Mean reward: 0.9274\n",
      "Episode 965/1000 - Policy loss: -0.1538, Value loss: 44.2616, Entropy: 8.9071\n",
      "Episode 966/1000 - Min reward: -7.8827, Max reward: 0.9990, Mean reward: 0.0839\n",
      "Episode 966/1000 - Policy loss: -0.0925, Value loss: 15.2855, Entropy: 8.8965\n",
      "Episode 967/1000 - Min reward: -15.2895, Max reward: 0.9661, Mean reward: -0.6679\n",
      "Episode 967/1000 - Policy loss: -0.1585, Value loss: 11.0259, Entropy: 8.8880\n",
      "Episode 968/1000 - Min reward: -21.0521, Max reward: 0.8925, Mean reward: -3.4883\n",
      "Episode 968/1000 - Policy loss: -0.0117, Value loss: 27.0582, Entropy: 8.8879\n",
      "Episode 969/1000 - Min reward: -22.7232, Max reward: 0.9999, Mean reward: -1.0926\n",
      "Episode 969/1000 - Policy loss: -0.2913, Value loss: 8.2316, Entropy: 8.8918\n",
      "Episode 970/1000 - Min reward: -2.1134, Max reward: 0.9998, Mean reward: 0.5231\n",
      "Episode 970/1000 - Policy loss: -0.0274, Value loss: 161.7698, Entropy: 8.9032\n",
      "Episode 971/1000 - Min reward: -8.8693, Max reward: 0.9999, Mean reward: 0.3276\n",
      "Episode 971/1000 - Policy loss: -0.1272, Value loss: 11.3478, Entropy: 8.9092\n",
      "Episode 972/1000 - Min reward: -11.8795, Max reward: 0.4628, Mean reward: -1.4529\n",
      "Episode 972/1000 - Policy loss: -0.0319, Value loss: 15.6786, Entropy: 8.9129\n",
      "Episode 973/1000 - Min reward: -25.1331, Max reward: 0.9768, Mean reward: -2.7794\n",
      "Episode 973/1000 - Policy loss: -0.3733, Value loss: 13.5315, Entropy: 8.9268\n",
      "Episode 974/1000 - Min reward: -11.5961, Max reward: 0.3344, Mean reward: -2.4350\n",
      "Episode 974/1000 - Policy loss: 0.1614, Value loss: 14.9788, Entropy: 8.9456\n",
      "Episode 975/1000 - Min reward: -16.3294, Max reward: 1.0000, Mean reward: -0.5535\n",
      "Episode 975/1000 - Policy loss: -0.0530, Value loss: 10.6462, Entropy: 8.9466\n",
      "Episode 976/1000 - Min reward: -9.1422, Max reward: 1.0000, Mean reward: 0.4678\n",
      "Episode 976/1000 - Policy loss: -0.1395, Value loss: 9.2299, Entropy: 8.9453\n",
      "Episode 977/1000 - Min reward: -24.0534, Max reward: 0.9839, Mean reward: -2.8858\n",
      "Episode 977/1000 - Policy loss: -0.1936, Value loss: 13.0054, Entropy: 8.9434\n",
      "Episode 978/1000 - Min reward: -9.8768, Max reward: 1.0000, Mean reward: 0.2728\n",
      "Episode 978/1000 - Policy loss: 0.0354, Value loss: 9.2178, Entropy: 8.9492\n",
      "Episode 979/1000 - Min reward: -17.3260, Max reward: 0.7067, Mean reward: -2.3269\n",
      "Episode 979/1000 - Policy loss: 0.2146, Value loss: 15.0334, Entropy: 8.9532\n",
      "Episode 980/1000 - Min reward: -18.3005, Max reward: 0.9827, Mean reward: -1.5127\n",
      "Episode 980/1000 - Policy loss: -0.3362, Value loss: 9.3237, Entropy: 8.9503\n",
      "Episode 981/1000 - Min reward: -14.2904, Max reward: 0.1794, Mean reward: -1.5157\n",
      "Episode 981/1000 - Policy loss: -0.3408, Value loss: 7.5360, Entropy: 8.9479\n",
      "Episode 982/1000 - Min reward: -18.7494, Max reward: 0.4207, Mean reward: -2.7434\n",
      "Episode 982/1000 - Policy loss: -0.1559, Value loss: 11.2564, Entropy: 8.9415\n",
      "Episode 983/1000 - Min reward: -18.8071, Max reward: 0.9945, Mean reward: -1.3625\n",
      "Episode 983/1000 - Policy loss: -0.0681, Value loss: 9.0679, Entropy: 8.9315\n",
      "Episode 984/1000 - Min reward: -8.8469, Max reward: 0.9989, Mean reward: -0.0073\n",
      "Episode 984/1000 - Policy loss: -0.2050, Value loss: 6.2471, Entropy: 8.9192\n",
      "Episode 985/1000 - Min reward: -18.1810, Max reward: 0.9993, Mean reward: -0.4884\n",
      "Episode 985/1000 - Policy loss: -0.1276, Value loss: 9.3365, Entropy: 8.9081\n",
      "Episode 986/1000 - Min reward: -13.4143, Max reward: 0.9972, Mean reward: -0.7423\n",
      "Episode 986/1000 - Policy loss: -0.3602, Value loss: 2.9544, Entropy: 8.9029\n",
      "Episode 987/1000 - Min reward: -24.6683, Max reward: 0.5342, Mean reward: -5.5101\n",
      "Episode 987/1000 - Policy loss: -0.1203, Value loss: 17.2956, Entropy: 8.9072\n",
      "Episode 988/1000 - Min reward: -16.8282, Max reward: 1.0000, Mean reward: -0.7722\n",
      "Episode 988/1000 - Policy loss: 0.0897, Value loss: 19.7181, Entropy: 8.9005\n",
      "Episode 989/1000 - Min reward: -17.9376, Max reward: 0.8530, Mean reward: -1.0969\n",
      "Episode 989/1000 - Policy loss: -0.3801, Value loss: 4.9577, Entropy: 8.8944\n",
      "Episode 990/1000 - Min reward: -16.1831, Max reward: 0.8435, Mean reward: -2.9947\n",
      "Episode 990/1000 - Policy loss: 0.0930, Value loss: 14.5399, Entropy: 8.8884\n",
      "Episode 991/1000 - Min reward: -16.1199, Max reward: 0.5037, Mean reward: -2.5417\n",
      "Episode 991/1000 - Policy loss: 0.0426, Value loss: 10.4519, Entropy: 8.9015\n",
      "Episode 992/1000 - Min reward: -6.8991, Max reward: 0.9978, Mean reward: 0.5082\n",
      "Episode 992/1000 - Policy loss: -0.1040, Value loss: 5.9852, Entropy: 8.9190\n",
      "Episode 993/1000 - Min reward: -22.8411, Max reward: 0.9870, Mean reward: -1.0381\n",
      "Episode 993/1000 - Policy loss: -0.3414, Value loss: 1.5254, Entropy: 8.9273\n",
      "Episode 994/1000 - Min reward: -13.7801, Max reward: 0.9694, Mean reward: -2.0557\n",
      "Episode 994/1000 - Policy loss: -0.1453, Value loss: 7.2710, Entropy: 8.9149\n",
      "Episode 995/1000 - Min reward: -11.6908, Max reward: 0.9671, Mean reward: 0.0877\n",
      "Episode 995/1000 - Policy loss: -0.3130, Value loss: 9.8814, Entropy: 8.8948\n",
      "Episode 996/1000 - Min reward: -21.2709, Max reward: 0.9418, Mean reward: -2.3066\n",
      "Episode 996/1000 - Policy loss: -0.0289, Value loss: 4.3886, Entropy: 8.8847\n",
      "Episode 997/1000 - Min reward: -16.6722, Max reward: 0.9991, Mean reward: -0.6509\n",
      "Episode 997/1000 - Policy loss: 0.1073, Value loss: 6.7773, Entropy: 8.8842\n",
      "Episode 998/1000 - Min reward: -12.4660, Max reward: 0.1654, Mean reward: -1.8495\n",
      "Episode 998/1000 - Policy loss: -0.1224, Value loss: 11.8859, Entropy: 8.8867\n",
      "Episode 999/1000 - Min reward: -11.5127, Max reward: 0.7658, Mean reward: -1.3344\n",
      "Episode 999/1000 - Policy loss: 0.0023, Value loss: 5.4268, Entropy: 8.8961\n",
      "Episode 1000/1000 - Min reward: -3.0259, Max reward: 0.9878, Mean reward: 0.4743\n",
      "Episode 1000/1000 - Policy loss: -0.1902, Value loss: 2.2958, Entropy: 8.8915\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize PPO agent (actor and critic)\n",
    "ppo_agent = PPOAgent(cfg, logger)\n",
    "\n",
    "log_rewards = []\n",
    "# Training loop\n",
    "for episode in range(cfg.training.num_episodes):\n",
    "    trajectory = ppo_agent.collect_trajectory(env)\n",
    "    rewards = trajectory[\"rewards\"]\n",
    "    min_rew, max_rew, mean_rew = rewards.min(), rewards.max(), rewards.mean()\n",
    "    print(f\"Episode {episode+1}/{cfg.training.num_episodes} - Min reward: {min_rew:.4f}, Max reward: {max_rew:.4f}, Mean reward: {mean_rew:.4f}\")\n",
    "    log_rewards.append(rewards.mean())\n",
    "    policy_loss, value_loss, entropy = ppo_agent.update(trajectory)\n",
    "    print(f\"Episode {episode+1}/{cfg.training.num_episodes} - Policy loss: {policy_loss:.4f}, Value loss: {value_loss:.4f}, Entropy: {entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e27a648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f18c0ffbf28>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(log_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68f8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
