{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aae562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-u8tan80v because the default path (/home/renaissance/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import renaissance.helper as hp\n",
    "from configparser import ConfigParser\n",
    "\n",
    "from renaissance.ppo_refinement import PPORefinement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51519fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_flag = 0\n",
    "lambda_partition = -2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d113bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lambda_max(p_tensor_single):\n",
    "    # Convert tensor to NumPy\n",
    "    p_numpy = p_tensor_single.detach().cpu().numpy()\n",
    "\n",
    "    # Determine size n from length = n(n+1)/2\n",
    "    L = p_numpy.shape[0]\n",
    "    # Solve n(n+1)/2 = L\n",
    "    n = int((np.sqrt(8 * L + 1) - 1) / 2)\n",
    "    assert n * (n + 1) // 2 == L, f\"Input length {L} is not valid for any symmetric n x n matrix.\"\n",
    "\n",
    "    # Fill upper triangular matrix\n",
    "    A = np.zeros((n, n))\n",
    "    idx = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):  # Only upper triangle including diagonal\n",
    "            A[i, j] = p_numpy[idx]\n",
    "            A[j, i] = p_numpy[idx]  # Mirror to lower triangle\n",
    "            idx += 1\n",
    "\n",
    "    # Compute eigenvalues\n",
    "    eigvals = np.linalg.eigvals(A)\n",
    "\n",
    "    # Sort by real part descending\n",
    "    eigvals_sorted = sorted(eigvals, key=lambda x: x.real, reverse=True)\n",
    "\n",
    "    return eigvals_sorted\n",
    "\n",
    "def compute_reward(p_tensor_single, n_consider=10):\n",
    "    lambdas_val = _get_lambda_max(p_tensor_single)\n",
    "\n",
    "    if reward_flag == 0:\n",
    "        lambda_max_val = lambdas_val[0]\n",
    "        penalty = np.maximum(0, lambda_max_val)\n",
    "        z = np.clip(lambda_max_val - lambda_partition, -20, +20)\n",
    "        r = 1.0 / (1.0 + np.exp(z))\n",
    "        # r -= penalty\n",
    "    else:\n",
    "        considered_avg = sum(lambdas_val[:n_consider]) / n_consider\n",
    "        r = np.exp(-0.1 * considered_avg) / 2\n",
    "    # TODO: Right now, we are not using the Incidence part of the reward.\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741db84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Begin PPO refinement strategy\n",
      "Training on cpu. 1 trajectories per update.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 10)) of distribution Normal(loc: torch.Size([1, 10]), scale: torch.Size([1, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n       grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-856c4767e817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrained_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_training_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthis_savepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#print(f\"PPO training finished. Rewards log saved to {this_savepath}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/renaissance/ppo_refinement.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_training_iterations, output_path)\u001b[0m\n\u001b[1;32m    227\u001b[0m             )\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_rollout_data_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_batch_final_rewards\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/renaissance/ppo_refinement.py\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m(self, rollout_data)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mmu_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_std_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mstd_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_std_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mdist_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mnew_log_probs_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 56\u001b[0;31m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;34mf\"of distribution {repr(self)} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 10)) of distribution Normal(loc: torch.Size([1, 10]), scale: torch.Size([1, 10])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n       grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "print('--- Begin PPO refinement strategy')\n",
    "\n",
    "configs = ConfigParser()\n",
    "configs.read('renaissance/configfile.ini')\n",
    "output_path = configs['PATHS']['output_path']\n",
    "this_savepath = f'output/ppo-refinement/ppo_sandbox/' \n",
    "os.makedirs(this_savepath, exist_ok=True)\n",
    "\n",
    "ppo_agent = PPORefinement(\n",
    "    param_dim=10,\n",
    "    noise_dim=10,\n",
    "    reward_function=compute_reward,\n",
    "    min_x_bounds=-10,\n",
    "    max_x_bounds=10,\n",
    "    ppo_epochs=10,\n",
    "    T_horizon=1,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-4,\n",
    "    n_trajectories=1,\n",
    ")\n",
    "\n",
    "trained_actor, rewards = ppo_agent.train(num_training_iterations=100, output_path=this_savepath)\n",
    "\n",
    "#print(f\"PPO training finished. Rewards log saved to {this_savepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a94691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'PPO Training Rewards')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Final Reward')\n",
    "plt.title('PPO Training Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74902174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "def evaluate_policy_incidence(trained_actor, ppo_agent, num_trials=50):\n",
    "    T_HORIZON = ppo_agent.T_horizon\n",
    "    NOISE_DIM = ppo_agent.noise_dim\n",
    "    N_TRIALS = num_trials\n",
    "    incidence = 0\n",
    "    for _ in range(N_TRIALS):\n",
    "        current_params_in_state = ppo_agent._initialize_current_params_for_state().clone()\n",
    "        generated_sequence = []\n",
    "        for _ in range(T_HORIZON):\n",
    "            noise = torch.randn(NOISE_DIM, device=ppo_agent.device)\n",
    "            state_1d = torch.cat((noise, current_params_in_state.detach()), dim=0)\n",
    "            state_batch = state_1d.unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                mu_raw, log_std_raw = trained_actor(state_batch)\n",
    "                # For generation, you might want to take the mean (mu_raw) or sample\n",
    "                # action_raw = mu_raw # Deterministic generation\n",
    "                action_raw = Normal(mu_raw, torch.exp(log_std_raw)).sample() # Stochastic generation\n",
    "            \n",
    "            ode_params = ppo_agent._transform_to_bounded(action_raw)\n",
    "            current_params_in_state = ode_params.squeeze(0)\n",
    "            generated_sequence.append(current_params_in_state.cpu().numpy())\n",
    "\n",
    "        final_generated_params = generated_sequence[-1]\n",
    "        final_reward_eval = compute_reward(torch.tensor(final_generated_params, device=ppo_agent.device))\n",
    "        lambda_max = _get_lambda_max(torch.tensor(final_generated_params, device=ppo_agent.device))[0]\n",
    "        if lambda_max < -2.5:\n",
    "            incidence += 1\n",
    "        print(f\"Final lambda_max: {lambda_max:.4f}\")\n",
    "        print(f\"Final reward: {final_reward_eval:.4f}\")\n",
    "    print(f\"Incidence over {N_TRIALS} trials: {incidence}/{N_TRIALS} = {incidence/N_TRIALS:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202681fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final lambda_max: -7.1210\n",
      "Final reward: 0.9903\n",
      "Final lambda_max: -8.2873\n",
      "Final reward: 0.9969\n",
      "Final lambda_max: -8.1637\n",
      "Final reward: 0.9965\n",
      "Final lambda_max: -7.4484\n",
      "Final reward: 0.9930\n",
      "Final lambda_max: -8.2206\n",
      "Final reward: 0.9967\n",
      "Final lambda_max: -7.8938\n",
      "Final reward: 0.9955\n",
      "Final lambda_max: -8.0707\n",
      "Final reward: 0.9962\n",
      "Final lambda_max: -7.3388\n",
      "Final reward: 0.9921\n",
      "Final lambda_max: -8.5687\n",
      "Final reward: 0.9977\n",
      "Final lambda_max: -7.9198\n",
      "Final reward: 0.9956\n",
      "Final lambda_max: -8.0095\n",
      "Final reward: 0.9960\n",
      "Final lambda_max: -8.1965\n",
      "Final reward: 0.9967\n",
      "Final lambda_max: -7.9705\n",
      "Final reward: 0.9958\n",
      "Final lambda_max: -8.0716\n",
      "Final reward: 0.9962\n",
      "Final lambda_max: -7.9326\n",
      "Final reward: 0.9956\n",
      "Final lambda_max: -8.0483\n",
      "Final reward: 0.9961\n",
      "Final lambda_max: -7.8731\n",
      "Final reward: 0.9954\n",
      "Final lambda_max: -8.1165\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -8.2331\n",
      "Final reward: 0.9968\n",
      "Final lambda_max: -8.6573\n",
      "Final reward: 0.9979\n",
      "Final lambda_max: -7.8948\n",
      "Final reward: 0.9955\n",
      "Final lambda_max: -8.3819\n",
      "Final reward: 0.9972\n",
      "Final lambda_max: -8.1332\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -8.1291\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -8.1537\n",
      "Final reward: 0.9965\n",
      "Final lambda_max: -8.2251\n",
      "Final reward: 0.9967\n",
      "Final lambda_max: -7.9329\n",
      "Final reward: 0.9956\n",
      "Final lambda_max: -8.0274\n",
      "Final reward: 0.9960\n",
      "Final lambda_max: -8.1259\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -7.9443\n",
      "Final reward: 0.9957\n",
      "Final lambda_max: -7.7594\n",
      "Final reward: 0.9948\n",
      "Final lambda_max: -8.7048\n",
      "Final reward: 0.9980\n",
      "Final lambda_max: -7.7205\n",
      "Final reward: 0.9946\n",
      "Final lambda_max: -8.0560\n",
      "Final reward: 0.9962\n",
      "Final lambda_max: -8.1431\n",
      "Final reward: 0.9965\n",
      "Final lambda_max: -8.2288\n",
      "Final reward: 0.9968\n",
      "Final lambda_max: -7.7969\n",
      "Final reward: 0.9950\n",
      "Final lambda_max: -7.6468\n",
      "Final reward: 0.9942\n",
      "Final lambda_max: -7.8991\n",
      "Final reward: 0.9955\n",
      "Final lambda_max: -8.0385\n",
      "Final reward: 0.9961\n",
      "Final lambda_max: -8.6161\n",
      "Final reward: 0.9978\n",
      "Final lambda_max: -7.9944\n",
      "Final reward: 0.9959\n",
      "Final lambda_max: -8.2521\n",
      "Final reward: 0.9968\n",
      "Final lambda_max: -7.3098\n",
      "Final reward: 0.9919\n",
      "Final lambda_max: -8.3445\n",
      "Final reward: 0.9971\n",
      "Final lambda_max: -7.6144\n",
      "Final reward: 0.9940\n",
      "Final lambda_max: -7.6060\n",
      "Final reward: 0.9940\n",
      "Final lambda_max: -8.1172\n",
      "Final reward: 0.9964\n",
      "Final lambda_max: -7.6183\n",
      "Final reward: 0.9940\n",
      "Final lambda_max: -7.2254\n",
      "Final reward: 0.9912\n",
      "Incidence over 50 trials: 50/50 = 1.00\n"
     ]
    }
   ],
   "source": [
    "evaluate_policy_incidence(trained_actor, ppo_agent, num_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cbd827",
   "metadata": {},
   "source": [
    "## Nonlinear env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b2a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_nonlinear_dynamics(x, theta):\n",
    "    n = x.shape[0]\n",
    "    assert theta.shape[0] >= n * n, f\"Need at least {n * n} parameters, got {theta.shape[0]}\"\n",
    "    \n",
    "    # Reshape first n^2 parameters into n x n matrix\n",
    "    T = theta[:n * n].reshape(n, n)\n",
    "\n",
    "    # Define nonlinear function: f_i(x) = sum_j T_ij * tanh(x_j) + sin(x_i) * x_i\n",
    "    x_tanh = torch.tanh(x)\n",
    "    linear_part = T @ x_tanh\n",
    "    nonlinear_part = torch.sin(x) * x\n",
    "\n",
    "    return linear_part + nonlinear_part\n",
    "\n",
    "def _get_lambda_max(p_tensor_single, x_eval=None):\n",
    "    # Infer n from length\n",
    "    L = p_tensor_single.shape[0]\n",
    "    n = int(np.sqrt(L))\n",
    "    if x_eval is None:\n",
    "        x_eval = torch.zeros(n, dtype=torch.float32, requires_grad=True)\n",
    "    else:\n",
    "        x_eval = x_eval.detach().clone().requires_grad_(True)\n",
    "\n",
    "    theta = p_tensor_single.detach().clone().requires_grad_(False)\n",
    "\n",
    "    # Compute f(x; theta)\n",
    "    f = generalized_nonlinear_dynamics(x_eval, theta)\n",
    "\n",
    "    # Compute Jacobian via autograd\n",
    "    J_rows = []\n",
    "    for f_i in f:\n",
    "        grad_f_i = torch.autograd.grad(f_i, x_eval, retain_graph=True)[0]\n",
    "        J_rows.append(grad_f_i)\n",
    "\n",
    "    J_matrix = torch.stack(J_rows)\n",
    "    J_np = J_matrix.detach().numpy()\n",
    "\n",
    "    # Compute eigenvalues\n",
    "    eigvals = np.linalg.eigvals(J_np)\n",
    "    eigvals_sorted = sorted(eigvals, key=lambda x: x.real, reverse=True)\n",
    "    return eigvals_sorted\n",
    "\n",
    "def compute_reward(p_tensor_single, n_consider=10):\n",
    "    lambdas_val = _get_lambda_max(p_tensor_single)\n",
    "\n",
    "    if reward_flag == 0:\n",
    "        lambda_max_val = np.real(lambdas_val[0])\n",
    "        penalty = np.maximum(0, lambda_max_val)\n",
    "        if lambda_max_val > 100:\n",
    "            lambda_max_val = 100\n",
    "        r = 1.0 / (1.0 + np.exp(lambda_max_val - lambda_partition))\n",
    "        r -= penalty\n",
    "    else:\n",
    "        considered_avg = sum(lambdas_val[:n_consider]) / n_consider\n",
    "        r = np.exp(-0.1 * considered_avg) / 2\n",
    "    # TODO: Right now, we are not using the Incidence part of the reward.\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66713fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Begin PPO refinement strategy\n",
      "Training on cpu. 64 trajectories per update.\n",
      "Iteration 1/100, Avg Batch Final Reward: -8.1502\n",
      "Avg actor loss -0.0546 Avg critic loss 90.9855\n",
      "Iteration 2/100, Avg Batch Final Reward: -7.1286\n",
      "Avg actor loss -0.0354 Avg critic loss 60.2790\n",
      "Iteration 3/100, Avg Batch Final Reward: -7.1762\n",
      "Avg actor loss -0.0347 Avg critic loss 56.6773\n",
      "Iteration 4/100, Avg Batch Final Reward: -8.4056\n",
      "Avg actor loss -0.0356 Avg critic loss 75.5095\n",
      "Iteration 5/100, Avg Batch Final Reward: -6.0248\n",
      "Avg actor loss -0.0318 Avg critic loss 37.7575\n",
      "Iteration 6/100, Avg Batch Final Reward: -7.9332\n",
      "Avg actor loss -0.0326 Avg critic loss 52.5337\n",
      "Iteration 7/100, Avg Batch Final Reward: -6.9918\n",
      "Avg actor loss -0.0432 Avg critic loss 39.7058\n",
      "Iteration 8/100, Avg Batch Final Reward: -5.9068\n",
      "Avg actor loss -0.0379 Avg critic loss 28.8744\n",
      "Iteration 9/100, Avg Batch Final Reward: -5.9719\n",
      "Avg actor loss -0.0392 Avg critic loss 27.2944\n",
      "Iteration 10/100, Avg Batch Final Reward: -5.5444\n",
      "Avg actor loss -0.0309 Avg critic loss 22.3010\n",
      "Iteration 11/100, Avg Batch Final Reward: -5.4704\n",
      "Avg actor loss -0.0310 Avg critic loss 22.7050\n",
      "Iteration 12/100, Avg Batch Final Reward: -5.8596\n",
      "Avg actor loss -0.0350 Avg critic loss 19.8186\n",
      "Iteration 13/100, Avg Batch Final Reward: -5.4764\n",
      "Avg actor loss -0.0410 Avg critic loss 22.8163\n",
      "Iteration 14/100, Avg Batch Final Reward: -3.7110\n",
      "Avg actor loss -0.0344 Avg critic loss 17.7593\n",
      "Iteration 15/100, Avg Batch Final Reward: -4.6262\n",
      "Avg actor loss -0.0308 Avg critic loss 18.7076\n",
      "Iteration 16/100, Avg Batch Final Reward: -4.4638\n",
      "Avg actor loss -0.0390 Avg critic loss 22.4018\n",
      "Iteration 17/100, Avg Batch Final Reward: -3.3952\n",
      "Avg actor loss -0.0431 Avg critic loss 15.7459\n",
      "Iteration 18/100, Avg Batch Final Reward: -3.2943\n",
      "Avg actor loss -0.0335 Avg critic loss 15.2044\n",
      "Iteration 19/100, Avg Batch Final Reward: -2.7225\n",
      "Avg actor loss -0.0436 Avg critic loss 14.1671\n",
      "Iteration 20/100, Avg Batch Final Reward: -2.9008\n",
      "Avg actor loss -0.0417 Avg critic loss 16.7047\n",
      "Iteration 21/100, Avg Batch Final Reward: -2.2672\n",
      "Avg actor loss -0.0358 Avg critic loss 10.9164\n",
      "Iteration 22/100, Avg Batch Final Reward: -1.9594\n",
      "Avg actor loss -0.0353 Avg critic loss 12.8179\n",
      "Iteration 23/100, Avg Batch Final Reward: -1.9363\n",
      "Avg actor loss -0.0323 Avg critic loss 9.0875\n",
      "Iteration 24/100, Avg Batch Final Reward: -2.4852\n",
      "Avg actor loss -0.0400 Avg critic loss 12.2847\n",
      "Iteration 25/100, Avg Batch Final Reward: -1.7237\n",
      "Avg actor loss -0.0400 Avg critic loss 10.3976\n",
      "Iteration 26/100, Avg Batch Final Reward: -1.6535\n",
      "Avg actor loss -0.0425 Avg critic loss 9.2996\n",
      "Iteration 27/100, Avg Batch Final Reward: -1.1057\n",
      "Avg actor loss -0.0390 Avg critic loss 6.0601\n",
      "Iteration 28/100, Avg Batch Final Reward: -0.3804\n",
      "Avg actor loss -0.0386 Avg critic loss 4.4721\n",
      "Iteration 29/100, Avg Batch Final Reward: -0.8164\n",
      "Avg actor loss -0.0521 Avg critic loss 5.5794\n",
      "Iteration 30/100, Avg Batch Final Reward: -0.6254\n",
      "Avg actor loss -0.0314 Avg critic loss 4.1274\n",
      "Iteration 31/100, Avg Batch Final Reward: -0.3215\n",
      "Avg actor loss -0.0293 Avg critic loss 3.9908\n",
      "Iteration 32/100, Avg Batch Final Reward: -0.4707\n",
      "Avg actor loss -0.0489 Avg critic loss 5.0051\n",
      "Iteration 33/100, Avg Batch Final Reward: -0.7278\n",
      "Avg actor loss -0.0353 Avg critic loss 5.1236\n",
      "Iteration 34/100, Avg Batch Final Reward: -0.4856\n",
      "Avg actor loss -0.0450 Avg critic loss 4.3288\n",
      "Iteration 35/100, Avg Batch Final Reward: -0.0329\n",
      "Avg actor loss -0.0385 Avg critic loss 3.2349\n",
      "Iteration 36/100, Avg Batch Final Reward: 0.2655\n",
      "Avg actor loss -0.0376 Avg critic loss 1.3125\n",
      "Iteration 37/100, Avg Batch Final Reward: 0.1565\n",
      "Avg actor loss -0.0356 Avg critic loss 1.8636\n",
      "Iteration 38/100, Avg Batch Final Reward: 0.0409\n",
      "Avg actor loss -0.0435 Avg critic loss 2.7948\n",
      "Iteration 39/100, Avg Batch Final Reward: 0.3793\n",
      "Avg actor loss -0.0327 Avg critic loss 1.3220\n",
      "Iteration 40/100, Avg Batch Final Reward: 0.3017\n",
      "Avg actor loss -0.0286 Avg critic loss 1.8296\n",
      "Iteration 41/100, Avg Batch Final Reward: 0.4070\n",
      "Avg actor loss -0.0242 Avg critic loss 0.7431\n",
      "Iteration 42/100, Avg Batch Final Reward: 0.4172\n",
      "Avg actor loss -0.0391 Avg critic loss 1.0508\n",
      "Iteration 43/100, Avg Batch Final Reward: 0.4573\n",
      "Avg actor loss -0.0284 Avg critic loss 0.9077\n",
      "Iteration 44/100, Avg Batch Final Reward: 0.6418\n",
      "Avg actor loss -0.0304 Avg critic loss 0.4505\n",
      "Iteration 45/100, Avg Batch Final Reward: 0.6175\n",
      "Avg actor loss -0.0341 Avg critic loss 0.5078\n",
      "Iteration 46/100, Avg Batch Final Reward: 0.6733\n",
      "Avg actor loss -0.0280 Avg critic loss 0.5422\n",
      "Iteration 47/100, Avg Batch Final Reward: 0.6879\n",
      "Avg actor loss -0.0427 Avg critic loss 0.2420\n",
      "Iteration 48/100, Avg Batch Final Reward: 0.7962\n",
      "Avg actor loss -0.0277 Avg critic loss 0.0968\n",
      "Iteration 49/100, Avg Batch Final Reward: 0.7017\n",
      "Avg actor loss -0.0292 Avg critic loss 0.7042\n",
      "Iteration 50/100, Avg Batch Final Reward: 0.8421\n",
      "Avg actor loss -0.0380 Avg critic loss 0.1528\n",
      "Iteration 51/100, Avg Batch Final Reward: 0.8423\n",
      "Avg actor loss -0.0302 Avg critic loss 0.0929\n",
      "Iteration 52/100, Avg Batch Final Reward: 0.8419\n",
      "Avg actor loss -0.0259 Avg critic loss 0.3267\n",
      "Iteration 53/100, Avg Batch Final Reward: 0.9113\n",
      "Avg actor loss -0.0296 Avg critic loss 0.0490\n",
      "Iteration 54/100, Avg Batch Final Reward: 0.8756\n",
      "Avg actor loss -0.0325 Avg critic loss 0.1015\n",
      "Iteration 55/100, Avg Batch Final Reward: 0.8922\n",
      "Avg actor loss -0.0336 Avg critic loss 0.0538\n",
      "Iteration 56/100, Avg Batch Final Reward: 0.9111\n",
      "Avg actor loss -0.0369 Avg critic loss 0.0262\n",
      "Iteration 57/100, Avg Batch Final Reward: 0.9238\n",
      "Avg actor loss -0.0290 Avg critic loss 0.0304\n",
      "Iteration 58/100, Avg Batch Final Reward: 0.9327\n",
      "Avg actor loss -0.0282 Avg critic loss 0.0196\n",
      "Iteration 59/100, Avg Batch Final Reward: 0.9084\n",
      "Avg actor loss -0.0429 Avg critic loss 0.0397\n",
      "Iteration 60/100, Avg Batch Final Reward: 0.9570\n",
      "Avg actor loss -0.0302 Avg critic loss 0.0120\n",
      "Iteration 61/100, Avg Batch Final Reward: 0.9483\n",
      "Avg actor loss -0.0296 Avg critic loss 0.0169\n",
      "Iteration 62/100, Avg Batch Final Reward: 0.9655\n",
      "Avg actor loss -0.0308 Avg critic loss 0.0041\n",
      "Iteration 63/100, Avg Batch Final Reward: 0.9571\n",
      "Avg actor loss -0.0373 Avg critic loss 0.0132\n",
      "Iteration 64/100, Avg Batch Final Reward: 0.9643\n",
      "Avg actor loss -0.0294 Avg critic loss 0.0106\n",
      "Iteration 65/100, Avg Batch Final Reward: 0.9396\n",
      "Avg actor loss -0.0301 Avg critic loss 0.0682\n",
      "Iteration 66/100, Avg Batch Final Reward: 0.9798\n",
      "Avg actor loss -0.0229 Avg critic loss 0.0044\n",
      "Iteration 67/100, Avg Batch Final Reward: 0.9844\n",
      "Avg actor loss -0.0235 Avg critic loss 0.0013\n",
      "Iteration 68/100, Avg Batch Final Reward: 0.9765\n",
      "Avg actor loss -0.0289 Avg critic loss 0.0024\n",
      "Iteration 69/100, Avg Batch Final Reward: 0.9852\n",
      "Avg actor loss -0.0322 Avg critic loss 0.0015\n",
      "Iteration 70/100, Avg Batch Final Reward: 0.9895\n",
      "Avg actor loss -0.0298 Avg critic loss 0.0006\n",
      "Iteration 71/100, Avg Batch Final Reward: 0.9706\n",
      "Avg actor loss -0.0254 Avg critic loss 0.0096\n",
      "Iteration 72/100, Avg Batch Final Reward: 0.9704\n",
      "Avg actor loss -0.0200 Avg critic loss 0.0128\n",
      "Iteration 73/100, Avg Batch Final Reward: 0.9870\n",
      "Avg actor loss -0.0444 Avg critic loss 0.0009\n",
      "Iteration 74/100, Avg Batch Final Reward: 0.9880\n",
      "Avg actor loss -0.0344 Avg critic loss 0.0009\n",
      "Iteration 75/100, Avg Batch Final Reward: 0.9900\n",
      "Avg actor loss -0.0459 Avg critic loss 0.0005\n",
      "Iteration 76/100, Avg Batch Final Reward: 0.9888\n",
      "Avg actor loss -0.0269 Avg critic loss 0.0004\n",
      "Iteration 77/100, Avg Batch Final Reward: 0.9869\n",
      "Avg actor loss -0.0282 Avg critic loss 0.0016\n",
      "Iteration 78/100, Avg Batch Final Reward: 0.9927\n",
      "Avg actor loss -0.0238 Avg critic loss 0.0004\n",
      "Iteration 79/100, Avg Batch Final Reward: 0.9901\n",
      "Avg actor loss -0.0350 Avg critic loss 0.0010\n",
      "Iteration 80/100, Avg Batch Final Reward: 0.9902\n",
      "Avg actor loss -0.0301 Avg critic loss 0.0011\n",
      "Iteration 81/100, Avg Batch Final Reward: 0.9908\n",
      "Avg actor loss -0.0287 Avg critic loss 0.0011\n",
      "Iteration 82/100, Avg Batch Final Reward: 0.9927\n",
      "Avg actor loss -0.0328 Avg critic loss 0.0003\n",
      "Iteration 83/100, Avg Batch Final Reward: 0.9941\n",
      "Avg actor loss -0.0196 Avg critic loss 0.0002\n",
      "Iteration 84/100, Avg Batch Final Reward: 0.9963\n",
      "Avg actor loss -0.0279 Avg critic loss 0.0001\n",
      "Iteration 85/100, Avg Batch Final Reward: 0.9957\n",
      "Avg actor loss -0.0233 Avg critic loss 0.0002\n",
      "Iteration 86/100, Avg Batch Final Reward: 0.9928\n",
      "Avg actor loss -0.0246 Avg critic loss 0.0004\n",
      "Iteration 87/100, Avg Batch Final Reward: 0.9914\n",
      "Avg actor loss -0.0312 Avg critic loss 0.0009\n",
      "Iteration 88/100, Avg Batch Final Reward: 0.9957\n",
      "Avg actor loss -0.0245 Avg critic loss 0.0001\n",
      "Iteration 89/100, Avg Batch Final Reward: 0.9953\n",
      "Avg actor loss -0.0170 Avg critic loss 0.0002\n",
      "Iteration 90/100, Avg Batch Final Reward: 0.9952\n",
      "Avg actor loss -0.0216 Avg critic loss 0.0002\n",
      "Iteration 91/100, Avg Batch Final Reward: 0.9944\n",
      "Avg actor loss -0.0337 Avg critic loss 0.0003\n",
      "Iteration 92/100, Avg Batch Final Reward: 0.9965\n",
      "Avg actor loss -0.0341 Avg critic loss 0.0001\n",
      "Iteration 93/100, Avg Batch Final Reward: 0.9942\n",
      "Avg actor loss -0.0311 Avg critic loss 0.0005\n",
      "Iteration 94/100, Avg Batch Final Reward: 0.9963\n",
      "Avg actor loss -0.0263 Avg critic loss 0.0001\n",
      "Iteration 95/100, Avg Batch Final Reward: 0.9967\n",
      "Avg actor loss -0.0173 Avg critic loss 0.0001\n",
      "Iteration 96/100, Avg Batch Final Reward: 0.9969\n",
      "Avg actor loss -0.0189 Avg critic loss 0.0001\n",
      "Iteration 97/100, Avg Batch Final Reward: 0.9969\n",
      "Avg actor loss -0.0228 Avg critic loss 0.0001\n",
      "Iteration 98/100, Avg Batch Final Reward: 0.9962\n",
      "Avg actor loss -0.0240 Avg critic loss 0.0001\n",
      "Iteration 99/100, Avg Batch Final Reward: 0.9959\n",
      "Avg actor loss -0.0271 Avg critic loss 0.0002\n",
      "Iteration 100/100, Avg Batch Final Reward: 0.9972\n",
      "Avg actor loss -0.0343 Avg critic loss 0.0001\n",
      "Model saved at iteration 100\n",
      "Final models saved to output/ppo-refinement/ppo_sandbox/\n",
      "Average final rewards per iteration saved at output/ppo-refinement/ppo_sandbox/avg_final_rewards_per_iteration.npy\n",
      "Training finished.\n",
      "PPO training finished. Rewards log saved to output/ppo-refinement/ppo_sandbox/\n"
     ]
    }
   ],
   "source": [
    "print('--- Begin PPO refinement strategy')\n",
    "\n",
    "configs = ConfigParser()\n",
    "configs.read('configfile.ini')\n",
    "output_path = configs['PATHS']['output_path']\n",
    "this_savepath = f'output/ppo-refinement/ppo_sandbox/' \n",
    "os.makedirs(this_savepath, exist_ok=True)\n",
    "\n",
    "ppo_agent = PPORefinement(\n",
    "    param_dim=10,\n",
    "    noise_dim=10,\n",
    "    reward_function=compute_reward,\n",
    "    min_x_bounds=-10,\n",
    "    max_x_bounds=10,\n",
    "    ppo_epochs=10,\n",
    "    T_horizon=1,\n",
    "    actor_lr=1e-5,\n",
    "    critic_lr=5e-5,\n",
    "    n_trajectories=64,\n",
    ")\n",
    "\n",
    "trained_actor, rewards = ppo_agent.train(num_training_iterations=100, output_path=this_savepath)\n",
    "\n",
    "print(f\"PPO training finished. Rewards log saved to {this_savepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eb5ef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'PPO Training Rewards')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Final Reward')\n",
    "plt.title('PPO Training Rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bd02e",
   "metadata": {},
   "source": [
    "# New implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253bc15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from renaissance.kinetics.jacobian_solver import check_jacobian\n",
    "\n",
    "from helpers.ppo_agent import PPOAgent\n",
    "from helpers.env import KineticEnv\n",
    "from helpers.utils import reward_func, load_pkl\n",
    "import torch\n",
    "\n",
    "# Hydra-style manual config loading in a notebook\n",
    "initialize(config_path=\"configs\", version_base=\"1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea57798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"train.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b868e67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "method:\n",
      "  parameter_dim: 384\n",
      "  latent_dim: 99\n",
      "  name: ppo_refinement\n",
      "  actor_lr: 0.0001\n",
      "  critic_lr: 0.0001\n",
      "  discount_factor: 0.99\n",
      "  gae_lambda: 0.98\n",
      "  clip_eps: 0.2\n",
      "  value_loss_weight: 0.5\n",
      "  entropy_loss_weight: 0.0\n",
      "  scale_action: 1.0\n",
      "seed: 42\n",
      "device: cpu\n",
      "logger:\n",
      "  project: rl-renaissance\n",
      "  entity: ludekcizinsky\n",
      "  tags:\n",
      "  - dev\n",
      "paths:\n",
      "  names_km: data/varma_ecoli_shikki/parameter_names_km_fdp1.pkl\n",
      "  output_dir: /home/renaissance/work/output\n",
      "  met_model_name: varma_ecoli_shikki\n",
      "constraints:\n",
      "  min_km: -25\n",
      "  max_km: 3\n",
      "  ss_idx: 1712\n",
      "reward:\n",
      "  eig_partition: -2.5\n",
      "env:\n",
      "  p0_init_mean: 0\n",
      "  p0_init_std: 0.1\n",
      "  p_size: 384\n",
      "training:\n",
      "  num_episodes: 100\n",
      "  max_steps_per_episode: 30\n",
      "  num_trajectories_per_episode: 1\n",
      "  n_dist_samples: 10\n",
      "  batch_size: 25\n",
      "  num_epochs: 10\n",
      "  max_grad_norm: 0.5\n",
      "\n",
      "--------------------------------------------------\n",
      "FYI: Loading kinetic and thermodynamic data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 12:57:55,520 - thermomodel_new - INFO - # Model initialized with units kcal/mol and temperature 298.15 K\n",
      "2025-05-19 12:57:58,966 - Unnamed - WARNING - Non integer stoichiometries found ['CYTBO3_4pp', 'LMPD_biomass_c_1_420', 'CYTBDpp'] change to integer for linear dependencies\n",
      "2025-05-19 12:57:59,588 - Unnamed - WARNING - Non integer stoichiometries found ['CYTBO3_4pp', 'LMPD_biomass_c_1_420', 'CYTBDpp'] change to integer for linear dependencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-28:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-15:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-3:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-30:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-11:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-32:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 50)\n",
    "print(OmegaConf.to_yaml(cfg))  # print config to verify\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Call solvers from SKimPy\n",
    "chk_jcbn = check_jacobian()\n",
    "\n",
    "# Integrate data\n",
    "print(\"FYI: Loading kinetic and thermodynamic data.\")\n",
    "chk_jcbn._load_ktmodels(cfg.paths.met_model_name, 'fdp1') # Load kinetic and thermodynamic data\n",
    "chk_jcbn._load_ssprofile(cfg.paths.met_model_name, 'fdp1', cfg.constraints.ss_idx) # Integrate steady state information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger setup, todo: for now disabled, else we would get w&b run object\n",
    "from functools import partial\n",
    "\n",
    "logger = None # get_logger(cfg)\n",
    "\n",
    "# Initialize environment\n",
    "names_km = load_pkl(cfg.paths.names_km)\n",
    "reward_fn = partial(reward_func, chk_jcbn, names_km, cfg.reward.eig_partition, cfg.include_penalty)\n",
    "#reward_fn = compute_reward\n",
    "env = KineticEnv(cfg, reward_fn)\n",
    "env.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4939e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100 - Min reward: -88.0231, Max reward: 0.0827, Mean reward: -24.0900, Last reward: -30.7876\n",
      "Episode 1/100 - Policy loss: 0.6542, Value loss: 1.5549, Entropy: 353.5416\n",
      "Episode 2/100 - Min reward: -4.5620, Max reward: 0.1107, Mean reward: -0.6270, Last reward: 0.0781\n",
      "Episode 2/100 - Policy loss: 0.2823, Value loss: 1.5571, Entropy: 354.1682\n",
      "Best model saved at episode 2 with mean reward 0.0781\n",
      "Episode 3/100 - Min reward: -99.9990, Max reward: 0.0908, Mean reward: -36.3806, Last reward: -12.5978\n",
      "Episode 3/100 - Policy loss: 0.3643, Value loss: 4.4082, Entropy: 354.7667\n",
      "Episode 4/100 - Min reward: -34.6853, Max reward: 0.0786, Mean reward: -3.5901, Last reward: 0.0786\n",
      "Episode 4/100 - Policy loss: 0.1228, Value loss: 0.3119, Entropy: 355.3142\n",
      "Best model saved at episode 4 with mean reward 0.0786\n",
      "Episode 5/100 - Min reward: 0.0585, Max reward: 0.0804, Mean reward: 0.0772, Last reward: 0.0781\n",
      "Episode 5/100 - Policy loss: -0.0521, Value loss: 0.3915, Entropy: 355.7780\n",
      "Episode 6/100 - Min reward: -1.6308, Max reward: 0.3501, Mean reward: -0.1750, Last reward: 0.0667\n",
      "Episode 6/100 - Policy loss: -0.4028, Value loss: 1.6227, Entropy: 356.2012\n",
      "Episode 7/100 - Min reward: -85.5009, Max reward: 0.0804, Mean reward: -21.0694, Last reward: 0.0804\n",
      "Episode 7/100 - Policy loss: -0.8559, Value loss: 2.1704, Entropy: 356.4410\n",
      "Best model saved at episode 7 with mean reward 0.0804\n",
      "Episode 8/100 - Min reward: -99.9990, Max reward: 0.0964, Mean reward: -16.5962, Last reward: 0.0809\n",
      "Episode 8/100 - Policy loss: 0.1750, Value loss: 9.7683, Entropy: 356.7694\n",
      "Best model saved at episode 8 with mean reward 0.0809\n",
      "Episode 9/100 - Min reward: -99.9990, Max reward: 0.1500, Mean reward: -65.7218, Last reward: -39.9408\n",
      "Episode 9/100 - Policy loss: -0.2027, Value loss: 5.6465, Entropy: 357.1925\n",
      "Episode 10/100 - Min reward: -99.9990, Max reward: 0.2156, Mean reward: -7.9803, Last reward: 0.0777\n",
      "Episode 10/100 - Policy loss: -0.3472, Value loss: 0.8817, Entropy: 357.4984\n",
      "Episode 11/100 - Min reward: 0.0774, Max reward: 0.0860, Mean reward: 0.0781, Last reward: 0.0781\n",
      "Episode 11/100 - Policy loss: -0.0014, Value loss: 1.3257, Entropy: 357.8112\n",
      "Episode 12/100 - Min reward: -13.0990, Max reward: 0.2331, Mean reward: -1.4223, Last reward: 0.0781\n",
      "Episode 12/100 - Policy loss: 0.3455, Value loss: 0.1083, Entropy: 358.0993\n",
      "Episode 13/100 - Min reward: -99.9990, Max reward: 0.1169, Mean reward: -24.0889, Last reward: -94.7844\n",
      "Episode 13/100 - Policy loss: 0.0096, Value loss: 2.4380, Entropy: 358.3932\n",
      "Episode 14/100 - Min reward: -0.6380, Max reward: 0.0809, Mean reward: 0.0447, Last reward: 0.0782\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-962a7371d259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Update agent with combined trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_trajectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Episode {episode+1}/{cfg.training.num_episodes} - Policy loss: {policy_loss:.4f}, Value loss: {value_loss:.4f}, Entropy: {entropy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/helpers/ppo_agent.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, trajectory)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;31m# value loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb_returns\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/helpers/ppo_agent.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 190\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2345\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m         )\n\u001b[0;32m-> 2347\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize PPO agent (actor and critic)\n",
    "ppo_agent = PPOAgent(cfg, logger)\n",
    "\n",
    "log_rewards = []\n",
    "best_last_reward = 0\n",
    "best_actor_path = f\"{cfg.paths.output_dir}/run_4/best_actor.pth\"\n",
    "best_critic_path = f\"{cfg.paths.output_dir}/run_4/best_critic.pth\"\n",
    "\n",
    "for episode in range(cfg.training.num_episodes):\n",
    "    # Collect multiple trajectories per episode\n",
    "    trajectories = []\n",
    "    for _ in range(cfg.training.num_trajectories_per_episode):\n",
    "        trajectory = ppo_agent.collect_trajectory(env)\n",
    "        trajectories.append(trajectory)\n",
    "    \n",
    "    # Combine all trajectories\n",
    "    combined_trajectory = {\n",
    "        \"states\": torch.cat([t[\"states\"] for t in trajectories]),\n",
    "        \"actions\": torch.cat([t[\"actions\"] for t in trajectories]),\n",
    "        \"log_probs\": torch.cat([t[\"log_probs\"] for t in trajectories]),\n",
    "        \"values\": torch.cat([t[\"values\"] for t in trajectories]),\n",
    "        \"rewards\": torch.cat([t[\"rewards\"] for t in trajectories]),\n",
    "        \"dones\": torch.cat([t[\"dones\"] for t in trajectories]),\n",
    "    }\n",
    "    \n",
    "    # Calculate combined rewards statistics\n",
    "    rewards = combined_trajectory[\"rewards\"]\n",
    "    min_rew = rewards.min().item()\n",
    "    max_rew = rewards.max().item()\n",
    "    mean_rew = rewards.mean().item()\n",
    "    print(f\"Episode {episode+1}/{cfg.training.num_episodes} - Min reward: {min_rew:.4f}, Max reward: {max_rew:.4f}, Mean reward: {mean_rew:.4f}, Last reward: {rewards[-1]:.4f}\")\n",
    "    log_rewards.append(rewards[-1])\n",
    "\n",
    "    # Update agent with combined trajectory\n",
    "    policy_loss, value_loss, entropy = ppo_agent.update(combined_trajectory)\n",
    "    print(f\"Episode {episode+1}/{cfg.training.num_episodes} - Policy loss: {policy_loss:.4f}, Value loss: {value_loss:.4f}, Entropy: {entropy:.4f}\")\n",
    "\n",
    "    if rewards[-1] > best_last_reward:\n",
    "        best_last_reward = rewards[-1]\n",
    "        import os, torch\n",
    "        os.makedirs(cfg.paths.output_dir + \"/run_4/\", exist_ok=True)\n",
    "        torch.save(ppo_agent.policy_net.state_dict(), best_actor_path)\n",
    "        torch.save(ppo_agent.value_net.state_dict(), best_critic_path)\n",
    "        print(f\"Best model saved at episode {episode+1} with mean reward {best_last_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a64886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7d69f03d0978>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e27a648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7d69f1b18cf8>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(log_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b34cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def evaluate_agent_incidence(cfg, actor_path, threshold, num_samples=100, logger=None):\n",
    "    \"\"\"\n",
    "    Loads a pretrained agent and calculates the incidence rate.\n",
    "    Incidence = percentage of samples with reward above the threshold.\n",
    "\n",
    "    Args:\n",
    "        cfg: Hydra config object.\n",
    "        actor_path: Path to the saved actor/policy model.\n",
    "        threshold: Reward threshold for incidence.\n",
    "        num_samples: Number of samples to evaluate.\n",
    "        logger: Optional logger.\n",
    "\n",
    "    Returns:\n",
    "        incidence_rate: Fraction of samples with reward > threshold.\n",
    "        rewards: List of all sampled rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load agent and policy weights\n",
    "    agent = PPOAgent(cfg, logger)\n",
    "    agent.policy_net.load_state_dict(torch.load(actor_path, map_location=\"cpu\"))\n",
    "    agent.policy_net.eval()\n",
    "\n",
    "    count_above = 0\n",
    "    for i in range(num_samples):\n",
    "        trajectory = ppo_agent.collect_trajectory(env)\n",
    "        reward = trajectory['rewards'][-1]\n",
    "        if reward > threshold:\n",
    "            count_above += 1\n",
    "        print(f\"Sample {i+1}/{num_samples}: Reward = {reward:.4f}\")\n",
    "\n",
    "    incidence_rate = count_above / num_samples\n",
    "    print(f\"Incidence rate (reward > {threshold}): {incidence_rate:.4f} ({count_above}/{num_samples})\")\n",
    "    return incidence_rate, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c3be750",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b7bc6036bbce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mincidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_agent_incidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output/run_4/best_actor.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-1fa87d52d734>\u001b[0m in \u001b[0;36mevaluate_agent_incidence\u001b[0;34m(cfg, actor_path, threshold, num_samples, logger)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcount_above\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/helpers/ppo_agent.py\u001b[0m in \u001b[0;36mcollect_trajectory\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msample_reward\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dist_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/helpers/env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# compute black-box reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/helpers/utils.py\u001b[0m in \u001b[0;36mreward_func\u001b[0;34m(chk_jcbn, names_km, eig_partition, gen_kinetic_params)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Calculate the maximum eigenvalue of the Jacobian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0meig_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchk_jcbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_eigenvalues_recal_vmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0meig_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mmax_eig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meig_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/renaissance/kinetics/jacobian_solver.py\u001b[0m in \u001b[0;36mcalc_eigenvalues_recal_vmax\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mthis_real_eigenvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigenvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_jacobian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mstore_eigen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_real_eigenvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mthis_param_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameterValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mparam_pop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_param_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skimpy/core/parameters.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parameter_values, kmodel)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mparameter_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Check if this is a good solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skimpy/core/parameters.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mparameter_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Check if this is a good solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sympy/core/basic.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "incidence, all_rewards = evaluate_agent_incidence(cfg, \"output/run_4/best_actor.pth\", threshold=0.5, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "381920bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(f\"/home/renaissance/work/output/rewards.npy\", np.array(log_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80d480f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " 'data',\n",
       " 'logs',\n",
       " 'helpers',\n",
       " 'output',\n",
       " 'configs',\n",
       " 'Dockerfile',\n",
       " 'train.py',\n",
       " '.gitignore',\n",
       " 'README.md',\n",
       " 'notebooks',\n",
       " 'renaissance',\n",
       " 'start_jupyter_server.sh',\n",
       " 'poster',\n",
       " 'docker']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a293c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
