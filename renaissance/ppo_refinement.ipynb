{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d7635ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import os\n",
    "import helper as hp\n",
    "from configparser import ConfigParser\n",
    "from ppo_refinement import PPORefinement\n",
    "\n",
    "from kinetics.jacobian_solver import check_jacobian\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add5ed0",
   "metadata": {},
   "source": [
    "# Define PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f40e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=(64, 64)):\n",
    "        super(Actor, self).__init__()\n",
    "        self.hidden_dims = hidden_dims \n",
    "        layers = []\n",
    "        input_d = state_dim\n",
    "        for hidden_d in hidden_dims:\n",
    "            layers.append(nn.Linear(input_d, hidden_d))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_d = hidden_d\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.mean_layer = nn.Linear(hidden_dims[-1], action_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_dims[-1], action_dim)\n",
    "        # Initialize log_std_layer to start with reasonable variance\n",
    "        nn.init.constant_(self.log_std_layer.bias, -0.5)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.network(state)\n",
    "        mean = self.mean_layer(x)\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, -20, 2)  # Prevent numerical instability\n",
    "        std = torch.exp(log_std)\n",
    "        return mean, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dims=(64, 64)):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hidden_dims = hidden_dims \n",
    "        layers = []\n",
    "        input_d = state_dim\n",
    "        for hidden_d in hidden_dims:\n",
    "            layers.append(nn.Linear(input_d, hidden_d))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_d = hidden_d\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.value_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.network(state)\n",
    "        value = self.value_layer(x)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b040b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPORefinement:\n",
    "    def __init__(self, param_dim, latent_dim, min_x_bounds, max_x_bounds, \n",
    "                 names_km_full, chk_jcbn,\n",
    "                 actor_hidden_dims=(256, 512, 1024), critic_hidden_dims=(256, 512, 1024),\n",
    "                 p0_init_std=1, actor_lr=1e-4, critic_lr=1e-4, \n",
    "                 gamma=0.99, epsilon=0.2, gae_lambda=0.95,\n",
    "                 ppo_epochs=10, num_episodes_per_update=32, \n",
    "                 T_horizon=5, k_reward_steepness=1.0,\n",
    "                 action_clip_range=(-0.1, 0.1),\n",
    "                 entropy_coeff=0.01, max_grad_norm=0.5):\n",
    "        \n",
    "        self.param_dim = param_dim\n",
    "        self.latent_dim = latent_dim # For z in state\n",
    "        self.min_x_bounds = min_x_bounds\n",
    "        self.max_x_bounds = max_x_bounds\n",
    "        self.p0_init_std = p0_init_std\n",
    "\n",
    "        self.names_km_full = names_km_full \n",
    "        self.chk_jcbn = chk_jcbn # Store the jacobian checker instance\n",
    "\n",
    "        # State dim: p_t (param_dim) + z (latent_dim) + lambda_max (1) + t (1)\n",
    "        self.state_dim = self.param_dim + self.latent_dim + 1 + 1\n",
    "        self.action_dim = self.param_dim # Actor outputs updates to p_t\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, hidden_dims=actor_hidden_dims)\n",
    "        self.critic = Critic(self.state_dim, hidden_dims=critic_hidden_dims)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.num_episodes_per_update = num_episodes_per_update\n",
    "        self.T_horizon = T_horizon\n",
    "        self.k_reward_steepness = k_reward_steepness \n",
    "        self.action_clip_range = action_clip_range \n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.eig_partition_final_reward = -2.5 \n",
    "\n",
    "    def _get_lambda_max(self, p_tensor_single):\n",
    "        p_numpy = p_tensor_single.detach().cpu().numpy()\n",
    "        # Use the stored chk_jcbn instance\n",
    "        self.chk_jcbn._prepare_parameters([p_numpy], self.names_km_full) \n",
    "        max_eig_list = self.chk_jcbn.calc_eigenvalues_recal_vmax()\n",
    "\n",
    "        return max_eig_list[0] \n",
    "\n",
    "    def _compute_reward(self, lambda_max_val):\n",
    "        if lambda_max_val > 10:\n",
    "            return 0.0\n",
    "        intermediate_r = 1.0 / (1.0 + np.exp(self.k_reward_steepness * (lambda_max_val - (self.eig_partition_final_reward))))\n",
    "        # TODO: Right now, we are not using the Incidence part of the reward.\n",
    "\n",
    "        return intermediate_r\n",
    "\n",
    "    def _collect_trajectories(self):\n",
    "        batch_states = []\n",
    "        batch_actions = []\n",
    "        batch_log_probs_actions = []\n",
    "        batch_rewards = []\n",
    "        batch_next_states = []\n",
    "        batch_dones = []\n",
    "        \n",
    "        all_episode_total_rewards = []\n",
    "\n",
    "        for i_episode in range(self.num_episodes_per_update):\n",
    "            print(f\"Collecting episode data {i_episode + 1}/{self.num_episodes_per_update}...\")\n",
    "            # Episode initialization\n",
    "            # Generate p0: small random values, then clamp. No parameter fixing.\n",
    "            p_curr_np = np.random.normal(0, self.p0_init_std, size=self.param_dim)\n",
    "            p_curr_np = (p_curr_np - p_curr_np.min()) / (p_curr_np.max() - p_curr_np.min())  # Normalize to [0, 1]\n",
    "            p_curr_np = p_curr_np * (self.max_x_bounds - self.min_x_bounds) + self.min_x_bounds  # Scale to [min_x_bounds, max_x_bounds]\n",
    "            \n",
    "            # Generate z for state\n",
    "            z_curr_np = np.random.normal(0, 1, size=self.latent_dim)\n",
    "\n",
    "            p_curr_torch = torch.tensor(p_curr_np, dtype=torch.float32)\n",
    "            z_torch_ep = torch.tensor(z_curr_np, dtype=torch.float32)\n",
    "\n",
    "            episode_total_reward = 0\n",
    "\n",
    "            for t_s in range(self.T_horizon):\n",
    "                lambda_max_pt_val = self._get_lambda_max(p_curr_torch) \n",
    "                \n",
    "                state_torch_flat = torch.cat((\n",
    "                    p_curr_torch, z_torch_ep,\n",
    "                    torch.tensor([lambda_max_pt_val], dtype=torch.float32),\n",
    "                    torch.tensor([t_s], dtype=torch.float32)\n",
    "                ))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    action_mean, action_std = self.actor(state_torch_flat.unsqueeze(0))\n",
    "                    # Apply tanh and scale to [-0.1, 0.1]\n",
    "                    action_raw = action_mean\n",
    "                    action_scaled = torch.tanh(action_raw) * 0.1\n",
    "                    # Adjust log probability for tanh transformation\n",
    "                    dist = Normal(action_mean, action_std)\n",
    "                    action_raw_sample = dist.sample()\n",
    "                    log_prob_raw = dist.log_prob(action_raw_sample).sum(dim=-1)\n",
    "                    # Approximate log prob for tanh-transformed action\n",
    "                    action_scaled_sample = torch.tanh(action_raw_sample) * 0.1\n",
    "                    log_prob_scaled = log_prob_raw - torch.sum(2 * (torch.log(torch.tensor(2.0)) - action_raw_sample - torch.nn.functional.softplus(-2 * action_raw_sample)), dim=-1)\n",
    "\n",
    "                batch_states.append(state_torch_flat)\n",
    "                batch_actions.append(action_scaled_sample.squeeze(0))\n",
    "                batch_log_probs_actions.append(log_prob_scaled)\n",
    "\n",
    "                p_next_torch = p_curr_torch + action_scaled_sample.squeeze(0)\n",
    "                p_next_torch = torch.clamp(p_next_torch, self.min_x_bounds, self.max_x_bounds)\n",
    "                \n",
    "                lambda_max_p_next_val = self._get_lambda_max(p_next_torch)\n",
    "                is_final_step = (t_s == self.T_horizon - 1)\n",
    "                reward_val = self._compute_reward(lambda_max_p_next_val)\n",
    "\n",
    "                batch_rewards.append(torch.tensor([reward_val], dtype=torch.float32))\n",
    "                episode_total_reward += reward_val / self.T_horizon\n",
    "\n",
    "                next_state_torch_flat = torch.cat((\n",
    "                    p_next_torch, z_torch_ep,\n",
    "                    torch.tensor([lambda_max_p_next_val], dtype=torch.float32),\n",
    "                    torch.tensor([t_s + 1], dtype=torch.float32)\n",
    "                ))\n",
    "                batch_next_states.append(next_state_torch_flat)\n",
    "                batch_dones.append(torch.tensor([1.0 if is_final_step else 0.0], dtype=torch.float32))\n",
    "                p_curr_torch = p_next_torch\n",
    "            \n",
    "            all_episode_total_rewards.append(episode_total_reward)\n",
    "\n",
    "        final_batch_states = torch.stack(batch_states) if batch_states else torch.empty(0, self.state_dim)\n",
    "        final_batch_actions = torch.stack(batch_actions) if batch_actions else torch.empty(0, self.action_dim)\n",
    "        final_batch_log_probs = torch.stack(batch_log_probs_actions) if batch_log_probs_actions else torch.empty(0,1)\n",
    "        final_batch_rewards = torch.stack(batch_rewards) if batch_rewards else torch.empty(0,1)\n",
    "        final_batch_next_states = torch.stack(batch_next_states) if batch_next_states else torch.empty(0, self.state_dim)\n",
    "        final_batch_dones = torch.stack(batch_dones) if batch_dones else torch.empty(0,1)\n",
    "        \n",
    "        avg_episode_reward_val = np.mean(all_episode_total_rewards) if all_episode_total_rewards else 0\n",
    "\n",
    "        return (final_batch_states, final_batch_actions, final_batch_log_probs, \n",
    "                final_batch_rewards, final_batch_next_states, final_batch_dones,\n",
    "                avg_episode_reward_val)\n",
    "\n",
    "    def _compute_gae(self, rewards, values, next_values, dones):\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        last_advantage = 0\n",
    "        if rewards.nelement() == 0: \n",
    "             return torch.zeros_like(rewards), torch.zeros_like(values) \n",
    "\n",
    "        for t in reversed(range(len(rewards))): \n",
    "            is_terminal_transition = dones[t].item() > 0.5 \n",
    "            delta = rewards[t] + self.gamma * next_values[t] * (1.0 - dones[t]) - values[t]\n",
    "            advantages[t] = last_advantage = delta + self.gamma * self.gae_lambda * (1.0 - dones[t]) * last_advantage\n",
    "        returns = advantages + values\n",
    "        return advantages, returns\n",
    "\n",
    "    def update(self, trajectories_data):\n",
    "        states, actions, log_probs_old, rewards, next_states, dones, _ = trajectories_data\n",
    "        \n",
    "        if states.nelement() == 0: \n",
    "            return 0.0, 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            values = self.critic(states)          \n",
    "            next_values = self.critic(next_states) \n",
    "\n",
    "        advantages, returns = self._compute_gae(rewards, values, next_values, dones)\n",
    "        \n",
    "        if advantages.nelement() == 0: \n",
    "             return 0.0, 0.0\n",
    "\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        actor_total_loss_epoch = 0\n",
    "        critic_total_loss_epoch = 0\n",
    "\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            current_pi_mean, current_pi_std = self.actor(states)\n",
    "            dist_new = Normal(current_pi_mean, current_pi_std)\n",
    "            log_probs_new = dist_new.log_prob(actions).sum(dim=-1, keepdim=True)\n",
    "            entropy = dist_new.entropy().mean()\n",
    "\n",
    "            ratios = torch.exp(log_probs_new - log_probs_old.detach()) \n",
    "            \n",
    "            surr1 = ratios * advantages.detach() \n",
    "            surr2 = torch.clamp(ratios, 1.0 - self.epsilon, 1.0 + self.epsilon) * advantages.detach()\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - self.entropy_coeff * entropy\n",
    "\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "            self.actor_optimizer.step()\n",
    "            actor_total_loss_epoch += actor_loss.item()\n",
    "\n",
    "            values_pred = self.critic(states) \n",
    "            critic_loss = (returns.detach() - values_pred).pow(2).mean()\n",
    "\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "            self.critic_optimizer.step()\n",
    "            critic_total_loss_epoch += critic_loss.item()\n",
    "        \n",
    "        avg_actor_loss = actor_total_loss_epoch / self.ppo_epochs\n",
    "        avg_critic_loss = critic_total_loss_epoch / self.ppo_epochs\n",
    "        return avg_actor_loss, avg_critic_loss\n",
    "\n",
    "\n",
    "    def train(self, num_iterations, output_path_base=\"ppo_training_output\"):\n",
    "        import os\n",
    "        os.makedirs(output_path_base, exist_ok=True)\n",
    "        \n",
    "        all_iter_avg_rewards = []\n",
    "        best_avg_reward = float('-inf')  # Initialize to negative infinity\n",
    "        best_actor_path = os.path.join(output_path_base, \"best_actor.pth\")\n",
    "        best_critic_path = os.path.join(output_path_base, \"best_critic.pth\")\n",
    "        \n",
    "        print(f\"Starting PPO training for {num_iterations} iterations (serial execution).\") \n",
    "        print(f\"State dim: {self.state_dim}, Action dim: {self.action_dim}, Latent (z) dim: {self.latent_dim}\")\n",
    "        print(f\"Num episodes per update: {self.num_episodes_per_update}, Horizon T: {self.T_horizon}\")\n",
    "        print(f\"p0 initialized with N(0, {self.p0_init_std**2}) and clamped to [{self.min_x_bounds}, {self.max_x_bounds}]\")\n",
    "\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            trajectories_data = self._collect_trajectories()\n",
    "            avg_episode_reward = trajectories_data[-1] \n",
    "            \n",
    "            if trajectories_data[0].nelement() == 0 and self.num_episodes_per_update > 0:\n",
    "                print(f\"Iter {iteration:04d}: No trajectories collected. Skipping update. Avg Ep Reward: {avg_episode_reward:.4f}\")\n",
    "                all_iter_avg_rewards.append(avg_episode_reward) \n",
    "                continue \n",
    "\n",
    "            actor_loss, critic_loss = self.update(trajectories_data)\n",
    "            all_iter_avg_rewards.append(avg_episode_reward)\n",
    "        \n",
    "            print(f\"Iter {iteration:04d}: Avg Ep Reward: {avg_episode_reward:.4f}, \"\n",
    "                    f\"Actor Loss: {actor_loss:.4f}, Critic Loss: {critic_loss:.4f}\")\n",
    "            \n",
    "             # Save the best model if the current average reward is the highest\n",
    "            if avg_episode_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_episode_reward\n",
    "                torch.save(self.actor.state_dict(), best_actor_path)\n",
    "                torch.save(self.critic.state_dict(), best_critic_path)\n",
    "                print(f\"Iter {iteration:04d}: New best model saved with Avg Ep Reward: {best_avg_reward:.4f}\")\n",
    "\n",
    "            if iteration % 50 == 0 or iteration == num_iterations -1 : \n",
    "                actor_path = os.path.join(output_path_base, f\"actor_iter_{iteration}.pth\")\n",
    "                critic_path = os.path.join(output_path_base, f\"critic_iter_{iteration}.pth\")\n",
    "                torch.save(self.actor.state_dict(), actor_path)\n",
    "                torch.save(self.critic.state_dict(), critic_path)\n",
    "        \n",
    "        print(\"Training finished.\")\n",
    "        return all_iter_avg_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba82737",
   "metadata": {},
   "source": [
    "# Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70e0c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse arguments from configfile\n",
    "configs = ConfigParser()\n",
    "configs.read('configfile.ini')\n",
    "\n",
    "n_samples = int(configs['MLP']['n_samples']) # Used by MLP for its internal sampling if any, and for p0 generation.\n",
    "\n",
    "lnminkm = float(configs['CONSTRAINTS']['min_km'])\n",
    "lnmaxkm = float(configs['CONSTRAINTS']['max_km'])\n",
    "\n",
    "repeats = int(configs['EVOSTRAT']['repeats'])\n",
    "generations = int(configs['EVOSTRAT']['generations']) # Will be used as num_iterations for PPO\n",
    "ss_idx = int(configs['EVOSTRAT']['ss_idx'])\n",
    "# n_threads = int(configs['EVOSTRAT']['n_threads']) # PPO collection is currently single-threaded\n",
    "\n",
    "output_path = configs['PATHS']['output_path']\n",
    "met_model = configs['PATHS']['met_model']\n",
    "names_km_config = hp.load_pkl(f'models/{met_model}/parameter_names_km_fdp1.pkl') # Full list of param names\n",
    "\n",
    "# Parameters needed directly by PPORefinement\n",
    "param_dim_config = int(configs['MLP']['no_kms'])\n",
    "latent_dim_config = int(configs['MLP']['latent_dim']) # For z vector in state\n",
    "\n",
    "\n",
    "# Call solvers from SKimPy (Used only for initial messages now)\n",
    "chk_jcbn = check_jacobian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93349eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load kinetic and thermodynamic data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 15:05:55,234 - thermomodel_new - INFO - # Model initialized with units kcal/mol and temperature 298.15 K\n",
      "2025-05-13 15:05:56,678 - Unnamed - WARNING - Non integer stoichiometries found ['CYTBO3_4pp', 'LMPD_biomass_c_1_420', 'CYTBDpp'] change to integer for linear dependencies\n",
      "2025-05-13 15:05:56,971 - Unnamed - WARNING - Non integer stoichiometries found ['CYTBO3_4pp', 'LMPD_biomass_c_1_420', 'CYTBDpp'] change to integer for linear dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load steady state data\n"
     ]
    }
   ],
   "source": [
    "# Integrate data\n",
    "print('---- Load kinetic and thermodynamic data')\n",
    "chk_jcbn._load_ktmodels(met_model, 'fdp1')           ## Load kinetic and thermodynamic data\n",
    "print('---- Load steady state data')\n",
    "chk_jcbn._load_ssprofile(met_model, 'fdp1', ss_idx)  ## Integrate steady state information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f14b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Begin PPO refinement strategy\n",
      "Repeat 0: Starting PPO training for 25 iterations (serial execution).\n",
      "Starting PPO training for 25 iterations (serial execution).\n",
      "State dim: 485, Action dim: 384, Latent (z) dim: 99\n",
      "Num episodes per update: 32, Horizon T: 5\n",
      "p0 initialized with N(0, 1) and clamped to [-25.0, 3.0]\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0000: Avg Ep Reward: 0.1183, Actor Loss: 0.2776, Critic Loss: 0.0815\n",
      "Iter 0000: New best model saved with Avg Ep Reward: 0.1183\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0001: Avg Ep Reward: 0.0860, Actor Loss: 0.2564, Critic Loss: 0.0614\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0002: Avg Ep Reward: 0.1288, Actor Loss: 0.2786, Critic Loss: 0.0639\n",
      "Iter 0002: New best model saved with Avg Ep Reward: 0.1288\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0003: Avg Ep Reward: 0.1291, Actor Loss: 0.2298, Critic Loss: 0.1462\n",
      "Iter 0003: New best model saved with Avg Ep Reward: 0.1291\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0004: Avg Ep Reward: 0.0701, Actor Loss: 0.2686, Critic Loss: 0.0542\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0005: Avg Ep Reward: 0.1282, Actor Loss: 0.2724, Critic Loss: 0.1149\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0006: Avg Ep Reward: 0.1198, Actor Loss: 0.2197, Critic Loss: 0.3602\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0007: Avg Ep Reward: 0.0957, Actor Loss: 0.2680, Critic Loss: 0.0454\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0008: Avg Ep Reward: 0.0817, Actor Loss: 0.2656, Critic Loss: 0.1090\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0009: Avg Ep Reward: 0.1249, Actor Loss: 0.2108, Critic Loss: 0.0909\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n",
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n",
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n",
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n"
     ]
    }
   ],
   "source": [
    "print('--- Begin PPO refinement strategy')\n",
    "for rep in range(repeats):\n",
    "    this_savepath = f'{output_path}/ppo_repeat_{rep}/' \n",
    "    os.makedirs(this_savepath, exist_ok=True)\n",
    "\n",
    "    # Instantiate PPORefinement agent with direct parameters for serial execution\n",
    "    ppo_agent = PPORefinement(\n",
    "        param_dim=param_dim_config,\n",
    "        latent_dim=latent_dim_config,\n",
    "        min_x_bounds=lnminkm,\n",
    "        max_x_bounds=lnmaxkm,\n",
    "        names_km_full=names_km_config, # Pass the full list of names\n",
    "        chk_jcbn=chk_jcbn,             # Pass the jacobian checker instance\n",
    "        p0_init_std=1, # Default is 0.01\n",
    "        ppo_epochs=30,\n",
    "    )\n",
    "    \n",
    "    print(f\"Repeat {rep}: Starting PPO training for {generations} iterations (serial execution).\")\n",
    "    ppo_iteration_rewards = ppo_agent.train(\n",
    "        num_iterations=generations, # Use 'generations' from config as PPO iterations\n",
    "        output_path_base=this_savepath\n",
    "    )\n",
    "    \n",
    "    hp.save_pkl(f'{this_savepath}/ppo_iteration_rewards.pkl', ppo_iteration_rewards)\n",
    "    print(f\"Repeat {rep}: PPO training finished. Rewards log saved to {this_savepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd8a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_incidence(ppo_instance, actor_path, num_trials=1):\n",
    "    \"\"\"\n",
    "    Evaluate the policy incidence using a pre-trained actor model.\n",
    "\n",
    "    Args:\n",
    "        ppo_instance: The PPORefinement instance.\n",
    "        actor_path: Path to the pre-trained actor model (best_actor.pth).\n",
    "        num_trials: Number of trials to evaluate the policy incidence.\n",
    "\n",
    "    Returns:\n",
    "        incidence_rate: The rate of valid models.\n",
    "        all_final_params: List of final parameters for valid models.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained actor model\n",
    "    ppo_instance.actor.load_state_dict(torch.load(actor_path))\n",
    "    ppo_instance.actor.eval()  # Set to evaluation mode\n",
    "\n",
    "    valid_count = 0\n",
    "    all_final_params = []\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        with torch.no_grad():\n",
    "            # Sample initial p0\n",
    "            p0_np = np.random.normal(0, ppo_instance.p0_init_std, size=ppo_instance.param_dim)\n",
    "            p0_np = (p0_np - p0_np.min()) / (p0_np.max() - p0_np.min())  # Normalize\n",
    "            p0_np = p0_np * (ppo_instance.max_x_bounds - ppo_instance.min_x_bounds) + ppo_instance.min_x_bounds\n",
    "            p_curr = torch.tensor(p0_np, dtype=torch.float32)\n",
    "\n",
    "            # Sample latent z\n",
    "            z = torch.tensor(np.random.normal(0, 1, size=ppo_instance.latent_dim), dtype=torch.float32)\n",
    "\n",
    "            for t_s in range(ppo_instance.T_horizon):\n",
    "                lambda_max_val = ppo_instance._get_lambda_max(p_curr)\n",
    "\n",
    "                state_torch = torch.cat((\n",
    "                    p_curr,\n",
    "                    z,\n",
    "                    torch.tensor([lambda_max_val], dtype=torch.float32),\n",
    "                    torch.tensor([t_s], dtype=torch.float32)\n",
    "                )).unsqueeze(0)\n",
    "\n",
    "                action_mean, action_std = ppo_instance.actor(state_torch)\n",
    "                dist = Normal(action_mean, action_std)\n",
    "                action = dist.sample()\n",
    "                action_clipped = torch.clamp(action.squeeze(0), ppo_instance.action_clip_range[0], ppo_instance.action_clip_range[1])\n",
    "                p_next = p_curr + action_clipped\n",
    "                p_next = torch.clamp(p_next, ppo_instance.min_x_bounds, ppo_instance.max_x_bounds)\n",
    "                p_curr = p_next\n",
    "\n",
    "            # Final p_curr after T steps\n",
    "            p_final_np = p_curr.detach().cpu().numpy()\n",
    "            ppo_instance.chk_jcbn._prepare_parameters([p_final_np], ppo_instance.names_km_full)\n",
    "\n",
    "            # max eigenvalue check\n",
    "            max_eig_list = ppo_instance.chk_jcbn.calc_eigenvalues_recal_vmax()\n",
    "            max_eig_val = max_eig_list[0]\n",
    "            print(max_eig_val)\n",
    "            is_valid = max_eig_val <= ppo_instance.eig_partition_final_reward\n",
    "\n",
    "            if is_valid:\n",
    "                valid_count += 1\n",
    "                all_final_params.append(p_final_np)\n",
    "\n",
    "    incidence_rate = valid_count / num_trials\n",
    "    print(f\"Incidence Rate (valid models): {incidence_rate:.4f} ({valid_count}/{num_trials})\")\n",
    "    return incidence_rate, all_final_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd129feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1094859447835708\n",
      "693.8429045727567\n",
      "-0.7188792773374121\n",
      "-1.087011491051713\n",
      "-0.4660001322079541\n",
      "-0.09650168805497428\n",
      "0.0570942819642999\n",
      "493.7787126633135\n",
      "0.10352312971380022\n",
      "4107.429776038251\n",
      "-1.4611242606177535\n",
      "-0.029828805024414515\n",
      "18.229720110554567\n",
      "347.22758142852\n",
      "580.2181280279999\n",
      "1.7965730831922282\n",
      "-0.002972771487642462\n",
      "84.29114134136417\n",
      "-1.252518466006712\n",
      "-1.4745974714918415\n",
      "323.2945142159184\n",
      "6.0634480922644265\n",
      "-0.29035366458418005\n",
      "53.87407357964971\n",
      "-1.1611409220517013\n",
      "722.5945895078701\n",
      "-1.0968019908923656\n",
      "-0.32074767960036954\n",
      "-0.31342187776613833\n",
      "0.9099709750530822\n",
      "-0.031142654685491666\n",
      "-1.0979984471673543\n",
      "16.862519169906655\n",
      "108.51177182248902\n",
      "11600.192183321475\n",
      "-1.4944925325402123\n",
      "-1.0777213937384698\n",
      "1.6950832560325275\n",
      "-0.6136174463503431\n",
      "-0.41964998294082495\n",
      "35.859567448218385\n",
      "-0.06897447595043207\n",
      "-0.29450676777311025\n",
      "-0.3521065808185181\n",
      "-2.3755775817367413\n",
      "-0.24907796858181183\n",
      "648.5068654503849\n",
      "-0.8842526309040254\n",
      "-0.059655216690691466\n",
      "-0.18699369525520068\n",
      "Incidence Rate (valid models): 0.0000 (0/50)\n"
     ]
    }
   ],
   "source": [
    "incidence_rate, all_final_params = evaluate_policy_incidence(\n",
    "    ppo_agent, \n",
    "    actor_path=os.getcwd() + \"/output/ppo-refinement/ppo_repeat_0/best_actor.pth\", \n",
    "    num_trials=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76daa3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
