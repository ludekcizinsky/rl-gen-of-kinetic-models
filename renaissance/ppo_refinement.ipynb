{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7635ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import os\n",
    "import helper as hp\n",
    "from configparser import ConfigParser\n",
    "from ppo_refinement import PPORefinement\n",
    "\n",
    "from kinetics.jacobian_solver import check_jacobian\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add5ed0",
   "metadata": {},
   "source": [
    "# Define PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f40e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=(256, 512, 1024)):\n",
    "        super(Actor, self).__init__()\n",
    "        self.hidden_dims = hidden_dims \n",
    "        layers = []\n",
    "        input_d = state_dim\n",
    "        for hidden_d in hidden_dims:\n",
    "            layers.append(nn.Linear(input_d, hidden_d))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_d = hidden_d\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.mean_layer = nn.Linear(hidden_dims[-1], action_dim)\n",
    "        self.log_std = nn.Parameter(torch.full((action_dim,), -0.5, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.network(state)\n",
    "        mean = self.mean_layer(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        if mean.ndim > 1 and std.ndim == 1 and std.shape[0] == mean.shape[1]:\n",
    "             std = std.unsqueeze(0).expand_as(mean)\n",
    "        return mean, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dims=(256, 512, 1024)):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hidden_dims = hidden_dims \n",
    "        layers = []\n",
    "        input_d = state_dim\n",
    "        for hidden_d in hidden_dims:\n",
    "            layers.append(nn.Linear(input_d, hidden_d))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_d = hidden_d\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.value_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.network(state)\n",
    "        value = self.value_layer(x)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b040b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPORefinement:\n",
    "    def __init__(self, param_dim, latent_dim, min_x_bounds, max_x_bounds, \n",
    "                 names_km_full, chk_jcbn,\n",
    "                 actor_hidden_dims=(256, 512, 1024), critic_hidden_dims=(256, 512, 1024),\n",
    "                 p0_init_std=1, actor_lr=1e-4, critic_lr=1e-4, \n",
    "                 gamma=0.99, epsilon=0.2, gae_lambda=0.95,\n",
    "                 ppo_epochs=10, num_episodes_per_update=32, \n",
    "                 T_horizon=5, k_reward_steepness=1.0,\n",
    "                 action_clip_range=(-0.1, 0.1),\n",
    "                 entropy_coeff=0.01, max_grad_norm=0.5):\n",
    "        \n",
    "        self.param_dim = param_dim\n",
    "        self.latent_dim = latent_dim # For z in state\n",
    "        self.min_x_bounds = min_x_bounds\n",
    "        self.max_x_bounds = max_x_bounds\n",
    "        self.p0_init_std = p0_init_std\n",
    "\n",
    "        self.names_km_full = names_km_full \n",
    "        self.chk_jcbn = chk_jcbn # Store the jacobian checker instance\n",
    "\n",
    "        # State dim: p_t (param_dim) + z (latent_dim) + lambda_max (1) + t (1)\n",
    "        self.state_dim = self.param_dim + self.latent_dim + 1 + 1\n",
    "        self.action_dim = self.param_dim # Actor outputs updates to p_t\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, hidden_dims=actor_hidden_dims)\n",
    "        self.critic = Critic(self.state_dim, hidden_dims=critic_hidden_dims)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.num_episodes_per_update = num_episodes_per_update\n",
    "        self.T_horizon = T_horizon\n",
    "        self.k_reward_steepness = k_reward_steepness \n",
    "        self.action_clip_range = action_clip_range \n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.eig_partition_final_reward = -2.5 \n",
    "\n",
    "    def _get_lambda_max(self, p_tensor_single):\n",
    "        p_numpy = p_tensor_single.detach().cpu().numpy()\n",
    "        # Use the stored chk_jcbn instance\n",
    "        self.chk_jcbn._prepare_parameters([p_numpy], self.names_km_full) \n",
    "        max_eig_list = self.chk_jcbn.calc_eigenvalues_recal_vmax()\n",
    "\n",
    "        return max_eig_list[0] \n",
    "\n",
    "    def _compute_reward(self, lambda_max_val):\n",
    "        intermediate_r = 1.0 / (1.0 + np.exp(self.k_reward_steepness * (lambda_max_val - (self.eig_partition_final_reward))))\n",
    "        # TODO: Right now, we are not using the Incidence part of the reward.\n",
    "\n",
    "        return intermediate_r\n",
    "\n",
    "    def _collect_trajectories(self):\n",
    "        batch_states = []\n",
    "        batch_actions = []\n",
    "        batch_log_probs_actions = []\n",
    "        batch_rewards = []\n",
    "        batch_next_states = []\n",
    "        batch_dones = []\n",
    "        \n",
    "        all_episode_total_rewards = []\n",
    "\n",
    "        for i_episode in range(self.num_episodes_per_update):\n",
    "            print(f\"Collecting episode data {i_episode + 1}/{self.num_episodes_per_update}...\")\n",
    "            # Episode initialization\n",
    "            # Generate p0: small random values, then clamp. No parameter fixing.\n",
    "            p_curr_np = np.random.normal(0, self.p0_init_std, size=self.param_dim)\n",
    "            p_curr_np = (p_curr_np - p_curr_np.min()) / (p_curr_np.max() - p_curr_np.min())  # Normalize to [0, 1]\n",
    "            p_curr_np = p_curr_np * (self.max_x_bounds - self.min_x_bounds) + self.min_x_bounds  # Scale to [min_x_bounds, max_x_bounds]\n",
    "            \n",
    "            # Generate z for state\n",
    "            z_curr_np = np.random.normal(0, 1, size=self.latent_dim)\n",
    "\n",
    "            p_curr_torch = torch.tensor(p_curr_np, dtype=torch.float32)\n",
    "            z_torch_ep = torch.tensor(z_curr_np, dtype=torch.float32)\n",
    "\n",
    "            episode_total_reward = 0\n",
    "\n",
    "            for t_s in range(self.T_horizon):\n",
    "                # Use instance methods for lambda_max and reward\n",
    "                lambda_max_pt_val = self._get_lambda_max(p_curr_torch) \n",
    "                \n",
    "                state_torch_flat = torch.cat((\n",
    "                    p_curr_torch, z_torch_ep,\n",
    "                    torch.tensor([lambda_max_pt_val], dtype=torch.float32),\n",
    "                    torch.tensor([t_s], dtype=torch.float32)\n",
    "                ))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    action_mean, action_std = self.actor(state_torch_flat.unsqueeze(0))\n",
    "                    dist = Normal(action_mean, action_std)\n",
    "                    action = dist.sample()\n",
    "                    log_prob_action = dist.log_prob(action).sum(dim=-1)\n",
    "\n",
    "                batch_states.append(state_torch_flat)\n",
    "                batch_actions.append(action.squeeze(0))\n",
    "                batch_log_probs_actions.append(log_prob_action)\n",
    "\n",
    "                action_clipped = torch.clamp(action.squeeze(0), self.action_clip_range[0], self.action_clip_range[1])\n",
    "                p_next_torch = p_curr_torch + action_clipped\n",
    "                p_next_torch = torch.clamp(p_next_torch, self.min_x_bounds, self.max_x_bounds)\n",
    "                \n",
    "                lambda_max_p_next_val = self._get_lambda_max(p_next_torch)\n",
    "                is_final_step = (t_s == self.T_horizon - 1)\n",
    "                # print(lambda_max_p_next_val) : for DEBUG\n",
    "                reward_val = self._compute_reward(lambda_max_p_next_val)\n",
    "\n",
    "                batch_rewards.append(torch.tensor([reward_val], dtype=torch.float32))\n",
    "                episode_total_reward += reward_val\n",
    "\n",
    "                next_state_torch_flat = torch.cat((\n",
    "                    p_next_torch, z_torch_ep,\n",
    "                    torch.tensor([lambda_max_p_next_val], dtype=torch.float32),\n",
    "                    torch.tensor([t_s + 1], dtype=torch.float32)\n",
    "                ))\n",
    "                batch_next_states.append(next_state_torch_flat)\n",
    "                batch_dones.append(torch.tensor([1.0 if is_final_step else 0.0], dtype=torch.float32))\n",
    "                p_curr_torch = p_next_torch\n",
    "            \n",
    "            all_episode_total_rewards.append(episode_total_reward)\n",
    "\n",
    "        # Concatenate collected data into batch tensors\n",
    "        final_batch_states = torch.stack(batch_states) if batch_states else torch.empty(0, self.state_dim)\n",
    "        final_batch_actions = torch.stack(batch_actions) if batch_actions else torch.empty(0, self.action_dim)\n",
    "        final_batch_log_probs = torch.stack(batch_log_probs_actions) if batch_log_probs_actions else torch.empty(0,1)\n",
    "        final_batch_rewards = torch.stack(batch_rewards) if batch_rewards else torch.empty(0,1)\n",
    "        final_batch_next_states = torch.stack(batch_next_states) if batch_next_states else torch.empty(0, self.state_dim)\n",
    "        final_batch_dones = torch.stack(batch_dones) if batch_dones else torch.empty(0,1)\n",
    "        \n",
    "        avg_episode_reward_val = np.mean(all_episode_total_rewards) if all_episode_total_rewards else 0\n",
    "\n",
    "        return (final_batch_states, final_batch_actions, final_batch_log_probs, \n",
    "                final_batch_rewards, final_batch_next_states, final_batch_dones,\n",
    "                avg_episode_reward_val)\n",
    "\n",
    "    def _compute_gae(self, rewards, values, next_values, dones):\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        last_advantage = 0\n",
    "        if rewards.nelement() == 0: \n",
    "             return torch.zeros_like(rewards), torch.zeros_like(values) \n",
    "\n",
    "        for t in reversed(range(len(rewards))): \n",
    "            is_terminal_transition = dones[t].item() > 0.5 \n",
    "            delta = rewards[t] + self.gamma * next_values[t] * (1.0 - dones[t]) - values[t]\n",
    "            advantages[t] = last_advantage = delta + self.gamma * self.gae_lambda * (1.0 - dones[t]) * last_advantage\n",
    "        returns = advantages + values\n",
    "        return advantages, returns\n",
    "\n",
    "    def update(self, trajectories_data):\n",
    "        states, actions, log_probs_old, rewards, next_states, dones, _ = trajectories_data\n",
    "        \n",
    "        if states.nelement() == 0: \n",
    "            return 0.0, 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            values = self.critic(states)          \n",
    "            next_values = self.critic(next_states) \n",
    "\n",
    "        advantages, returns = self._compute_gae(rewards, values, next_values, dones)\n",
    "        \n",
    "        if advantages.nelement() == 0: \n",
    "             return 0.0, 0.0\n",
    "\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        actor_total_loss_epoch = 0\n",
    "        critic_total_loss_epoch = 0\n",
    "\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            current_pi_mean, current_pi_std = self.actor(states)\n",
    "            dist_new = Normal(current_pi_mean, current_pi_std)\n",
    "            log_probs_new = dist_new.log_prob(actions).sum(dim=-1, keepdim=True)\n",
    "            entropy = dist_new.entropy().mean()\n",
    "\n",
    "            ratios = torch.exp(log_probs_new - log_probs_old.detach()) \n",
    "            \n",
    "            surr1 = ratios * advantages.detach() \n",
    "            surr2 = torch.clamp(ratios, 1.0 - self.epsilon, 1.0 + self.epsilon) * advantages.detach()\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - self.entropy_coeff * entropy\n",
    "\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "            self.actor_optimizer.step()\n",
    "            actor_total_loss_epoch += actor_loss.item()\n",
    "\n",
    "            values_pred = self.critic(states) \n",
    "            critic_loss = (returns.detach() - values_pred).pow(2).mean()\n",
    "\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "            self.critic_optimizer.step()\n",
    "            critic_total_loss_epoch += critic_loss.item()\n",
    "        \n",
    "        avg_actor_loss = actor_total_loss_epoch / self.ppo_epochs\n",
    "        avg_critic_loss = critic_total_loss_epoch / self.ppo_epochs\n",
    "        return avg_actor_loss, avg_critic_loss\n",
    "\n",
    "\n",
    "    def train(self, num_iterations, output_path_base=\"ppo_training_output\"):\n",
    "        import os\n",
    "        os.makedirs(output_path_base, exist_ok=True)\n",
    "        \n",
    "        all_iter_avg_rewards = []\n",
    "        best_avg_reward = float('-inf')  # Initialize to negative infinity\n",
    "        best_actor_path = os.path.join(output_path_base, \"best_actor.pth\")\n",
    "        best_critic_path = os.path.join(output_path_base, \"best_critic.pth\")\n",
    "        \n",
    "        print(f\"Starting PPO training for {num_iterations} iterations (serial execution).\") \n",
    "        print(f\"State dim: {self.state_dim}, Action dim: {self.action_dim}, Latent (z) dim: {self.latent_dim}\")\n",
    "        print(f\"Num episodes per update: {self.num_episodes_per_update}, Horizon T: {self.T_horizon}\")\n",
    "        print(f\"p0 initialized with N(0, {self.p0_init_std**2}) and clamped to [{self.min_x_bounds}, {self.max_x_bounds}]\")\n",
    "\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            trajectories_data = self._collect_trajectories()\n",
    "            avg_episode_reward = trajectories_data[-1] \n",
    "            \n",
    "            if trajectories_data[0].nelement() == 0 and self.num_episodes_per_update > 0:\n",
    "                print(f\"Iter {iteration:04d}: No trajectories collected. Skipping update. Avg Ep Reward: {avg_episode_reward:.4f}\")\n",
    "                all_iter_avg_rewards.append(avg_episode_reward) \n",
    "                continue \n",
    "\n",
    "            actor_loss, critic_loss = self.update(trajectories_data)\n",
    "            all_iter_avg_rewards.append(avg_episode_reward)\n",
    "        \n",
    "            print(f\"Iter {iteration:04d}: Avg Ep Reward: {avg_episode_reward:.4f}, \"\n",
    "                    f\"Actor Loss: {actor_loss:.4f}, Critic Loss: {critic_loss:.4f}\")\n",
    "            \n",
    "             # Save the best model if the current average reward is the highest\n",
    "            if avg_episode_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_episode_reward\n",
    "                torch.save(self.actor.state_dict(), best_actor_path)\n",
    "                torch.save(self.critic.state_dict(), best_critic_path)\n",
    "                print(f\"Iter {iteration:04d}: New best model saved with Avg Ep Reward: {best_avg_reward:.4f}\")\n",
    "\n",
    "            if iteration % 50 == 0 or iteration == num_iterations -1 : \n",
    "                actor_path = os.path.join(output_path_base, f\"actor_iter_{iteration}.pth\")\n",
    "                critic_path = os.path.join(output_path_base, f\"critic_iter_{iteration}.pth\")\n",
    "                torch.save(self.actor.state_dict(), actor_path)\n",
    "                torch.save(self.critic.state_dict(), critic_path)\n",
    "        \n",
    "        print(\"Training finished.\")\n",
    "        return all_iter_avg_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba82737",
   "metadata": {},
   "source": [
    "# Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e0c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse arguments from configfile\n",
    "configs = ConfigParser()\n",
    "configs.read('configfile.ini')\n",
    "\n",
    "n_samples = int(configs['MLP']['n_samples']) # Used by MLP for its internal sampling if any, and for p0 generation.\n",
    "\n",
    "lnminkm = float(configs['CONSTRAINTS']['min_km'])\n",
    "lnmaxkm = float(configs['CONSTRAINTS']['max_km'])\n",
    "\n",
    "repeats = int(configs['EVOSTRAT']['repeats'])\n",
    "generations = int(configs['EVOSTRAT']['generations']) # Will be used as num_iterations for PPO\n",
    "ss_idx = int(configs['EVOSTRAT']['ss_idx'])\n",
    "# n_threads = int(configs['EVOSTRAT']['n_threads']) # PPO collection is currently single-threaded\n",
    "\n",
    "output_path = configs['PATHS']['output_path']\n",
    "met_model = configs['PATHS']['met_model']\n",
    "names_km_config = hp.load_pkl(f'models/{met_model}/parameter_names_km_fdp1.pkl') # Full list of param names\n",
    "\n",
    "# Parameters needed directly by PPORefinement\n",
    "param_dim_config = int(configs['MLP']['no_kms'])\n",
    "latent_dim_config = int(configs['MLP']['latent_dim']) # For z vector in state\n",
    "\n",
    "\n",
    "# Call solvers from SKimPy (Used only for initial messages now)\n",
    "chk_jcbn = check_jacobian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93349eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load kinetic and thermodynamic data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 14:09:48,712 - thermomodel_new - INFO - # Model initialized with units kcal/mol and temperature 298.15 K\n",
      "2025-05-13 14:09:50,191 - Unnamed - WARNING - Non integer stoichiometries found ['CYTBO3_4pp', 'LMPD_biomass_c_1_420', 'CYTBDpp'] change to integer for linear dependencies\n",
      "2025-05-13 14:09:50,480 - Unnamed - WARNING - Non integer stoichiometries found ['CYTBO3_4pp', 'LMPD_biomass_c_1_420', 'CYTBDpp'] change to integer for linear dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load steady state data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-30:\n",
      "Process ForkPoolWorker-32:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-28:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-27:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-23:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Integrate data\n",
    "print('---- Load kinetic and thermodynamic data')\n",
    "chk_jcbn._load_ktmodels(met_model, 'fdp1')           ## Load kinetic and thermodynamic data\n",
    "print('---- Load steady state data')\n",
    "chk_jcbn._load_ssprofile(met_model, 'fdp1', ss_idx)  ## Integrate steady state information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7f14b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Begin PPO refinement strategy\n",
      "Repeat 0: Starting PPO training for 10 iterations (serial execution).\n",
      "Starting PPO training for 10 iterations (serial execution).\n",
      "State dim: 485, Action dim: 384, Latent (z) dim: 99\n",
      "Num episodes per update: 32, Horizon T: 5\n",
      "p0 initialized with N(0, 1) and clamped to [-25.0, 3.0]\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting episode data 3/32...\n",
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n",
      "Collecting episode data 8/32...\n",
      "Collecting episode data 9/32...\n",
      "Collecting episode data 10/32...\n",
      "Collecting episode data 11/32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting episode data 12/32...\n",
      "Collecting episode data 13/32...\n",
      "Collecting episode data 14/32...\n",
      "Collecting episode data 15/32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting episode data 16/32...\n",
      "Collecting episode data 17/32...\n",
      "Collecting episode data 18/32...\n",
      "Collecting episode data 19/32...\n",
      "Collecting episode data 20/32...\n",
      "Collecting episode data 21/32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting episode data 22/32...\n",
      "Collecting episode data 23/32...\n",
      "Collecting episode data 24/32...\n",
      "Collecting episode data 25/32...\n",
      "Collecting episode data 26/32...\n",
      "Collecting episode data 27/32...\n",
      "Collecting episode data 28/32...\n",
      "Collecting episode data 29/32...\n",
      "Collecting episode data 30/32...\n",
      "Collecting episode data 31/32...\n",
      "Collecting episode data 32/32...\n",
      "Iter 0000: Avg Ep Reward: 0.5282, Actor Loss: -0.0250, Critic Loss: 0.7348\n",
      "Iter 0000: New best model saved with Avg Ep Reward: 0.5282\n",
      "Collecting episode data 1/32...\n",
      "Collecting episode data 2/32...\n",
      "Collecting episode data 3/32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:52 \u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting episode data 4/32...\n",
      "Collecting episode data 5/32...\n",
      "Collecting episode data 6/32...\n",
      "Collecting episode data 7/32...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-91a927b64582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     ppo_iteration_rewards = ppo_agent.train(\n\u001b[1;32m     19\u001b[0m         \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Use 'generations' from config as PPO iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput_path_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthis_savepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6633af91a2f3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_iterations, output_path_base)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mtrajectories_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collect_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mavg_episode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrajectories_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6633af91a2f3>\u001b[0m in \u001b[0;36m_collect_trajectories\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mp_next_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_next_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_x_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_x_bounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mlambda_max_p_next_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lambda_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_next_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mis_final_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_s\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_horizon\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;31m# print(lambda_max_p_next_val) : for DEBUG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6633af91a2f3>\u001b[0m in \u001b[0;36m_get_lambda_max\u001b[0;34m(self, p_tensor_single)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Use the stored chk_jcbn instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchk_jcbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_numpy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames_km_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mmax_eig_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchk_jcbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_eigenvalues_recal_vmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax_eig_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/kinetics/jacobian_solver.py\u001b[0m in \u001b[0;36mcalc_eigenvalues_recal_vmax\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconc_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_param\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0mc_sym\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_sym\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconc_series\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skimpy/core/kinmodel.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthis_reaction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mreaction_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis_reaction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreaction_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/skimpy/core/kinmodel.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthis_reaction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mreaction_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis_reaction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreaction_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sympy/core/basic.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('--- Begin PPO refinement strategy')\n",
    "for rep in range(repeats):\n",
    "    this_savepath = f'{output_path}/ppo_repeat_{rep}/' \n",
    "    os.makedirs(this_savepath, exist_ok=True)\n",
    "\n",
    "    # Instantiate PPORefinement agent with direct parameters for serial execution\n",
    "    ppo_agent = PPORefinement(\n",
    "        param_dim=param_dim_config,\n",
    "        latent_dim=latent_dim_config,\n",
    "        min_x_bounds=lnminkm,\n",
    "        max_x_bounds=lnmaxkm,\n",
    "        names_km_full=names_km_config, # Pass the full list of names\n",
    "        chk_jcbn=chk_jcbn,             # Pass the jacobian checker instance\n",
    "        p0_init_std=1, # Default is 0.01\n",
    "    )\n",
    "    \n",
    "    print(f\"Repeat {rep}: Starting PPO training for {generations} iterations (serial execution).\")\n",
    "    ppo_iteration_rewards = ppo_agent.train(\n",
    "        num_iterations=generations, # Use 'generations' from config as PPO iterations\n",
    "        output_path_base=this_savepath\n",
    "    )\n",
    "    \n",
    "    hp.save_pkl(f'{this_savepath}/ppo_iteration_rewards.pkl', ppo_iteration_rewards)\n",
    "    print(f\"Repeat {rep}: PPO training finished. Rewards log saved to {this_savepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd8a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_incidence(ppo_instance, actor_path, num_trials=1):\n",
    "    \"\"\"\n",
    "    Evaluate the policy incidence using a pre-trained actor model.\n",
    "\n",
    "    Args:\n",
    "        ppo_instance: The PPORefinement instance.\n",
    "        actor_path: Path to the pre-trained actor model (best_actor.pth).\n",
    "        num_trials: Number of trials to evaluate the policy incidence.\n",
    "\n",
    "    Returns:\n",
    "        incidence_rate: The rate of valid models.\n",
    "        all_final_params: List of final parameters for valid models.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained actor model\n",
    "    ppo_instance.actor.load_state_dict(torch.load(actor_path))\n",
    "    ppo_instance.actor.eval()  # Set to evaluation mode\n",
    "\n",
    "    valid_count = 0\n",
    "    all_final_params = []\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        with torch.no_grad():\n",
    "            # Sample initial p0\n",
    "            p0_np = np.random.normal(0, ppo_instance.p0_init_std, size=ppo_instance.param_dim)\n",
    "            p0_np = (p0_np - p0_np.min()) / (p0_np.max() - p0_np.min())  # Normalize\n",
    "            p0_np = p0_np * (ppo_instance.max_x_bounds - ppo_instance.min_x_bounds) + ppo_instance.min_x_bounds\n",
    "            p_curr = torch.tensor(p0_np, dtype=torch.float32)\n",
    "\n",
    "            # Sample latent z\n",
    "            z = torch.tensor(np.random.normal(0, 1, size=ppo_instance.latent_dim), dtype=torch.float32)\n",
    "\n",
    "            for t_s in range(ppo_instance.T_horizon):\n",
    "                lambda_max_val = ppo_instance._get_lambda_max(p_curr)\n",
    "\n",
    "                state_torch = torch.cat((\n",
    "                    p_curr,\n",
    "                    z,\n",
    "                    torch.tensor([lambda_max_val], dtype=torch.float32),\n",
    "                    torch.tensor([t_s], dtype=torch.float32)\n",
    "                )).unsqueeze(0)\n",
    "\n",
    "                action_mean, action_std = ppo_instance.actor(state_torch)\n",
    "                dist = Normal(action_mean, action_std)\n",
    "                action = dist.sample()\n",
    "                action_clipped = torch.clamp(action.squeeze(0), ppo_instance.action_clip_range[0], ppo_instance.action_clip_range[1])\n",
    "                p_next = p_curr + action_clipped\n",
    "                p_next = torch.clamp(p_next, ppo_instance.min_x_bounds, ppo_instance.max_x_bounds)\n",
    "                p_curr = p_next\n",
    "\n",
    "            # Final p_curr after T steps\n",
    "            p_final_np = p_curr.detach().cpu().numpy()\n",
    "            ppo_instance.chk_jcbn._prepare_parameters([p_final_np], ppo_instance.names_km_full)\n",
    "\n",
    "            # max eigenvalue check\n",
    "            max_eig_list = ppo_instance.chk_jcbn.calc_eigenvalues_recal_vmax()\n",
    "            max_eig_val = max_eig_list[0]\n",
    "            is_valid = max_eig_val <= ppo_instance.eig_partition_final_reward\n",
    "\n",
    "            if is_valid:\n",
    "                valid_count += 1\n",
    "                all_final_params.append(p_final_np)\n",
    "\n",
    "    incidence_rate = valid_count / num_trials\n",
    "    print(f\"Incidence Rate (valid models): {incidence_rate:.4f} ({valid_count}/{num_trials})\")\n",
    "    return incidence_rate, all_final_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
