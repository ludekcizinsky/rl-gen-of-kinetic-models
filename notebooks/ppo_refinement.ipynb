{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7635ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import os\n",
    "import helper as hp\n",
    "from configparser import ConfigParser\n",
    "from ppo_refinement import PPORefinement\n",
    "\n",
    "from kinetics.jacobian_solver import check_jacobian\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add5ed0",
   "metadata": {},
   "source": [
    "# Define PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f40e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# PPO Algorithm Components\n",
    "# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=(256, 512, 1024)):\n",
    "        super(Actor, self).__init__()\n",
    "        self.hidden_dims = hidden_dims \n",
    "        layers = []\n",
    "        input_d = state_dim\n",
    "        for hidden_d in hidden_dims:\n",
    "            layers.append(nn.Linear(input_d, hidden_d))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_d = hidden_d\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.mean_layer = nn.Linear(hidden_dims[-1], action_dim)\n",
    "        self.log_std = nn.Parameter(torch.full((action_dim,), -0.5, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.network(state)\n",
    "        mean = self.mean_layer(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        if mean.ndim > 1 and std.ndim == 1 and std.shape[0] == mean.shape[1]:\n",
    "             std = std.unsqueeze(0).expand_as(mean)\n",
    "        return mean, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dims=(256, 512, 1024)):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hidden_dims = hidden_dims \n",
    "        layers = []\n",
    "        input_d = state_dim\n",
    "        for hidden_d in hidden_dims:\n",
    "            layers.append(nn.Linear(input_d, hidden_d))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_d = hidden_d\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.value_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.network(state)\n",
    "        value = self.value_layer(x)\n",
    "        return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b040b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPORefinement:\n",
    "    def __init__(self, param_dim, latent_dim, min_x_bounds, max_x_bounds, \n",
    "                 names_km_full, chk_jcbn,\n",
    "                 actor_hidden_dims=(256, 512, 1024), critic_hidden_dims=(256, 512, 1024),\n",
    "                 p0_init_std=1, actor_lr=1e-4, critic_lr=1e-4, \n",
    "                 gamma=0.99, epsilon=0.2, gae_lambda=0.95,\n",
    "                 ppo_epochs=10, num_episodes_per_update=64, \n",
    "                 T_horizon=5, k_reward_steepness=1.0,\n",
    "                 action_clip_range=(-0.1, 0.1),\n",
    "                 entropy_coeff=0.01, max_grad_norm=0.5,\n",
    "                 num_threads=8):  # Added num_threads parameter\n",
    "        \n",
    "        self.param_dim = param_dim\n",
    "        self.latent_dim = latent_dim  # For z in state\n",
    "        self.min_x_bounds = min_x_bounds\n",
    "        self.max_x_bounds = max_x_bounds\n",
    "        self.p0_init_std = p0_init_std\n",
    "        self.num_threads = num_threads  # Store the number of threads to use\n",
    "\n",
    "        self.names_km_full = names_km_full \n",
    "        self.chk_jcbn = chk_jcbn  # Store the jacobian checker instance\n",
    "        \n",
    "        # Thread-local storage for chk_jcbn instances to avoid race conditions\n",
    "        self.thread_local = threading.local()\n",
    "\n",
    "        # State dim: p_t (param_dim) + z (latent_dim) + lambda_max (1) + t (1)\n",
    "        self.state_dim = self.param_dim + self.latent_dim + 1 + 1\n",
    "        self.action_dim = self.param_dim  # Actor outputs updates to p_t\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, hidden_dims=actor_hidden_dims)\n",
    "        self.critic = Critic(self.state_dim, hidden_dims=critic_hidden_dims)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.num_episodes_per_update = num_episodes_per_update\n",
    "        self.T_horizon = T_horizon\n",
    "        self.k_reward_steepness = k_reward_steepness \n",
    "        self.action_clip_range = action_clip_range \n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.eig_partition_final_reward = -2.5 \n",
    "        \n",
    "        # Initialize the thread pool executor\n",
    "        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.num_threads)\n",
    "\n",
    "    def _get_thread_local_chk_jcbn(self):\n",
    "        \"\"\"Get or create a thread-local copy of chk_jcbn to avoid race conditions\"\"\"\n",
    "        if not hasattr(self.thread_local, 'chk_jcbn'):\n",
    "            # Clone the chk_jcbn object for this thread\n",
    "            # This assumes chk_jcbn has a copy or clone method, adjust as needed\n",
    "            # If no clone method exists, you might need to re-initialize the object\n",
    "            # or implement thread safety in another way\n",
    "            self.thread_local.chk_jcbn = self.chk_jcbn  # Replace with proper cloning if needed\n",
    "        return self.thread_local.chk_jcbn\n",
    "\n",
    "    def _get_lambda_max_single(self, p_tensor_single):\n",
    "        \"\"\"Single-threaded version of lambda_max calculation\"\"\"\n",
    "        p_numpy = p_tensor_single.detach().cpu().numpy()\n",
    "        # Use thread-local chk_jcbn instance\n",
    "        chk_jcbn_local = self._get_thread_local_chk_jcbn()\n",
    "        chk_jcbn_local._prepare_parameters([p_numpy], self.names_km_full) \n",
    "        max_eig_list = chk_jcbn_local.calc_eigenvalues_recal_vmax()\n",
    "        return max_eig_list[0]\n",
    "\n",
    "    def _get_lambda_max_batch(self, p_tensor_batch):\n",
    "        \"\"\"Calculate lambda_max for a batch of parameters in parallel\"\"\"\n",
    "        # Convert tensor batch to list of numpy arrays\n",
    "        p_numpy_list = [p.detach().cpu().numpy() for p in p_tensor_batch]\n",
    "        \n",
    "        # Use ThreadPoolExecutor to parallelize calculations\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_threads) as executor:\n",
    "            results = list(executor.map(self._get_lambda_max_single, p_tensor_batch))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _get_lambda_max(self, p_tensor_single):\n",
    "        \"\"\"Single parameter version - this maintains the original API\"\"\"\n",
    "        return self._get_lambda_max_single(p_tensor_single)\n",
    "\n",
    "    def _compute_reward(self, lambda_max_val):\n",
    "        intermediate_r = 1.0 / (1.0 + np.exp(self.k_reward_steepness * (lambda_max_val - (self.eig_partition_final_reward))))\n",
    "        # TODO: Right now, we are not using the Incidence part of the reward.\n",
    "        return intermediate_r\n",
    "\n",
    "    def _collect_trajectories(self):\n",
    "        batch_states = []\n",
    "        batch_actions = []\n",
    "        batch_log_probs_actions = []\n",
    "        batch_rewards = []\n",
    "        batch_next_states = []\n",
    "        batch_dones = []\n",
    "        \n",
    "        all_episode_total_rewards = []\n",
    "        \n",
    "        # Collect episode data in parallel\n",
    "        episode_futures = []\n",
    "        \n",
    "        for i_episode in range(self.num_episodes_per_update):\n",
    "            episode_future = self.executor.submit(self._collect_single_episode, i_episode)\n",
    "            episode_futures.append(episode_future)\n",
    "        \n",
    "        # Gather results\n",
    "        for future in concurrent.futures.as_completed(episode_futures):\n",
    "            ep_result = future.result()\n",
    "            if ep_result:  # Check if the episode returned valid data\n",
    "                (ep_states, ep_actions, ep_log_probs, ep_rewards, \n",
    "                 ep_next_states, ep_dones, ep_total_reward) = ep_result\n",
    "                \n",
    "                batch_states.extend(ep_states)\n",
    "                batch_actions.extend(ep_actions)\n",
    "                batch_log_probs_actions.extend(ep_log_probs)\n",
    "                batch_rewards.extend(ep_rewards)\n",
    "                batch_next_states.extend(ep_next_states)\n",
    "                batch_dones.extend(ep_dones)\n",
    "                all_episode_total_rewards.append(ep_total_reward)\n",
    "        \n",
    "        # Concatenate collected data into batch tensors\n",
    "        final_batch_states = torch.stack(batch_states) if batch_states else torch.empty(0, self.state_dim)\n",
    "        final_batch_actions = torch.stack(batch_actions) if batch_actions else torch.empty(0, self.action_dim)\n",
    "        final_batch_log_probs = torch.stack(batch_log_probs_actions) if batch_log_probs_actions else torch.empty(0,1)\n",
    "        final_batch_rewards = torch.stack(batch_rewards) if batch_rewards else torch.empty(0,1)\n",
    "        final_batch_next_states = torch.stack(batch_next_states) if batch_next_states else torch.empty(0, self.state_dim)\n",
    "        final_batch_dones = torch.stack(batch_dones) if batch_dones else torch.empty(0,1)\n",
    "        \n",
    "        avg_episode_reward_val = np.mean(all_episode_total_rewards) if all_episode_total_rewards else 0\n",
    "\n",
    "        return (final_batch_states, final_batch_actions, final_batch_log_probs, \n",
    "                final_batch_rewards, final_batch_next_states, final_batch_dones,\n",
    "                avg_episode_reward_val)\n",
    "\n",
    "    def _collect_single_episode(self, i_episode):\n",
    "        \"\"\"Collect data for a single episode - to be run in parallel\"\"\"\n",
    "        print(f\"Collecting episode data {i_episode + 1}/{self.num_episodes_per_update}...\")\n",
    "        \n",
    "        ep_states = []\n",
    "        ep_actions = []\n",
    "        ep_log_probs = []\n",
    "        ep_rewards = []\n",
    "        ep_next_states = []\n",
    "        ep_dones = []\n",
    "        \n",
    "        # Episode initialization\n",
    "        # Generate p0: small random values, then clamp. No parameter fixing.\n",
    "        p_curr_np = np.random.normal(0, self.p0_init_std, size=self.param_dim)\n",
    "        p_curr_np = np.clip(p_curr_np, self.min_x_bounds, self.max_x_bounds)\n",
    "        \n",
    "        # Generate z for state\n",
    "        z_curr_np = np.random.normal(0, 1, size=self.latent_dim)\n",
    "\n",
    "        p_curr_torch = torch.tensor(p_curr_np, dtype=torch.float32)\n",
    "        z_torch_ep = torch.tensor(z_curr_np, dtype=torch.float32)\n",
    "\n",
    "        episode_total_reward = 0\n",
    "\n",
    "        for t_s in range(self.T_horizon):\n",
    "            # Use instance methods for lambda_max and reward\n",
    "            lambda_max_pt_val = self._get_lambda_max(p_curr_torch) \n",
    "            \n",
    "            state_torch_flat = torch.cat((\n",
    "                p_curr_torch, z_torch_ep,\n",
    "                torch.tensor([lambda_max_pt_val], dtype=torch.float32),\n",
    "                torch.tensor([t_s], dtype=torch.float32)\n",
    "            ))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action_mean, action_std = self.actor(state_torch_flat.unsqueeze(0))\n",
    "                dist = Normal(action_mean, action_std)\n",
    "                action = dist.sample()\n",
    "                log_prob_action = dist.log_prob(action).sum(dim=-1)\n",
    "\n",
    "            ep_states.append(state_torch_flat)\n",
    "            ep_actions.append(action.squeeze(0))\n",
    "            ep_log_probs.append(log_prob_action)\n",
    "\n",
    "            action_clipped = torch.clamp(action.squeeze(0), self.action_clip_range[0], self.action_clip_range[1])\n",
    "            p_next_torch = p_curr_torch + action_clipped\n",
    "            p_next_torch = torch.clamp(p_next_torch, self.min_x_bounds, self.max_x_bounds)\n",
    "            \n",
    "            lambda_max_p_next_val = self._get_lambda_max(p_next_torch)\n",
    "            is_final_step = (t_s == self.T_horizon - 1)\n",
    "            reward_val = self._compute_reward(lambda_max_p_next_val)\n",
    "\n",
    "            ep_rewards.append(torch.tensor([reward_val], dtype=torch.float32))\n",
    "            episode_total_reward += reward_val\n",
    "\n",
    "            next_state_torch_flat = torch.cat((\n",
    "                p_next_torch, z_torch_ep,\n",
    "                torch.tensor([lambda_max_p_next_val], dtype=torch.float32),\n",
    "                torch.tensor([t_s + 1], dtype=torch.float32)\n",
    "            ))\n",
    "            ep_next_states.append(next_state_torch_flat)\n",
    "            ep_dones.append(torch.tensor([1.0 if is_final_step else 0.0], dtype=torch.float32))\n",
    "            p_curr_torch = p_next_torch\n",
    "        \n",
    "        return (ep_states, ep_actions, ep_log_probs, ep_rewards, \n",
    "                ep_next_states, ep_dones, episode_total_reward)\n",
    "\n",
    "    def _compute_gae(self, rewards, values, next_values, dones):\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        last_advantage = 0\n",
    "        if rewards.nelement() == 0: \n",
    "             return torch.zeros_like(rewards), torch.zeros_like(values) \n",
    "\n",
    "        for t in reversed(range(len(rewards))): \n",
    "            is_terminal_transition = dones[t].item() > 0.5 \n",
    "            delta = rewards[t] + self.gamma * next_values[t] * (1.0 - dones[t]) - values[t]\n",
    "            advantages[t] = last_advantage = delta + self.gamma * self.gae_lambda * (1.0 - dones[t]) * last_advantage\n",
    "        returns = advantages + values\n",
    "        return advantages, returns\n",
    "\n",
    "    def update(self, trajectories_data):\n",
    "        states, actions, log_probs_old, rewards, next_states, dones, _ = trajectories_data\n",
    "        \n",
    "        if states.nelement() == 0: \n",
    "            return 0.0, 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            values = self.critic(states)          \n",
    "            next_values = self.critic(next_states) \n",
    "\n",
    "        advantages, returns = self._compute_gae(rewards, values, next_values, dones)\n",
    "        \n",
    "        if advantages.nelement() == 0: \n",
    "             return 0.0, 0.0\n",
    "\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        actor_total_loss_epoch = 0\n",
    "        critic_total_loss_epoch = 0\n",
    "\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            current_pi_mean, current_pi_std = self.actor(states)\n",
    "            dist_new = Normal(current_pi_mean, current_pi_std)\n",
    "            log_probs_new = dist_new.log_prob(actions).sum(dim=-1, keepdim=True)\n",
    "            entropy = dist_new.entropy().mean()\n",
    "\n",
    "            ratios = torch.exp(log_probs_new - log_probs_old.detach()) \n",
    "            \n",
    "            surr1 = ratios * advantages.detach() \n",
    "            surr2 = torch.clamp(ratios, 1.0 - self.epsilon, 1.0 + self.epsilon) * advantages.detach()\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - self.entropy_coeff * entropy\n",
    "\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "            self.actor_optimizer.step()\n",
    "            actor_total_loss_epoch += actor_loss.item()\n",
    "\n",
    "            values_pred = self.critic(states) \n",
    "            critic_loss = (returns.detach() - values_pred).pow(2).mean()\n",
    "\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "            self.critic_optimizer.step()\n",
    "            critic_total_loss_epoch += critic_loss.item()\n",
    "        \n",
    "        avg_actor_loss = actor_total_loss_epoch / self.ppo_epochs\n",
    "        avg_critic_loss = critic_total_loss_epoch / self.ppo_epochs\n",
    "        return avg_actor_loss, avg_critic_loss\n",
    "\n",
    "    def train(self, num_iterations, output_path_base=\"ppo_training_output\"):\n",
    "        import os\n",
    "        os.makedirs(output_path_base, exist_ok=True)\n",
    "        \n",
    "        all_iter_avg_rewards = []\n",
    "        print(f\"Starting PPO training for {num_iterations} iterations with {self.num_threads} threads.\") \n",
    "        print(f\"State dim: {self.state_dim}, Action dim: {self.action_dim}, Latent (z) dim: {self.latent_dim}\")\n",
    "        print(f\"Num episodes per update: {self.num_episodes_per_update}, Horizon T: {self.T_horizon}\")\n",
    "        print(f\"p0 initialized with N(0, {self.p0_init_std**2}) and clamped to [{self.min_x_bounds}, {self.max_x_bounds}]\")\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            trajectories_data = self._collect_trajectories()\n",
    "            avg_episode_reward = trajectories_data[-1] \n",
    "            \n",
    "            if trajectories_data[0].nelement() == 0 and self.num_episodes_per_update > 0:\n",
    "                print(f\"Iter {iteration:04d}: No trajectories collected. Skipping update. Avg Ep Reward: {avg_episode_reward:.4f}\")\n",
    "                all_iter_avg_rewards.append(avg_episode_reward) \n",
    "                continue \n",
    "\n",
    "            actor_loss, critic_loss = self.update(trajectories_data)\n",
    "            all_iter_avg_rewards.append(avg_episode_reward)\n",
    "        \n",
    "            print(f\"Iter {iteration:04d}: Avg Ep Reward: {avg_episode_reward:.4f}, \"\n",
    "                    f\"Actor Loss: {actor_loss:.4f}, Critic Loss: {critic_loss:.4f}\")\n",
    "            \n",
    "            if iteration % 50 == 0 or iteration == num_iterations - 1: \n",
    "                actor_path = os.path.join(output_path_base, f\"actor_iter_{iteration}.pth\")\n",
    "                critic_path = os.path.join(output_path_base, f\"critic_iter_{iteration}.pth\")\n",
    "                torch.save(self.actor.state_dict(), actor_path)\n",
    "                torch.save(self.critic.state_dict(), critic_path)\n",
    "        \n",
    "        # Shutdown the executor\n",
    "        self.executor.shutdown()\n",
    "        print(\"Training finished.\")\n",
    "        return all_iter_avg_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba82737",
   "metadata": {},
   "source": [
    "# Train PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70e0c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse arguments from configfile\n",
    "configs = ConfigParser()\n",
    "configs.read('configfile.ini')\n",
    "\n",
    "n_samples = int(configs['MLP']['n_samples']) # Used by MLP for its internal sampling if any, and for p0 generation.\n",
    "\n",
    "lnminkm = float(configs['CONSTRAINTS']['min_km'])\n",
    "lnmaxkm = float(configs['CONSTRAINTS']['max_km'])\n",
    "\n",
    "repeats = int(configs['EVOSTRAT']['repeats'])\n",
    "generations = int(configs['EVOSTRAT']['generations']) # Will be used as num_iterations for PPO\n",
    "ss_idx = int(configs['EVOSTRAT']['ss_idx'])\n",
    "# n_threads = int(configs['EVOSTRAT']['n_threads']) # PPO collection is currently single-threaded\n",
    "\n",
    "output_path = configs['PATHS']['output_path']\n",
    "met_model = configs['PATHS']['met_model']\n",
    "names_km_config = hp.load_pkl(f'models/{met_model}/parameter_names_km_fdp1.pkl') # Full list of param names\n",
    "\n",
    "# Parameters needed directly by PPORefinement\n",
    "param_dim_config = int(configs['MLP']['no_kms'])\n",
    "latent_dim_config = int(configs['MLP']['latent_dim']) # For z vector in state\n",
    "\n",
    "\n",
    "# Call solvers from SKimPy (Used only for initial messages now)\n",
    "chk_jcbn = check_jacobian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93349eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load kinetic and thermodynamic data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 17:18:13,194 - thermomodel_new - INFO - # Model initialized with units kcal/mol and temperature 298.15 K\n",
      "2025-05-12 17:18:14,651 - Unnamed - WARNING - Non integer stoichiometries found ['CYTBO3_4pp', 'LMPD_biomass_c_1_420', 'CYTBDpp'] change to integer for linear dependencies\n",
      "2025-05-12 17:18:14,901 - Unnamed - WARNING - Non integer stoichiometries found ['CYTBO3_4pp', 'LMPD_biomass_c_1_420', 'CYTBDpp'] change to integer for linear dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Load steady state data\n"
     ]
    }
   ],
   "source": [
    "# Integrate data\n",
    "print('---- Load kinetic and thermodynamic data')\n",
    "chk_jcbn._load_ktmodels(met_model, 'fdp1')           ## Load kinetic and thermodynamic data\n",
    "print('---- Load steady state data')\n",
    "chk_jcbn._load_ssprofile(met_model, 'fdp1', ss_idx)  ## Integrate steady state information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f14b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Begin PPO refinement strategy\n",
      "Repeat 0: Starting PPO training for 25 iterations (serial execution).\n",
      "Starting PPO training for 25 iterations with 32 threads.\n",
      "State dim: 485, Action dim: 384, Latent (z) dim: 99\n",
      "Num episodes per update: 64, Horizon T: 5\n",
      "p0 initialized with N(0, 1) and clamped to [-25.0, 3.0]\n",
      "Collecting episode data 1/64...\n",
      "Collecting episode data 2/64...Collecting episode data 3/64...\n",
      "\n",
      "Collecting episode data 4/64...\n",
      "Collecting episode data 5/64...\n",
      "Collecting episode data 6/64...\n",
      "Collecting episode data 7/64...\n",
      "Collecting episode data 8/64...\n",
      "Collecting episode data 9/64...\n",
      "Collecting episode data 10/64...\n",
      "Collecting episode data 11/64...\n",
      "Collecting episode data 12/64...\n",
      "Collecting episode data 13/64...\n",
      "Collecting episode data 14/64...\n",
      "Collecting episode data 15/64...\n",
      "Collecting episode data 16/64...Collecting episode data 17/64...\n",
      "\n",
      "Collecting episode data 18/64...\n",
      "Collecting episode data 19/64...\n",
      "Collecting episode data 20/64...\n",
      "Collecting episode data 21/64...\n",
      "Collecting episode data 22/64...\n",
      "Collecting episode data 23/64...\n",
      "Collecting episode data 24/64...\n",
      "Collecting episode data 25/64...\n",
      "Collecting episode data 26/64...\n",
      "Collecting episode data 27/64...\n",
      "Collecting episode data 28/64...\n",
      "Collecting episode data 29/64...Collecting episode data 30/64...\n",
      "\n",
      "Collecting episode data 31/64...\n",
      "Collecting episode data 32/64...\n",
      "Collecting episode data 33/64...\n",
      "Collecting episode data 34/64...\n",
      "Collecting episode data 35/64...\n",
      "Collecting episode data 36/64...\n",
      "Collecting episode data 37/64...\n",
      "Collecting episode data 38/64...\n",
      "Collecting episode data 39/64...\n",
      "Collecting episode data 40/64...\n",
      "Collecting episode data 41/64...\n",
      "Collecting episode data 42/64...\n",
      "Collecting episode data 43/64...\n",
      "Collecting episode data 44/64...\n",
      "Collecting episode data 45/64...\n",
      "Collecting episode data 46/64...\n",
      "Collecting episode data 47/64...Collecting episode data 48/64...\n",
      "\n",
      "Collecting episode data 49/64...\n",
      "Collecting episode data 50/64...\n",
      "Collecting episode data 51/64...\n",
      "Collecting episode data 52/64...\n",
      "Collecting episode data 53/64...\n",
      "Collecting episode data 54/64...\n",
      "Collecting episode data 55/64...\n",
      "Collecting episode data 56/64...\n",
      "Collecting episode data 57/64...\n",
      "Collecting episode data 58/64...\n",
      "Collecting episode data 59/64...\n",
      "Collecting episode data 60/64...\n",
      "Collecting episode data 61/64...\n",
      "Collecting episode data 62/64...\n",
      "Collecting episode data 63/64...\n",
      "Collecting episode data 64/64...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0b60755ff989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     ppo_iteration_rewards = ppo_agent.train(\n\u001b[1;32m     20\u001b[0m         \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Use 'generations' from config as PPO iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput_path_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthis_savepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-68d8058dfdf5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_iterations, output_path_base)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mtrajectories_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collect_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mavg_episode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrajectories_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-68d8058dfdf5>\u001b[0m in \u001b[0;36m_collect_trajectories\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Gather results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_futures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mep_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mep_result\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Check if the episode returned valid data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 (ep_states, ep_actions, ep_log_probs, ep_rewards, \n",
      "\u001b[0;32m/usr/local/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-68d8058dfdf5>\u001b[0m in \u001b[0;36m_collect_single_episode\u001b[0;34m(self, i_episode)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt_s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT_horizon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# Use instance methods for lambda_max and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mlambda_max_pt_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lambda_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_curr_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             state_torch_flat = torch.cat((\n",
      "\u001b[0;32m<ipython-input-3-68d8058dfdf5>\u001b[0m in \u001b[0;36m_get_lambda_max\u001b[0;34m(self, p_tensor_single)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_lambda_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tensor_single\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;34m\"\"\"Single parameter version - this maintains the original API\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lambda_max_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_tensor_single\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_max_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-68d8058dfdf5>\u001b[0m in \u001b[0;36m_get_lambda_max_single\u001b[0;34m(self, p_tensor_single)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mchk_jcbn_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_thread_local_chk_jcbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mchk_jcbn_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_numpy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames_km_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mmax_eig_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchk_jcbn_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_eigenvalues_recal_vmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax_eig_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/renaissance/kinetics/jacobian_solver.py\u001b[0m in \u001b[0;36mcalc_eigenvalues_recal_vmax\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                     self.conc_series[self.kmodel.reactants],self.parameter_sample)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mthis_real_eigenvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigenvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_jacobian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mstore_eigen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_real_eigenvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mthis_param_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameterValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meigvals\u001b[0;34m(a, b, overwrite_a, check_finite, homogeneous_eigvals)\u001b[0m\n\u001b[1;32m    767\u001b[0m     return eig(a, b=b, left=0, right=0, overwrite_a=overwrite_a,\n\u001b[1;32m    768\u001b[0m                \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                homogeneous_eigvals=homogeneous_eigvals)\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meig\u001b[0;34m(a, b, left, right, overwrite_a, overwrite_b, check_finite, homogeneous_eigvals)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \"\"\"\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected square matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'masked arrays are not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         raise ValueError(\n\u001b[0;32m--> 499\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m    500\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "print('--- Begin PPO refinement strategy')\n",
    "for rep in range(repeats):\n",
    "    this_savepath = f'{output_path}/ppo_repeat_{rep}/' \n",
    "    os.makedirs(this_savepath, exist_ok=True)\n",
    "\n",
    "    # Instantiate PPORefinement agent with direct parameters for serial execution\n",
    "    ppo_agent = PPORefinement(\n",
    "        param_dim=param_dim_config,\n",
    "        latent_dim=latent_dim_config,\n",
    "        min_x_bounds=lnminkm,\n",
    "        max_x_bounds=lnmaxkm,\n",
    "        names_km_full=names_km_config, # Pass the full list of names\n",
    "        chk_jcbn=chk_jcbn,             # Pass the jacobian checker instance\n",
    "        p0_init_std=1, # Default is 0.01\n",
    "        num_threads=32\n",
    "    )\n",
    "    \n",
    "    print(f\"Repeat {rep}: Starting PPO training for {generations} iterations (serial execution).\")\n",
    "    ppo_iteration_rewards = ppo_agent.train(\n",
    "        num_iterations=generations, # Use 'generations' from config as PPO iterations\n",
    "        output_path_base=this_savepath\n",
    "    )\n",
    "    \n",
    "    hp.save_pkl(f'{this_savepath}/ppo_iteration_rewards.pkl', ppo_iteration_rewards)\n",
    "    print(f\"Repeat {rep}: PPO training finished. Rewards log saved to {this_savepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd8a2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
